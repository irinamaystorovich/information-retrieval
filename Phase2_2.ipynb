{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Phase 1**\n"
      ],
      "metadata": {
        "id": "QEmhhkJzLQN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TutC-7uTsxTZ",
        "outputId": "aa7794f9-a4c7-472c-ebc7-9cdcdced6ad1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUMUD64nr7nc",
        "outputId": "626e6575-2c9f-4edf-8832-dd892ef52a4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_1\n",
            "Parsed documents (first 10): ['FT911-1', 'FT911-2', 'FT911-3', 'FT911-4', 'FT911-5', 'FT911-6', 'FT911-7', 'FT911-8', 'FT911-9', 'FT911-10']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_2\n",
            "Parsed documents (first 10): ['FT911-376', 'FT911-377', 'FT911-378', 'FT911-379', 'FT911-380', 'FT911-381', 'FT911-382', 'FT911-383', 'FT911-384', 'FT911-385']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_3\n",
            "Parsed documents (first 10): ['FT911-722', 'FT911-723', 'FT911-724', 'FT911-725', 'FT911-726', 'FT911-727', 'FT911-728', 'FT911-729', 'FT911-730', 'FT911-731']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_4\n",
            "Parsed documents (first 10): ['FT911-1099', 'FT911-1100', 'FT911-1101', 'FT911-1102', 'FT911-1103', 'FT911-1104', 'FT911-1105', 'FT911-1106', 'FT911-1107', 'FT911-1108']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_5\n",
            "Parsed documents (first 10): ['FT911-1479', 'FT911-1480', 'FT911-1481', 'FT911-1482', 'FT911-1483', 'FT911-1484', 'FT911-1485', 'FT911-1486', 'FT911-1487', 'FT911-1488']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_6\n",
            "Parsed documents (first 10): ['FT911-1832', 'FT911-1833', 'FT911-1834', 'FT911-1835', 'FT911-1836', 'FT911-1837', 'FT911-1838', 'FT911-1839', 'FT911-1840', 'FT911-1841']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_7\n",
            "Parsed documents (first 10): ['FT911-2212', 'FT911-2213', 'FT911-2214', 'FT911-2215', 'FT911-2216', 'FT911-2217', 'FT911-2218', 'FT911-2219', 'FT911-2220', 'FT911-2221']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_8\n",
            "Parsed documents (first 10): ['FT911-2621', 'FT911-2622', 'FT911-2623', 'FT911-2624', 'FT911-2625', 'FT911-2626', 'FT911-2627', 'FT911-2628', 'FT911-2629', 'FT911-2630']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_9\n",
            "Parsed documents (first 10): ['FT911-2950', 'FT911-2951', 'FT911-2952', 'FT911-2953', 'FT911-2954', 'FT911-2955', 'FT911-2956', 'FT911-2957', 'FT911-2958', 'FT911-2959']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_10\n",
            "Parsed documents (first 10): ['FT911-3323', 'FT911-3324', 'FT911-3325', 'FT911-3326', 'FT911-3327', 'FT911-3328', 'FT911-3329', 'FT911-3330', 'FT911-3331', 'FT911-3332']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_11\n",
            "Parsed documents (first 10): ['FT911-3694', 'FT911-3695', 'FT911-3696', 'FT911-3697', 'FT911-3698', 'FT911-3699', 'FT911-3700', 'FT911-3701', 'FT911-3702', 'FT911-3703']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_12\n",
            "Parsed documents (first 10): ['FT911-4016', 'FT911-4017', 'FT911-4018', 'FT911-4019', 'FT911-4020', 'FT911-4021', 'FT911-4022', 'FT911-4023', 'FT911-4024', 'FT911-4025']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_13\n",
            "Parsed documents (first 10): ['FT911-4366', 'FT911-4367', 'FT911-4368', 'FT911-4369', 'FT911-4370', 'FT911-4371', 'FT911-4372', 'FT911-4373', 'FT911-4374', 'FT911-4375']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_15\n",
            "Parsed documents (first 10): ['FT911-5129', 'FT911-5130', 'FT911-5131', 'FT911-5132', 'FT911-5133', 'FT911-5134', 'FT911-5135', 'FT911-5136', 'FT911-5137', 'FT911-5138']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_14\n",
            "Parsed documents (first 10): ['FT911-4749', 'FT911-4750', 'FT911-4751', 'FT911-4752', 'FT911-4753', 'FT911-4754', 'FT911-4755', 'FT911-4756', 'FT911-4757', 'FT911-4758']\n",
            "WordDictionary saved to /content/drive/MyDrive/InfoRetrieval/word_dictionary.txt\n",
            "FileDictionary saved to /content/drive/MyDrive/InfoRetrieval/file_dictionary.txt\n",
            "FTFileMapping saved to /content/drive/MyDrive/InfoRetrieval/ft_file_mapping.txt\n",
            "DocumentToFileMapping saved to /content/drive/MyDrive/InfoRetrieval/doc_to_file_mapping.txt\n",
            "parser_output.txt saved to /content/drive/MyDrive/InfoRetrieval/parser_output.txt\n",
            "Token and Token ID Mapping (First 20):\n",
            "aa\t1\n",
            "aaa\t2\n",
            "aachen\t3\n",
            "aaf\t4\n",
            "aah\t5\n",
            "aakvaag\t6\n",
            "aalborg\t7\n",
            "aaron\t8\n",
            "ab\t9\n",
            "ababa\t10\n",
            "aback\t11\n",
            "abalkin\t12\n",
            "abandon\t13\n",
            "abash\t14\n",
            "abat\t15\n",
            "abattoir\t16\n",
            "abb\t17\n",
            "abba\t18\n",
            "abbacchio\t19\n",
            "abbado\t20\n",
            "DocumentToFileMapping (Sample):\n",
            "('FT911-3', '901')\n",
            "('FT911-3', '902')\n",
            "('FT911-3', '903')\n",
            "('FT911-3', '904')\n",
            "('FT911-3', '905')\n",
            "('FT911-3', '906')\n",
            "('FT911-3', '907')\n",
            "('FT911-3', '908')\n",
            "('FT911-3', '909')\n",
            "('FT911-3', '910')\n"
          ]
        }
      ],
      "source": [
        "# Load stopwords\n",
        "stopwords_path = \"/content/drive/MyDrive/InfoRetrieval/stopwordlist.txt\"\n",
        "with open(stopwords_path, 'r', encoding='utf-8') as f:\n",
        "    stopwords = set(f.read().split())\n",
        "\n",
        "# Initialize the stemmer\n",
        "word_stemmer = PorterStemmer()\n",
        "\n",
        "docs_path = \"/content/drive/MyDrive/InfoRetrieval/ft911/\"\n",
        "processed_docs = {}\n",
        "#maps: file_name to list of DOCNOs\n",
        "FTFileMapping = {}\n",
        "#maps\n",
        "FileDictionary = {}  # Mapping: (file_name, local_doc_num) -> global_doc_id\n",
        "DocumentToFileMapping = []  # list of (file_name, numeric_DOCNO)\n",
        "\n",
        "# Files are sorted numerically according to suffix\n",
        "file_list = sorted(\n",
        "    [f for f in os.listdir(docs_path) if f.startswith(\"ft911_\")],\n",
        "    key=lambda x: int(re.findall(r'\\d+', x)[0])\n",
        ")\n",
        "\n",
        "# Process each file\n",
        "global_doc_id = 1\n",
        "for file_name in file_list:\n",
        "    full_file_path = os.path.join(docs_path, file_name)\n",
        "\n",
        "    if os.path.isfile(full_file_path):\n",
        "        print(f\"Checking file: {full_file_path}\")\n",
        "        with open(full_file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "            file_content = file.read()\n",
        "\n",
        "            # Parse TREC documents\n",
        "            parsed_docs = {}\n",
        "            current_doc_lines = []\n",
        "            current_doc_id = None\n",
        "            for line in file_content.splitlines():\n",
        "                if line.startswith(\"<DOCNO>\"):\n",
        "                    if current_doc_id is not None:\n",
        "                        parsed_docs[current_doc_id] = \" \".join(current_doc_lines)\n",
        "                    current_doc_id = line.replace(\"<DOCNO>\", \"\").replace(\"</DOCNO>\", \"\").strip()\n",
        "                    current_doc_lines = []\n",
        "                elif not line.startswith(\"<\") and current_doc_id:\n",
        "                    current_doc_lines.append(line.strip())\n",
        "            if current_doc_id is not None:\n",
        "                parsed_docs[current_doc_id] = \" \".join(current_doc_lines)\n",
        "\n",
        "            print(f\"Parsed documents (first 10): {list(parsed_docs.keys())[:10]}\")\n",
        "\n",
        "            # Save file -> DOCNOs\n",
        "            FTFileMapping[file_name] = list(parsed_docs.keys())\n",
        "\n",
        "            # Save cleaned (FT911-X, numeric_DOCNO)\n",
        "            for docno in parsed_docs.keys():\n",
        "                numeric_docno = docno.split(\"-\")[-1]\n",
        "                ft_formatted = f\"FT911-{file_name.split('_')[-1]}\"\n",
        "                DocumentToFileMapping.append((ft_formatted, numeric_docno))\n",
        "\n",
        "            # Assign global and local doc IDs\n",
        "            local_id = 1\n",
        "            for doc_id in parsed_docs:\n",
        "                FileDictionary[(file_name, local_id)] = global_doc_id\n",
        "                local_id += 1\n",
        "                global_doc_id += 1\n",
        "\n",
        "            # Process text\n",
        "            for doc_id, doc_text in sorted(parsed_docs.items()):\n",
        "                text = doc_text.lower()\n",
        "                text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
        "                tokens = nltk.word_tokenize(text)\n",
        "                tokens = [word for word in tokens if word.isalpha()]\n",
        "                filtered_tokens = [word for word in tokens if word not in stopwords]\n",
        "                stemmed_tokens = [word_stemmer.stem(word) for word in filtered_tokens]\n",
        "                processed_docs[doc_id] = stemmed_tokens\n",
        "# Create WordDictionary: sort all unique words alphabetically, then assign IDs\n",
        "all_words = set()\n",
        "for words in processed_docs.values():\n",
        "    all_words.update(words)\n",
        "\n",
        "WordDictionary = {}\n",
        "for idx, word in enumerate(sorted(all_words), start=1):\n",
        "    WordDictionary[word] = idx\n",
        "\n",
        "# Sort FileDictionary\n",
        "FileDictionary = dict(sorted(FileDictionary.items()))\n",
        "\n",
        "# Sort DocumentToFileMapping: sort FT911-14 before FT911-15\n",
        "def sort_key(doc):\n",
        "    prefix, number = doc[0].split(\"-\")\n",
        "    return int(number), int(doc[1])\n",
        "\n",
        "DocumentToFileMapping = sorted(DocumentToFileMapping, key=sort_key)\n",
        "\n",
        "# Save WordDictionary\n",
        "word_dict_path = \"/content/drive/MyDrive/InfoRetrieval/word_dictionary.txt\"\n",
        "with open(word_dict_path, \"w\") as word_file:\n",
        "    for word, word_id in WordDictionary.items():\n",
        "        word_file.write(f\"{word}\\t{word_id}\\n\")\n",
        "\n",
        "# Save FileDictionary (filename + local doc number)\n",
        "file_dict_path = \"/content/drive/MyDrive/InfoRetrieval/file_dictionary.txt\"\n",
        "with open(file_dict_path, \"w\") as file_file:\n",
        "    for (file_name, local_doc_num), global_id in FileDictionary.items():\n",
        "        file_file.write(f\"{file_name}\\t{local_doc_num}\\t{global_id}\\n\")\n",
        "\n",
        "# Save FTFileMapping (file -> list of DOCNOs)\n",
        "ft_map_path = \"/content/drive/MyDrive/InfoRetrieval/ft_file_mapping.txt\"\n",
        "with open(ft_map_path, \"w\") as map_file:\n",
        "    for file, doc_ids in FTFileMapping.items():\n",
        "        map_file.write(f\"{file}:\\t{', '.join(doc_ids)}\\n\")\n",
        "\n",
        "# Save final formatted DocumentToFileMapping\n",
        "doc_file_map_path = \"/content/drive/MyDrive/InfoRetrieval/doc_to_file_mapping.txt\"\n",
        "with open(doc_file_map_path, \"w\") as out_file:\n",
        "    for formatted_file, numeric_docno in DocumentToFileMapping:\n",
        "        out_file.write(f\"{formatted_file}\\t{numeric_docno}\\n\")\n",
        "\n",
        "# Create parser_output.txt with token dictionary and document ID mapping\n",
        "parser_output_path = \"/content/drive/MyDrive/InfoRetrieval/parser_output.txt\"\n",
        "with open(parser_output_path, \"w\") as out_file:\n",
        "    # First print word -> ID mapping\n",
        "    for word, word_id in WordDictionary.items():\n",
        "        out_file.write(f\"{word}\\t\\t{word_id}\\n\")\n",
        "\n",
        "    out_file.write(\"\\n\")\n",
        "\n",
        "    # Then print document -> ID mapping\n",
        "    for idx, (formatted_file, numeric_docno) in enumerate(DocumentToFileMapping, start=1):\n",
        "        out_file.write(f\"{formatted_file}\\t{numeric_docno}\\n\")\n",
        "\n",
        "# Logs\n",
        "print(f\"WordDictionary saved to {word_dict_path}\")\n",
        "print(f\"FileDictionary saved to {file_dict_path}\")\n",
        "print(f\"FTFileMapping saved to {ft_map_path}\")\n",
        "print(f\"DocumentToFileMapping saved to {doc_file_map_path}\")\n",
        "print(f\"parser_output.txt saved to {parser_output_path}\")\n",
        "\n",
        "# Print sample of token to ID mapping\n",
        "print(\"Token and Token ID Mapping (First 20):\")\n",
        "for token, token_id in list(WordDictionary.items())[:20]:\n",
        "    print(f\"{token}\\t{token_id}\")\n",
        "\n",
        "# Print DocumentToFileMapping like your format\n",
        "print(\"DocumentToFileMapping (Sample):\")\n",
        "for pair in DocumentToFileMapping[900:910]:\n",
        "    print(pair)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Phase 2**"
      ],
      "metadata": {
        "id": "6VPF_Q4vtJvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import time\n",
        "import sys\n",
        "from collections import defaultdict\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Load stopwords\n",
        "stopwords_path = \"/content/drive/MyDrive/InfoRetrieval/stopwordlist.txt\"\n",
        "with open(stopwords_path, 'r', encoding='utf-8') as f:\n",
        "    stopwords = set(f.read().split())\n",
        "\n",
        "# Initialize the stemmer\n",
        "word_stemmer = PorterStemmer()\n",
        "\n",
        "# === Test mode with testdata.txt ===\n",
        "test_file_path = \"/content/drive/MyDrive/InfoRetrieval/testdata.txt\"\n",
        "processed_docs = {}\n",
        "\n",
        "with open(test_file_path, 'r', encoding='utf-8') as f:\n",
        "    content = f.read()\n",
        "\n",
        "# Measure start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Parse test documents\n",
        "parsed_docs = {}\n",
        "current_doc_lines = []\n",
        "current_doc_id = None\n",
        "for line in content.splitlines():\n",
        "    if line.startswith(\"<DOCNO>\"):\n",
        "        if current_doc_id is not None:\n",
        "            parsed_docs[current_doc_id] = \" \".join(current_doc_lines)\n",
        "        current_doc_id = line.replace(\"<DOCNO>\", \"\").replace(\"</DOCNO>\", \"\").strip()\n",
        "        current_doc_lines = []\n",
        "    elif not line.startswith(\"<\") and current_doc_id:\n",
        "        current_doc_lines.append(line.strip())\n",
        "if current_doc_id is not None:\n",
        "    parsed_docs[current_doc_id] = \" \".join(current_doc_lines)\n",
        "\n",
        "# Preprocess and stem\n",
        "for doc_id, doc_text in parsed_docs.items():\n",
        "    text = doc_text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    filtered_tokens = [word for word in tokens if word not in stopwords]\n",
        "    stemmed_tokens = [word_stemmer.stem(word) for word in filtered_tokens]\n",
        "    processed_docs[doc_id] = stemmed_tokens\n",
        "\n",
        "# Build dictionary and indices\n",
        "all_words = set(word for words in processed_docs.values() for word in words)\n",
        "WordDictionary = {word: idx for idx, word in enumerate(sorted(all_words), start=1)}\n",
        "forward_index = defaultdict(lambda: defaultdict(int))\n",
        "inverted_index = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "for doc_id, words in processed_docs.items():\n",
        "    for word in words:\n",
        "        wid = WordDictionary[word]\n",
        "        forward_index[doc_id][wid] += 1\n",
        "        inverted_index[wid][doc_id] += 1\n",
        "\n",
        "# Save forward index\n",
        "forward_index_path = \"/content/drive/MyDrive/InfoRetrieval/forward_index.txt\"\n",
        "with open(forward_index_path, \"w\") as f:\n",
        "    for doc_id in sorted(forward_index):\n",
        "        entries = [f\"{wid}:{freq}\" for wid, freq in forward_index[doc_id].items()]\n",
        "        f.write(f\"{doc_id}: {'; '.join(entries)}\\n\")\n",
        "\n",
        "# Save inverted index\n",
        "inverted_index_path = \"/content/drive/MyDrive/InfoRetrieval/inverted_index.txt\"\n",
        "with open(inverted_index_path, \"w\") as f:\n",
        "    for wid in sorted(inverted_index):\n",
        "        entries = [f\"{doc_id}:{freq}\" for doc_id, freq in inverted_index[wid].items()]\n",
        "        f.write(f\"{wid}: {'; '.join(entries)}\\n\")\n",
        "\n",
        "# Measure end time\n",
        "end_time = time.time()\n",
        "print(f\"Indexing completed in {end_time - start_time:.4f} seconds.\")\n",
        "\n",
        "# Estimate memory usage (roughly)\n",
        "def get_dict_size(d):\n",
        "    return sys.getsizeof(d) + sum(sys.getsizeof(k) + sys.getsizeof(v) for k, v in d.items())\n",
        "\n",
        "forward_size = get_dict_size(forward_index)\n",
        "inverted_size = get_dict_size(inverted_index)\n",
        "word_dict_size = get_dict_size(WordDictionary)\n",
        "\n",
        "total_index_size_kb = (forward_size + inverted_size + word_dict_size) / 1024\n",
        "print(f\"Estimated index size in memory: {total_index_size_kb:.2f} KB\")\n",
        "\n",
        "# Estimate file sizes\n",
        "def file_size_in_kb(path):\n",
        "    return os.path.getsize(path) / 1024\n",
        "\n",
        "forward_file_size_kb = file_size_in_kb(forward_index_path)\n",
        "inverted_file_size_kb = file_size_in_kb(inverted_index_path)\n",
        "\n",
        "print(f\"forward_index.txt size: {forward_file_size_kb:.2f} KB\")\n",
        "print(f\"inverted_index.txt size: {inverted_file_size_kb:.2f} KB\")\n",
        "\n",
        "# Test query\n",
        "test_query = input(\"Enter a word to look up (testdata): \").strip().lower()\n",
        "if test_query in stopwords:\n",
        "    print(\"This is a stopword and is ignored.\")\n",
        "else:\n",
        "    stemmed = word_stemmer.stem(test_query)\n",
        "    if stemmed in WordDictionary:\n",
        "        wid = WordDictionary[stemmed]\n",
        "        postings = inverted_index[wid]\n",
        "        output = \"; \".join([f\"{doc_id}:{freq}\" for doc_id, freq in postings.items()])\n",
        "        print(f\"{stemmed} ({wid}): {output}\")\n",
        "    else:\n",
        "        print(\"Word not found in the dictionary.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVObMO-8IJi0",
        "outputId": "6a10b630-f3f6-41f2-ec4c-ca52b8d5da51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indexing completed in 0.0219 seconds.\n",
            "Estimated index size in memory: 3.40 KB\n",
            "forward_index.txt size: 0.05 KB\n",
            "inverted_index.txt size: 0.06 KB\n",
            "Enter a word to look up (testdata): word\n",
            "Word not found in the dictionary.\n"
          ]
        }
      ]
    }
  ]
}