{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUNLMBPlmLy5",
        "outputId": "4956dd96-e48f-4658-a72c-935811a326e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Load stopwords from file*"
      ],
      "metadata": {
        "id": "yWs_jdw-qbuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_stopwords(FileName):\n",
        "    with open(FileName, 'r') as f:\n",
        "        stop_words = set(f.read().split())\n",
        "    return stop_words"
      ],
      "metadata": {
        "id": "dAx4LO3tqZE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lowercase, Non-alphabetic Removal, Tokenize"
      ],
      "metadata": {
        "id": "CmpkjcNwlz9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Text=\"/content/drive/MyDrive/InfoRetrieval/ft911/ft911_14\"\n",
        "Text = Text.lower()\n",
        "Text = re.sub(r'[^a-zA-Z\\s]', ' ', Text)\n",
        "tokens = nltk.word_tokenize(Text)\n",
        "tokens = [word for word in tokens if word.isalpha()]\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3wZ3ScjCwxU",
        "outputId": "04caee4b-fcfb-4d45-90a7-a89d728ced84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['content', 'drive', 'mydrive', 'inforetrieval', 'ft', 'ft']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QI5mNH6NlwVH"
      },
      "outputs": [],
      "source": [
        "def tokenize(Text):\n",
        "    Text = Text.lower()\n",
        "    Text = re.sub(r'[^a-zA-Z\\s]', ' ', Text)\n",
        "    tokens = nltk.word_tokenize(Text)\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove stepwords, Stemming"
      ],
      "metadata": {
        "id": "7-hQdS9zppWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_text(Text, Stopwords, Stemmer):\n",
        "    #Tokenization\n",
        "    tokens = tokenize(Text)\n",
        "\n",
        "    #Remove stopwords\n",
        "    filtered_tokens = [word for word in tokens if word not in Stopwords]\n",
        "\n",
        "    #Stemming\n",
        "    stemmed_tokens = [Stemmer.stem(word) for word in filtered_tokens]\n",
        "\n",
        "    return stemmed_tokens"
      ],
      "metadata": {
        "id": "73VOB-Pxl_-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parsing the Documents"
      ],
      "metadata": {
        "id": "ymJK9XkE7it1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_trec_documents(file_content):\n",
        "    # Dictionary thas stores the parsed documents\n",
        "    parsed_docs = {}\n",
        "\n",
        "\n",
        "    current_doc_lines = []\n",
        "    current_doc_id = None\n",
        "\n",
        "\n",
        "    # Split the file content into individual lines and process each line\n",
        "    for line in file_content.splitlines():\n",
        "\n",
        "        if line.startswith(\"<DOCNO>\"):\n",
        "\n",
        "            if current_doc_id is not None:\n",
        "                parsed_docs[current_doc_id] = \" \".join(current_doc_lines)\n",
        "\n",
        "\n",
        "            current_doc_id = line.replace(\"<DOCNO>\", \"\").replace(\"</DOCNO>\", \"\").strip()\n",
        "\n",
        "\n",
        "            current_doc_lines = []\n",
        "\n",
        "        elif not line.startswith(\"<\") and current_doc_id:\n",
        "            current_doc_lines.append(line.strip())\n",
        "\n",
        "    if current_doc_id is not None:\n",
        "        parsed_docs[current_doc_id] = \" \".join(current_doc_lines)\n",
        "\n",
        "\n",
        "    return parsed_docs"
      ],
      "metadata": {
        "id": "fe3iYlkW7a_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assign IDs to words and documents"
      ],
      "metadata": {
        "id": "Q1gZfQCfpwqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_dictionaries(parsed_docs):\n",
        "    # Dictionaries for word and doc IDs\n",
        "    doc_to_id_map = {}\n",
        "    word_to_id_map = {}\n",
        "\n",
        "    #Counters initialization and doc IDs\n",
        "    current_doc_id = 1\n",
        "    current_word_id = 1\n",
        "\n",
        "    word_id_lines = []\n",
        "    all_output_lines = []\n",
        "    doc_id_lines = []\n",
        "\n",
        "    #looping through each document and its list of words\n",
        "    for doc_name, words in parsed_docs.items():\n",
        "        if doc_name not in doc_to_id_map:\n",
        "            doc_to_id_map[doc_name] = current_doc_id\n",
        "            doc_id_lines.append(f\"{doc_name}\\t{current_doc_id}\")\n",
        "            current_doc_id += 1\n",
        "\n",
        "        # loop through each word in the document\n",
        "        for word in words:\n",
        "            if word not in word_to_id_map:\n",
        "                word_to_id_map[word] = current_word_id\n",
        "                word_id_lines.append(f\"{word}\\t{current_word_id}\")\n",
        "                current_word_id += 1\n",
        "\n",
        "    # Combine the word and document lines into one list\n",
        "    all_output_lines.extend(word_id_lines)\n",
        "    all_output_lines.append(\"\\n\")\n",
        "    all_output_lines.extend(doc_id_lines)\n",
        "\n",
        "\n",
        "    return word_to_id_map, doc_to_id_map, all_output_lines\n"
      ],
      "metadata": {
        "id": "W3h0zpyom-8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Procesing files. Output generation"
      ],
      "metadata": {
        "id": "0Yi3Mdu2p3Xh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_documents(folder_path, stopwords):\n",
        "    # Initialize the stemmer\n",
        "    word_stemmer = PorterStemmer()\n",
        "\n",
        "    # Dictionary that store processed documents\n",
        "    processed_docs = {}\n",
        "\n",
        "    # Loop through all files in the specified folder\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        full_file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "        if os.path.isfile(full_file_path) and file_name.startswith(\"ft911_\"):\n",
        "            with open(full_file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "                file_content = file.read()\n",
        "\n",
        "                # Parse the file content into individual documents\n",
        "                parsed_documents = parse_trec_documents(file_content)\n",
        "\n",
        "                # Process each document\n",
        "                for doc_id, doc_text in parsed_documents.items():\n",
        "                    processed_docs[doc_id] = process_text(doc_text, stopwords, word_stemmer)\n",
        "\n",
        "    # Create dictionaries mapping words and documents to unique IDs\n",
        "    word_to_id_map, doc_to_id_map, output_lines = create_dictionaries(processed_docs)\n",
        "\n",
        "    output_file_path = \"/content/drive/MyDrive/InfoRetrieval/parser_output.txt\"\n",
        "\n",
        "    with open(output_file_path, \"w\") as output_file:\n",
        "        for line in output_lines:\n",
        "            output_file.write(line + \"\\n\")\n",
        "\n",
        "    # Print sample data\n",
        "    print(f\"The parsed data has been saved to {output_file_path}\")\n",
        "    print(f\"Sample document IDs: {list(doc_to_id_map.items())[:15]}\")\n",
        "    print(f\"Sample tokens: {list(word_to_id_map.items())[:20]}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rSJomq9vnDZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MAIN FUNCTION**"
      ],
      "metadata": {
        "id": "eQpMoL-_qQ2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function\n",
        "def main():\n",
        "    docs_path = \"/content/drive/MyDrive/InfoRetrieval/ft911/\"\n",
        "\n",
        "    stopwords_path = \"/content/drive/MyDrive/InfoRetrieval/stopwordlist.txt\"\n",
        "\n",
        "    stopwords = load_stopwords(stopwords_path)\n",
        "\n",
        "    process_documents(docs_path, stopwords)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2dZ4GGOnIeR",
        "outputId": "514154c1-88f9-4a5b-e607-82e91f71acd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The parsed data has been saved to /content/drive/MyDrive/InfoRetrieval/parser_output.txt\n",
            "Sample document IDs: [('FT911-1', 1), ('FT911-2', 2), ('FT911-3', 3), ('FT911-4', 4), ('FT911-5', 5), ('FT911-6', 6), ('FT911-7', 7), ('FT911-8', 8), ('FT911-9', 9), ('FT911-10', 10), ('FT911-11', 11), ('FT911-12', 12), ('FT911-13', 13), ('FT911-14', 14), ('FT911-15', 15)]\n",
            "Sample tokens: [('ft', 1), ('correct', 2), ('jubile', 3), ('jet', 4), ('design', 5), ('publish', 6), ('append', 7), ('articl', 8), ('frank', 9), ('fli', 10), ('shout', 11), ('sir', 12), ('whittl', 13), ('maiden', 14), ('flight', 15), ('british', 16), ('repli', 17), ('patent', 18), ('aircraft', 19), ('ga', 20)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_documents(folder_path, stopwords):\n",
        "    # Initialize the stemmer\n",
        "    word_stemmer = PorterStemmer()\n",
        "\n",
        "    # Dictionary that stores processed documents\n",
        "    processed_docs = {}\n",
        "\n",
        "    # Loop through all files in the specified folder\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        full_file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "        if os.path.isfile(full_file_path) and file_name.startswith(\"ft911_\"):\n",
        "            print(f\"Checking file: {full_file_path}\")  # Debugging line\n",
        "            with open(full_file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "                file_content = file.read()\n",
        "\n",
        "                # Parse the file content into individual documents\n",
        "                parsed_documents = parse_trec_documents(file_content)\n",
        "                print(f\"Parsed documents (first 10): {list(parsed_documents.keys())[:10]}\")  # Debugging line\n",
        "\n",
        "                # Process each document\n",
        "                for doc_id, doc_text in parsed_documents.items():\n",
        "                    processed_docs[doc_id] = process_text(doc_text, stopwords, word_stemmer)\n",
        "\n",
        "    # Create dictionaries mapping words and documents to unique IDs\n",
        "    word_to_id_map, doc_to_id_map, output_lines = create_dictionaries(processed_docs)\n",
        "\n",
        "    output_file_path = \"/content/drive/MyDrive/InfoRetrieval/parser_output.txt\"\n",
        "\n",
        "    with open(output_file_path, \"w\") as output_file:\n",
        "        for line in output_lines:\n",
        "            output_file.write(line + \"\\n\")\n",
        "\n",
        "    # Print formatted token and token ID (first 100 only)\n",
        "    print(\"Token and Token ID Mapping (First 100):\")\n",
        "    for token, token_id in list(word_to_id_map.items())[:100]:\n",
        "        print(f\"{token}\\t{token_id}\")\n",
        "\n",
        "    print(f\"The parsed data has been saved to {output_file_path}\")\n",
        "    print(f\"Sample document IDs: {list(doc_to_id_map.items())[:10]}\")\n",
        "    print(f\"Sample tokens: {list(word_to_id_map.items())[:20]}\")\n",
        "\n",
        "def main():\n",
        "    docs_path = \"/content/drive/MyDrive/InfoRetrieval/ft911/\"\n",
        "    stopwords_path = \"/content/drive/MyDrive/InfoRetrieval/stopwordlist.txt\"\n",
        "\n",
        "    stopwords = load_stopwords(stopwords_path)\n",
        "    process_documents(docs_path, stopwords)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DpIoAilF06v",
        "outputId": "8f418116-d63e-439d-e44d-e4fc2ff6a8e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_1\n",
            "Parsed documents (first 10): ['FT911-1', 'FT911-2', 'FT911-3', 'FT911-4', 'FT911-5', 'FT911-6', 'FT911-7', 'FT911-8', 'FT911-9', 'FT911-10']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_2\n",
            "Parsed documents (first 10): ['FT911-376', 'FT911-377', 'FT911-378', 'FT911-379', 'FT911-380', 'FT911-381', 'FT911-382', 'FT911-383', 'FT911-384', 'FT911-385']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_3\n",
            "Parsed documents (first 10): ['FT911-722', 'FT911-723', 'FT911-724', 'FT911-725', 'FT911-726', 'FT911-727', 'FT911-728', 'FT911-729', 'FT911-730', 'FT911-731']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_4\n",
            "Parsed documents (first 10): ['FT911-1099', 'FT911-1100', 'FT911-1101', 'FT911-1102', 'FT911-1103', 'FT911-1104', 'FT911-1105', 'FT911-1106', 'FT911-1107', 'FT911-1108']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_5\n",
            "Parsed documents (first 10): ['FT911-1479', 'FT911-1480', 'FT911-1481', 'FT911-1482', 'FT911-1483', 'FT911-1484', 'FT911-1485', 'FT911-1486', 'FT911-1487', 'FT911-1488']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_6\n",
            "Parsed documents (first 10): ['FT911-1832', 'FT911-1833', 'FT911-1834', 'FT911-1835', 'FT911-1836', 'FT911-1837', 'FT911-1838', 'FT911-1839', 'FT911-1840', 'FT911-1841']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_7\n",
            "Parsed documents (first 10): ['FT911-2212', 'FT911-2213', 'FT911-2214', 'FT911-2215', 'FT911-2216', 'FT911-2217', 'FT911-2218', 'FT911-2219', 'FT911-2220', 'FT911-2221']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_8\n",
            "Parsed documents (first 10): ['FT911-2621', 'FT911-2622', 'FT911-2623', 'FT911-2624', 'FT911-2625', 'FT911-2626', 'FT911-2627', 'FT911-2628', 'FT911-2629', 'FT911-2630']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_9\n",
            "Parsed documents (first 10): ['FT911-2950', 'FT911-2951', 'FT911-2952', 'FT911-2953', 'FT911-2954', 'FT911-2955', 'FT911-2956', 'FT911-2957', 'FT911-2958', 'FT911-2959']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_10\n",
            "Parsed documents (first 10): ['FT911-3323', 'FT911-3324', 'FT911-3325', 'FT911-3326', 'FT911-3327', 'FT911-3328', 'FT911-3329', 'FT911-3330', 'FT911-3331', 'FT911-3332']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_11\n",
            "Parsed documents (first 10): ['FT911-3694', 'FT911-3695', 'FT911-3696', 'FT911-3697', 'FT911-3698', 'FT911-3699', 'FT911-3700', 'FT911-3701', 'FT911-3702', 'FT911-3703']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_12\n",
            "Parsed documents (first 10): ['FT911-4016', 'FT911-4017', 'FT911-4018', 'FT911-4019', 'FT911-4020', 'FT911-4021', 'FT911-4022', 'FT911-4023', 'FT911-4024', 'FT911-4025']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_13\n",
            "Parsed documents (first 10): ['FT911-4366', 'FT911-4367', 'FT911-4368', 'FT911-4369', 'FT911-4370', 'FT911-4371', 'FT911-4372', 'FT911-4373', 'FT911-4374', 'FT911-4375']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_15\n",
            "Parsed documents (first 10): ['FT911-5129', 'FT911-5130', 'FT911-5131', 'FT911-5132', 'FT911-5133', 'FT911-5134', 'FT911-5135', 'FT911-5136', 'FT911-5137', 'FT911-5138']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_14\n",
            "Parsed documents (first 10): ['FT911-4749', 'FT911-4750', 'FT911-4751', 'FT911-4752', 'FT911-4753', 'FT911-4754', 'FT911-4755', 'FT911-4756', 'FT911-4757', 'FT911-4758']\n",
            "Token and Token ID Mapping (First 100):\n",
            "ft\t1\n",
            "correct\t2\n",
            "jubile\t3\n",
            "jet\t4\n",
            "design\t5\n",
            "publish\t6\n",
            "append\t7\n",
            "articl\t8\n",
            "frank\t9\n",
            "fli\t10\n",
            "shout\t11\n",
            "sir\t12\n",
            "whittl\t13\n",
            "maiden\t14\n",
            "flight\t15\n",
            "british\t16\n",
            "repli\t17\n",
            "patent\t18\n",
            "aircraft\t19\n",
            "ga\t20\n",
            "turbin\t21\n",
            "bloodi\t22\n",
            "wasn\t23\n",
            "year\t24\n",
            "ago\t25\n",
            "yesterday\t26\n",
            "made\t27\n",
            "minut\t28\n",
            "raf\t29\n",
            "cranwel\t30\n",
            "lincolnshir\t31\n",
            "celebr\t32\n",
            "event\t33\n",
            "mr\t34\n",
            "eric\t35\n",
            "winkl\t36\n",
            "brown\t37\n",
            "test\t38\n",
            "pilot\t39\n",
            "prototyp\t40\n",
            "gloster\t41\n",
            "geoffrey\t42\n",
            "bone\t43\n",
            "engin\t44\n",
            "charl\t45\n",
            "mcclure\t46\n",
            "return\t47\n",
            "front\t48\n",
            "restor\t49\n",
            "meteor\t50\n",
            "nf\t51\n",
            "unabl\t52\n",
            "attend\t53\n",
            "ill\t54\n",
            "health\t55\n",
            "heinkel\t56\n",
            "august\t57\n",
            "month\t58\n",
            "pictur\t59\n",
            "page\t60\n",
            "issu\t61\n",
            "tuesday\t62\n",
            "bournemouth\t63\n",
            "airport\t64\n",
            "state\t65\n",
            "caption\t66\n",
            "london\t67\n",
            "photograph\t68\n",
            "omit\t69\n",
            "uk\t70\n",
            "compani\t71\n",
            "news\t72\n",
            "geevor\t73\n",
            "merger\t74\n",
            "hit\t75\n",
            "rock\t76\n",
            "pre\t77\n",
            "condit\t78\n",
            "kenneth\t79\n",
            "good\t80\n",
            "mine\t81\n",
            "correspond\t82\n",
            "group\t83\n",
            "fight\t84\n",
            "surviv\t85\n",
            "canadian\t86\n",
            "imperi\t87\n",
            "bank\t88\n",
            "commerc\t89\n",
            "call\t90\n",
            "pound\t91\n",
            "loan\t92\n",
            "extraordinari\t93\n",
            "circumst\t94\n",
            "januari\t95\n",
            "suffer\t96\n",
            "set\t97\n",
            "back\t98\n",
            "propos\t99\n",
            "european\t100\n",
            "The parsed data has been saved to /content/drive/MyDrive/InfoRetrieval/parser_output.txt\n",
            "Sample document IDs: [('FT911-1', 1), ('FT911-2', 2), ('FT911-3', 3), ('FT911-4', 4), ('FT911-5', 5), ('FT911-6', 6), ('FT911-7', 7), ('FT911-8', 8), ('FT911-9', 9), ('FT911-10', 10)]\n",
            "Sample tokens: [('ft', 1), ('correct', 2), ('jubile', 3), ('jet', 4), ('design', 5), ('publish', 6), ('append', 7), ('articl', 8), ('frank', 9), ('fli', 10), ('shout', 11), ('sir', 12), ('whittl', 13), ('maiden', 14), ('flight', 15), ('british', 16), ('repli', 17), ('patent', 18), ('aircraft', 19), ('ga', 20)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Z_7h8sBj1Lo",
        "outputId": "5287020e-f171-4da7-af10-0bd51a5912e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load stopwords\n",
        "stopwords_path = \"/content/drive/MyDrive/InfoRetrieval/stopwordlist.txt\"\n",
        "with open(stopwords_path, 'r') as f:\n",
        "    stopwords = set(f.read().split())\n",
        "    print(stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xvKEOZRj3SF",
        "outputId": "ae661afc-7f42-48f1-d47e-9fdc073bedf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'per', 'wherever', 'h', 'been', 'asking', 'you', 'mainly', 'usually', 'thanks', 'o', 'course', 'et', 'may', 'at', 'neither', 'perhaps', 'allow', 's', 'name', 'rather', 'anywhere', 'whence', 'elsewhere', 'aside', 'indicate', 'mostly', 'everywhere', 'right', 'qv', 'possible', 'otherwise', 'once', 'ex', 'just', 'always', 'unless', 'appreciate', 'little', 'consider', 'tell', 'thanx', 'themselves', 'went', 'until', 'must', 'followed', 'obviously', 'thus', 'ever', 'though', 'two', 'j', 'am', 'plus', 'able', 'c', 'to', 'ltd', 'mean', 'trying', 'sup', 'hereafter', 'besides', 'while', 'considering', 'rd', 'yet', 'see', 'whether', 'vs', 'its', 'her', 'exactly', 'que', 'want', 'x', 'own', 'on', 'consequently', 'doing', 'th', 'looks', 'ourselves', 'about', 'go', 'me', 'y', 'them', 't', 'anyhow', 'concerning', 'cause', 'these', 'yourself', 'm', 'last', 'if', 'wish', 'really', 'k', 'indicated', 'got', 'whatever', 'thorough', 'n', 'say', 'very', 'okay', 'myself', 'sensible', 'seeing', 'what', 'often', 'their', 'so', 'below', 'gives', 'hither', 'itself', 'else', 'near', 'nowhere', 'serious', 'appropriate', 'awfully', 'wonder', 'away', 'or', 'hereby', 'old', 'fifth', 'novel', 'outside', 'sorry', 'becoming', 'was', 'apart', 'sometime', 'inc', 'clearly', 'anybody', 'u', 'nearly', 'seeming', 'willing', 'w', 'anyways', 'actually', 'some', 'unlikely', 'p', 'ours', 'without', 'during', 'came', 'within', 'has', 'far', 'enough', 'relatively', 'indeed', 'nobody', 'thank', 'anyway', 'known', 'too', 'better', 'behind', 'forth', 'tried', 'because', 'nor', 'then', 'eg', 'various', 'r', 'an', 'we', 'than', 'most', 'certain', 'placed', 'taken', 'available', 'everyone', 'such', 'now', 'inner', 'here', 'although', 'and', 'furthermore', 'hers', 'my', 'third', 'wants', 'after', 'downwards', 'can', 'shall', 'all', 'few', 'theres', 'getting', 'q', 'wherein', 'normally', 'same', 'thats', 'sub', 'non', 'secondly', 'who', 'thence', 'since', 'ie', 'particularly', 'specified', 'gone', 'uucp', 'corresponding', 'well', 'best', 'each', 'this', 'next', 'those', 'goes', 'zero', 'his', 'latterly', 'tries', 'regards', 'yourselves', 'beyond', 'howbeit', 'were', 'between', 'namely', 'inasmuch', 'others', 'especially', 'ignored', 'yes', 'out', 'therefore', 'uses', 'formerly', 'other', 'throughout', 'none', 'whereupon', 'provides', 'co', 'self', 'thoroughly', 'meanwhile', 'believe', 'liked', 'hardly', 'everybody', 'seems', 'upon', 'either', 'indicates', 'the', 'moreover', 'still', 'particular', 'off', 'over', 'afterwards', 'regardless', 'specify', 'whenever', 'd', 'later', 'above', 'that', 'follows', 'etc', 'whoever', 'is', 'soon', 'among', 'do', 'specifying', 'kept', 'hereupon', 'even', 'our', 'for', 'former', 'described', 'theirs', 'allows', 'before', 'accordingly', 'not', 'first', 'anyone', 'said', 'up', 'likely', 'us', 'v', 'around', 'unto', 'think', 'are', 'ought', 'six', 'oh', 'lately', 'overall', 'somewhat', 'with', 'thereby', 'will', 'something', 'thereafter', 'could', 'by', 'she', 'in', 'never', 'seriously', 'almost', 'welcome', 'keep', 'useful', 'any', 'saw', 'nine', 'unfortunately', 'took', 'somebody', 'be', 'being', 'cant', 'l', 'looking', 'many', 'causes', 'gotten', 'ones', 'of', 'from', 'both', 'help', 'following', 'get', 'three', 'five', 'gets', 'they', 'much', 'further', 'selves', 'inward', 'down', 'whereafter', 'sometimes', 'maybe', 'seven', 'un', 'having', 'already', 'yours', 'z', 'whom', 'only', 'try', 'amongst', 'nd', 'no', 'take', 'i', 'currently', 'did', 'quite', 'several', 'necessary', 'becomes', 'lest', 'someone', 'had', 'immediate', 'value', 'let', 'hi', 'whereby', 'use', 'less', 'going', 'greetings', 'another', 'hence', 'where', 'despite', 'beforehand', 'against', 'sent', 'seemed', 'know', 'reasonably', 'used', 'through', 'might', 'given', 'there', 'definitely', 'also', 'would', 'happens', 'therein', 'contains', 'entirely', 'whole', 'according', 'certainly', 'contain', 'how', 'different', 'seen', 'himself', 'into', 'however', 'but', 'towards', 'whose', 'toward', 'changes', 'have', 'respectively', 'presumably', 'every', 'except', 'tends', 'g', 'regarding', 'nevertheless', 'together', 'whereas', 'keeps', 'should', 'herein', 'more', 'knows', 'least', 'under', 'whither', 'ok', 'become', 'alone', 'b', 'four', 'somehow', 'look', 'which', 'became', 'example', 'associated', 'new', 'along', 'thru', 'needs', 'thereupon', 'second', 'comes', 'a', 'when', 'probably', 'ask', 'truly', 'viz', 'it', 'herself', 'latter', 're', 'eight', 'need', 'hello', 'your', 'e', 'appear', 'using', 'containing', 'sure', 'anything', 'does', 'again', 'brief', 'as', 'way', 'one', 'twice', 'cannot', 'done', 'insofar', 'instead', 'like', 'across', 'come', 'beside', 'saying', 'says', 'he', 'via', 'everything', 'why', 'onto', 'him', 'noone', 'com', 'merely', 'somewhere', 'please', 'seem', 'nothing', 'hopefully', 'edu', 'f'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the stemmer\n",
        "word_stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "zTaX1vi1j6V3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_path = \"/content/drive/MyDrive/InfoRetrieval/ft911/\"\n",
        "processed_docs = {}"
      ],
      "metadata": {
        "id": "0AZ953NEj9Mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process each file in the dataset\n",
        "for file_name in os.listdir(docs_path):\n",
        "    full_file_path = os.path.join(docs_path, file_name)\n",
        "\n",
        "    if os.path.isfile(full_file_path) and file_name.startswith(\"ft911_\"):\n",
        "        print(f\"Checking file: {full_file_path}\")  # Debugging line\n",
        "        with open(full_file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "            file_content = file.read()\n",
        "\n",
        "            # Parse TREC documents\n",
        "            parsed_docs = {}\n",
        "            current_doc_lines = []\n",
        "            current_doc_id = None\n",
        "            for line in file_content.splitlines():\n",
        "                if line.startswith(\"<DOCNO>\"):\n",
        "                    if current_doc_id is not None:\n",
        "                        parsed_docs[current_doc_id] = \" \".join(current_doc_lines)\n",
        "                    current_doc_id = line.replace(\"<DOCNO>\", \"\").replace(\"</DOCNO>\", \"\").strip()\n",
        "                    current_doc_lines = []\n",
        "                elif not line.startswith(\"<\") and current_doc_id:\n",
        "                    current_doc_lines.append(line.strip())\n",
        "            if current_doc_id is not None:\n",
        "                parsed_docs[current_doc_id] = \" \".join(current_doc_lines)\n",
        "\n",
        "            print(f\"Parsed documents (first 10): {list(parsed_docs.keys())[:10]}\")  # Debugging line\n",
        "\n",
        "            # Process text\n",
        "            for doc_id, doc_text in parsed_docs.items():\n",
        "                text = doc_text.lower()\n",
        "                text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
        "                tokens = nltk.word_tokenize(text)\n",
        "                tokens = [word for word in tokens if word.isalpha()]\n",
        "                filtered_tokens = [word for word in tokens if word not in stopwords]\n",
        "                stemmed_tokens = [word_stemmer.stem(word) for word in filtered_tokens]\n",
        "                processed_docs[doc_id] = stemmed_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCw2em3FkCrh",
        "outputId": "4d3de0f9-48d9-41cf-c951-3d83c5e62350"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_1\n",
            "Parsed documents (first 10): ['FT911-1', 'FT911-2', 'FT911-3', 'FT911-4', 'FT911-5', 'FT911-6', 'FT911-7', 'FT911-8', 'FT911-9', 'FT911-10']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_2\n",
            "Parsed documents (first 10): ['FT911-376', 'FT911-377', 'FT911-378', 'FT911-379', 'FT911-380', 'FT911-381', 'FT911-382', 'FT911-383', 'FT911-384', 'FT911-385']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_3\n",
            "Parsed documents (first 10): ['FT911-722', 'FT911-723', 'FT911-724', 'FT911-725', 'FT911-726', 'FT911-727', 'FT911-728', 'FT911-729', 'FT911-730', 'FT911-731']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_4\n",
            "Parsed documents (first 10): ['FT911-1099', 'FT911-1100', 'FT911-1101', 'FT911-1102', 'FT911-1103', 'FT911-1104', 'FT911-1105', 'FT911-1106', 'FT911-1107', 'FT911-1108']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_5\n",
            "Parsed documents (first 10): ['FT911-1479', 'FT911-1480', 'FT911-1481', 'FT911-1482', 'FT911-1483', 'FT911-1484', 'FT911-1485', 'FT911-1486', 'FT911-1487', 'FT911-1488']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_6\n",
            "Parsed documents (first 10): ['FT911-1832', 'FT911-1833', 'FT911-1834', 'FT911-1835', 'FT911-1836', 'FT911-1837', 'FT911-1838', 'FT911-1839', 'FT911-1840', 'FT911-1841']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_7\n",
            "Parsed documents (first 10): ['FT911-2212', 'FT911-2213', 'FT911-2214', 'FT911-2215', 'FT911-2216', 'FT911-2217', 'FT911-2218', 'FT911-2219', 'FT911-2220', 'FT911-2221']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_8\n",
            "Parsed documents (first 10): ['FT911-2621', 'FT911-2622', 'FT911-2623', 'FT911-2624', 'FT911-2625', 'FT911-2626', 'FT911-2627', 'FT911-2628', 'FT911-2629', 'FT911-2630']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_9\n",
            "Parsed documents (first 10): ['FT911-2950', 'FT911-2951', 'FT911-2952', 'FT911-2953', 'FT911-2954', 'FT911-2955', 'FT911-2956', 'FT911-2957', 'FT911-2958', 'FT911-2959']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_10\n",
            "Parsed documents (first 10): ['FT911-3323', 'FT911-3324', 'FT911-3325', 'FT911-3326', 'FT911-3327', 'FT911-3328', 'FT911-3329', 'FT911-3330', 'FT911-3331', 'FT911-3332']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_11\n",
            "Parsed documents (first 10): ['FT911-3694', 'FT911-3695', 'FT911-3696', 'FT911-3697', 'FT911-3698', 'FT911-3699', 'FT911-3700', 'FT911-3701', 'FT911-3702', 'FT911-3703']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_12\n",
            "Parsed documents (first 10): ['FT911-4016', 'FT911-4017', 'FT911-4018', 'FT911-4019', 'FT911-4020', 'FT911-4021', 'FT911-4022', 'FT911-4023', 'FT911-4024', 'FT911-4025']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_13\n",
            "Parsed documents (first 10): ['FT911-4366', 'FT911-4367', 'FT911-4368', 'FT911-4369', 'FT911-4370', 'FT911-4371', 'FT911-4372', 'FT911-4373', 'FT911-4374', 'FT911-4375']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_15\n",
            "Parsed documents (first 10): ['FT911-5129', 'FT911-5130', 'FT911-5131', 'FT911-5132', 'FT911-5133', 'FT911-5134', 'FT911-5135', 'FT911-5136', 'FT911-5137', 'FT911-5138']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_14\n",
            "Parsed documents (first 10): ['FT911-4749', 'FT911-4750', 'FT911-4751', 'FT911-4752', 'FT911-4753', 'FT911-4754', 'FT911-4755', 'FT911-4756', 'FT911-4757', 'FT911-4758']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dictionaries mapping words and documents to unique IDs\n",
        "doc_to_id_map = {}\n",
        "word_to_id_map = {}\n",
        "output_lines = []\n",
        "\n",
        "current_doc_id = 1\n",
        "current_word_id = 1\n",
        "\n",
        "for doc_name, words in processed_docs.items():\n",
        "    if doc_name not in doc_to_id_map:\n",
        "        doc_to_id_map[doc_name] = current_doc_id\n",
        "        output_lines.append(f\"{doc_name}\\t{current_doc_id}\")\n",
        "        current_doc_id += 1\n",
        "\n",
        "    for word in words:\n",
        "        if word not in word_to_id_map:\n",
        "            word_to_id_map[word] = current_word_id\n",
        "            output_lines.append(f\"{word}\\t{current_word_id}\")\n",
        "            current_word_id += 1"
      ],
      "metadata": {
        "id": "ll46Tf_wkG0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure consistent word order\n",
        "word_to_id_map = dict(sorted(word_to_id_map.items()))\n",
        "\n",
        "output_file_path = \"/content/drive/MyDrive/InfoRetrieval/parser_output.txt\"\n",
        "with open(output_file_path, \"w\") as output_file:\n",
        "    for token, token_id in list(word_to_id_map.items()):\n",
        "        output_file.write(f\"{token}\\t{token_id}\\n\")\n",
        "\n",
        "print(\"Token and Token ID Mapping (First 100):\")\n",
        "for token, token_id in list(word_to_id_map.items())[:100]:\n",
        "    print(f\"{token}\\t{token_id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJxlA6ZomGXY",
        "outputId": "4f80b0f1-b407-44de-fb99-2e2d29ec4d13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token and Token ID Mapping (First 100):\n",
            "aa\t6530\n",
            "aaa\t13313\n",
            "aachen\t13936\n",
            "aaf\t8268\n",
            "aah\t20455\n",
            "aakvaag\t32703\n",
            "aalborg\t30434\n",
            "aaron\t17933\n",
            "ab\t1016\n",
            "ababa\t22651\n",
            "aback\t13024\n",
            "abalkin\t3974\n",
            "abandon\t390\n",
            "abash\t15264\n",
            "abat\t15396\n",
            "abattoir\t22113\n",
            "abb\t4205\n",
            "abba\t22179\n",
            "abbacchio\t18245\n",
            "abbado\t6922\n",
            "abbatoir\t20349\n",
            "abbey\t11270\n",
            "abbot\t18730\n",
            "abbott\t923\n",
            "abbrevi\t20194\n",
            "abc\t14815\n",
            "abcc\t22751\n",
            "abcd\t30331\n",
            "abci\t27009\n",
            "abdali\t23047\n",
            "abdel\t21093\n",
            "abdelaziz\t24529\n",
            "abdic\t15804\n",
            "abduct\t7269\n",
            "abdul\t6810\n",
            "abdulla\t12153\n",
            "abdullah\t20274\n",
            "abe\t32960\n",
            "abel\t7042\n",
            "abela\t27132\n",
            "abercorn\t24021\n",
            "aberdeen\t4924\n",
            "aberdeenshir\t14726\n",
            "aberforth\t9459\n",
            "abergavenni\t12926\n",
            "aberr\t16785\n",
            "abet\t20983\n",
            "abey\t17054\n",
            "abhor\t2101\n",
            "abhorr\t17326\n",
            "abi\t30652\n",
            "abid\t9704\n",
            "abidin\t28714\n",
            "abil\t1429\n",
            "abingdon\t14899\n",
            "abington\t24389\n",
            "abingworth\t27431\n",
            "abitibi\t20809\n",
            "abitur\t31992\n",
            "abject\t17375\n",
            "abk\t30859\n",
            "ablaz\t18224\n",
            "abli\t15487\n",
            "ablitt\t26515\n",
            "ablut\t10456\n",
            "abn\t13229\n",
            "abnorm\t13927\n",
            "aboard\t18096\n",
            "abod\t29376\n",
            "abol\t28004\n",
            "abolhassan\t19623\n",
            "abolish\t3168\n",
            "abolit\t1725\n",
            "abolitionist\t26881\n",
            "abomin\t32251\n",
            "aborigin\t13858\n",
            "abort\t1682\n",
            "abouloff\t15924\n",
            "abound\t4634\n",
            "abouthow\t23620\n",
            "aboutth\t23185\n",
            "abpi\t31966\n",
            "abraham\t5471\n",
            "abram\t6106\n",
            "abramowitz\t18199\n",
            "abras\t12705\n",
            "abreast\t20229\n",
            "abridg\t7341\n",
            "abroad\t290\n",
            "abrog\t20237\n",
            "abrupt\t2885\n",
            "abruptli\t18040\n",
            "abruzzo\t31574\n",
            "absa\t16978\n",
            "abscond\t31613\n",
            "absenc\t399\n",
            "absent\t4155\n",
            "absente\t12099\n",
            "absentia\t27146\n",
            "absinth\t17797\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create WordDictionary and FileDictionary\n",
        "WordDictionary = {}\n",
        "FileDictionary = {}\n",
        "output_lines = []\n",
        "\n",
        "current_doc_id = 1\n",
        "current_word_id = 1\n",
        "\n",
        "for doc_name, words in processed_docs.items():\n",
        "    if doc_name not in FileDictionary:\n",
        "        FileDictionary[doc_name] = current_doc_id\n",
        "        output_lines.append(f\"{doc_name}\\t{current_doc_id}\")\n",
        "        current_doc_id += 1\n",
        "\n",
        "    for word in words:\n",
        "        if word not in WordDictionary:\n",
        "            WordDictionary[word] = current_word_id\n",
        "            output_lines.append(f\"{word}\\t{current_word_id}\")\n",
        "            current_word_id += 1\n",
        "\n",
        "# Ensure consistent word order\n",
        "WordDictionary = dict(sorted(WordDictionary.items()))\n",
        "FileDictionary = dict(sorted(FileDictionary.items()))\n",
        "\n",
        "# Save WordDictionary to file\n",
        "word_dict_path = \"/content/drive/MyDrive/InfoRetrieval/word_dictionary.txt\"\n",
        "with open(word_dict_path, \"w\") as word_file:\n",
        "    for word, word_id in WordDictionary.items():\n",
        "        word_file.write(f\"{word}\\t{word_id}\\n\")\n",
        "\n",
        "# Save FileDictionary to file\n",
        "file_dict_path = \"/content/drive/MyDrive/InfoRetrieval/file_dictionary.txt\"\n",
        "with open(file_dict_path, \"w\") as file_file:\n",
        "    for file_name, file_id in FileDictionary.items():\n",
        "        file_file.write(f\"{file_name}\\t{file_id}\\n\")\n",
        "\n",
        "print(f\"WordDictionary has been saved to {word_dict_path}\")\n",
        "print(f\"FileDictionary has been saved to {file_dict_path}\")\n",
        "\n",
        "# Print sample output\n",
        "print(\"Token and Token ID Mapping (First 100):\")\n",
        "for token, token_id in list(WordDictionary.items())[:100]:\n",
        "    print(f\"{token}\\t{token_id}\")\n",
        "\n",
        "print(f\"Sample document IDs: {list(FileDictionary.items())[:10]}\")\n",
        "print(f\"Sample tokens: {list(WordDictionary.items())[:20]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1jXDZfpmcW_",
        "outputId": "f816acaf-c920-497e-ce0b-fa78db60204a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WordDictionary has been saved to /content/drive/MyDrive/InfoRetrieval/word_dictionary.txt\n",
            "FileDictionary has been saved to /content/drive/MyDrive/InfoRetrieval/file_dictionary.txt\n",
            "Token and Token ID Mapping (First 100):\n",
            "aa\t6530\n",
            "aaa\t13313\n",
            "aachen\t13936\n",
            "aaf\t8268\n",
            "aah\t20455\n",
            "aakvaag\t32703\n",
            "aalborg\t30434\n",
            "aaron\t17933\n",
            "ab\t1016\n",
            "ababa\t22651\n",
            "aback\t13024\n",
            "abalkin\t3974\n",
            "abandon\t390\n",
            "abash\t15264\n",
            "abat\t15396\n",
            "abattoir\t22113\n",
            "abb\t4205\n",
            "abba\t22179\n",
            "abbacchio\t18245\n",
            "abbado\t6922\n",
            "abbatoir\t20349\n",
            "abbey\t11270\n",
            "abbot\t18730\n",
            "abbott\t923\n",
            "abbrevi\t20194\n",
            "abc\t14815\n",
            "abcc\t22751\n",
            "abcd\t30331\n",
            "abci\t27009\n",
            "abdali\t23047\n",
            "abdel\t21093\n",
            "abdelaziz\t24529\n",
            "abdic\t15804\n",
            "abduct\t7269\n",
            "abdul\t6810\n",
            "abdulla\t12153\n",
            "abdullah\t20274\n",
            "abe\t32960\n",
            "abel\t7042\n",
            "abela\t27132\n",
            "abercorn\t24021\n",
            "aberdeen\t4924\n",
            "aberdeenshir\t14726\n",
            "aberforth\t9459\n",
            "abergavenni\t12926\n",
            "aberr\t16785\n",
            "abet\t20983\n",
            "abey\t17054\n",
            "abhor\t2101\n",
            "abhorr\t17326\n",
            "abi\t30652\n",
            "abid\t9704\n",
            "abidin\t28714\n",
            "abil\t1429\n",
            "abingdon\t14899\n",
            "abington\t24389\n",
            "abingworth\t27431\n",
            "abitibi\t20809\n",
            "abitur\t31992\n",
            "abject\t17375\n",
            "abk\t30859\n",
            "ablaz\t18224\n",
            "abli\t15487\n",
            "ablitt\t26515\n",
            "ablut\t10456\n",
            "abn\t13229\n",
            "abnorm\t13927\n",
            "aboard\t18096\n",
            "abod\t29376\n",
            "abol\t28004\n",
            "abolhassan\t19623\n",
            "abolish\t3168\n",
            "abolit\t1725\n",
            "abolitionist\t26881\n",
            "abomin\t32251\n",
            "aborigin\t13858\n",
            "abort\t1682\n",
            "abouloff\t15924\n",
            "abound\t4634\n",
            "abouthow\t23620\n",
            "aboutth\t23185\n",
            "abpi\t31966\n",
            "abraham\t5471\n",
            "abram\t6106\n",
            "abramowitz\t18199\n",
            "abras\t12705\n",
            "abreast\t20229\n",
            "abridg\t7341\n",
            "abroad\t290\n",
            "abrog\t20237\n",
            "abrupt\t2885\n",
            "abruptli\t18040\n",
            "abruzzo\t31574\n",
            "absa\t16978\n",
            "abscond\t31613\n",
            "absenc\t399\n",
            "absent\t4155\n",
            "absente\t12099\n",
            "absentia\t27146\n",
            "absinth\t17797\n",
            "Sample document IDs: [('FT911-1', 1), ('FT911-10', 10), ('FT911-100', 100), ('FT911-1000', 1000), ('FT911-1001', 1001), ('FT911-1002', 1002), ('FT911-1003', 1003), ('FT911-1004', 1004), ('FT911-1005', 1005), ('FT911-1006', 1006)]\n",
            "Sample tokens: [('aa', 6530), ('aaa', 13313), ('aachen', 13936), ('aaf', 8268), ('aah', 20455), ('aakvaag', 32703), ('aalborg', 30434), ('aaron', 17933), ('ab', 1016), ('ababa', 22651), ('aback', 13024), ('abalkin', 3974), ('abandon', 390), ('abash', 15264), ('abat', 15396), ('abattoir', 22113), ('abb', 4205), ('abba', 22179), ('abbacchio', 18245), ('abbado', 6922)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure consistent word order\n",
        "word_to_id_map = dict(sorted(word_to_id_map.items()))\n",
        "print(\"Token and Token ID Mapping (First 100):\")\n",
        "for token, token_id in list(word_to_id_map.items())[:100]:\n",
        "    print(f\"{token}\\t{token_id}\")\n",
        "\n",
        "output_file_path = \"/content/drive/MyDrive/InfoRetrieval/parser_output.txt\"\n",
        "with open(output_file_path, \"w\") as output_file:\n",
        "    for line in output_lines:\n",
        "        output_file.write(line + \"\\n\")\n",
        "\n",
        "print(f\"The parsed data has been saved to {output_file_path}\")\n",
        "print(f\"Sample document IDs: {list(doc_to_id_map.items())[:10]}\")\n",
        "print(f\"Sample tokens: {list(word_to_id_map.items())[:20]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYxF5pqRkKa3",
        "outputId": "cd759ffa-9645-481c-a105-ea9b2002676c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token and Token ID Mapping (First 100):\n",
            "aa\t6530\n",
            "aaa\t13313\n",
            "aachen\t13936\n",
            "aaf\t8268\n",
            "aah\t20455\n",
            "aakvaag\t32703\n",
            "aalborg\t30434\n",
            "aaron\t17933\n",
            "ab\t1016\n",
            "ababa\t22651\n",
            "aback\t13024\n",
            "abalkin\t3974\n",
            "abandon\t390\n",
            "abash\t15264\n",
            "abat\t15396\n",
            "abattoir\t22113\n",
            "abb\t4205\n",
            "abba\t22179\n",
            "abbacchio\t18245\n",
            "abbado\t6922\n",
            "abbatoir\t20349\n",
            "abbey\t11270\n",
            "abbot\t18730\n",
            "abbott\t923\n",
            "abbrevi\t20194\n",
            "abc\t14815\n",
            "abcc\t22751\n",
            "abcd\t30331\n",
            "abci\t27009\n",
            "abdali\t23047\n",
            "abdel\t21093\n",
            "abdelaziz\t24529\n",
            "abdic\t15804\n",
            "abduct\t7269\n",
            "abdul\t6810\n",
            "abdulla\t12153\n",
            "abdullah\t20274\n",
            "abe\t32960\n",
            "abel\t7042\n",
            "abela\t27132\n",
            "abercorn\t24021\n",
            "aberdeen\t4924\n",
            "aberdeenshir\t14726\n",
            "aberforth\t9459\n",
            "abergavenni\t12926\n",
            "aberr\t16785\n",
            "abet\t20983\n",
            "abey\t17054\n",
            "abhor\t2101\n",
            "abhorr\t17326\n",
            "abi\t30652\n",
            "abid\t9704\n",
            "abidin\t28714\n",
            "abil\t1429\n",
            "abingdon\t14899\n",
            "abington\t24389\n",
            "abingworth\t27431\n",
            "abitibi\t20809\n",
            "abitur\t31992\n",
            "abject\t17375\n",
            "abk\t30859\n",
            "ablaz\t18224\n",
            "abli\t15487\n",
            "ablitt\t26515\n",
            "ablut\t10456\n",
            "abn\t13229\n",
            "abnorm\t13927\n",
            "aboard\t18096\n",
            "abod\t29376\n",
            "abol\t28004\n",
            "abolhassan\t19623\n",
            "abolish\t3168\n",
            "abolit\t1725\n",
            "abolitionist\t26881\n",
            "abomin\t32251\n",
            "aborigin\t13858\n",
            "abort\t1682\n",
            "abouloff\t15924\n",
            "abound\t4634\n",
            "abouthow\t23620\n",
            "aboutth\t23185\n",
            "abpi\t31966\n",
            "abraham\t5471\n",
            "abram\t6106\n",
            "abramowitz\t18199\n",
            "abras\t12705\n",
            "abreast\t20229\n",
            "abridg\t7341\n",
            "abroad\t290\n",
            "abrog\t20237\n",
            "abrupt\t2885\n",
            "abruptli\t18040\n",
            "abruzzo\t31574\n",
            "absa\t16978\n",
            "abscond\t31613\n",
            "absenc\t399\n",
            "absent\t4155\n",
            "absente\t12099\n",
            "absentia\t27146\n",
            "absinth\t17797\n",
            "The parsed data has been saved to /content/drive/MyDrive/InfoRetrieval/parser_output.txt\n",
            "Sample document IDs: [('FT911-1', 1), ('FT911-2', 2), ('FT911-3', 3), ('FT911-4', 4), ('FT911-5', 5), ('FT911-6', 6), ('FT911-7', 7), ('FT911-8', 8), ('FT911-9', 9), ('FT911-10', 10)]\n",
            "Sample tokens: [('aa', 6530), ('aaa', 13313), ('aachen', 13936), ('aaf', 8268), ('aah', 20455), ('aakvaag', 32703), ('aalborg', 30434), ('aaron', 17933), ('ab', 1016), ('ababa', 22651), ('aback', 13024), ('abalkin', 3974), ('abandon', 390), ('abash', 15264), ('abat', 15396), ('abattoir', 22113), ('abb', 4205), ('abba', 22179), ('abbacchio', 18245), ('abbado', 6922)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Load stopwords\n",
        "stopwords_path = \"/content/drive/MyDrive/InfoRetrieval/stopwordlist.txt\"\n",
        "with open(stopwords_path, 'r', encoding='utf-8') as f:\n",
        "    stopwords = set(f.read().split())\n",
        "\n",
        "# Initialize the stemmer\n",
        "word_stemmer = PorterStemmer()\n",
        "\n",
        "docs_path = \"/content/drive/MyDrive/InfoRetrieval/ft911/\"\n",
        "processed_docs = {}\n",
        "\n",
        "# Process each file in the dataset\n",
        "for file_name in os.listdir(docs_path):\n",
        "    full_file_path = os.path.join(docs_path, file_name)\n",
        "\n",
        "    if os.path.isfile(full_file_path) and file_name.startswith(\"ft911_\"):\n",
        "        print(f\"Checking file: {full_file_path}\")  # Debugging line\n",
        "        with open(full_file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "            file_content = file.read()\n",
        "\n",
        "            # Parse TREC documents\n",
        "            parsed_docs = {}\n",
        "            current_doc_lines = []\n",
        "            current_doc_id = None\n",
        "            for line in file_content.splitlines():\n",
        "                if line.startswith(\"<DOCNO>\"):\n",
        "                    if current_doc_id is not None:\n",
        "                        parsed_docs[current_doc_id] = \" \".join(current_doc_lines)\n",
        "                    current_doc_id = line.replace(\"<DOCNO>\", \"\").replace(\"</DOCNO>\", \"\").strip()\n",
        "                    current_doc_lines = []\n",
        "                elif not line.startswith(\"<\") and current_doc_id:\n",
        "                    current_doc_lines.append(line.strip())\n",
        "            if current_doc_id is not None:\n",
        "                parsed_docs[current_doc_id] = \" \".join(current_doc_lines)\n",
        "\n",
        "            print(f\"Parsed documents (first 10): {list(parsed_docs.keys())[:10]}\")  # Debugging line\n",
        "\n",
        "            # Process text\n",
        "            for doc_id, doc_text in parsed_docs.items():\n",
        "                text = doc_text.lower()\n",
        "                text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
        "                tokens = nltk.word_tokenize(text)\n",
        "                tokens = [word for word in tokens if word.isalpha()]\n",
        "                filtered_tokens = [word for word in tokens if word not in stopwords]\n",
        "                stemmed_tokens = [word_stemmer.stem(word) for word in filtered_tokens]\n",
        "                processed_docs[f\"{file_name}-{doc_id}\"] = stemmed_tokens\n",
        "\n",
        "# Create WordDictionary and FileDictionary\n",
        "WordDictionary = {}\n",
        "FileDictionary = {}\n",
        "output_lines = []\n",
        "\n",
        "current_doc_id = 1\n",
        "current_word_id = 1\n",
        "\n",
        "for doc_name, words in processed_docs.items():\n",
        "    if doc_name not in FileDictionary:\n",
        "        FileDictionary[doc_name] = current_doc_id\n",
        "        output_lines.append(f\"{doc_name}\\t{current_doc_id}\")\n",
        "        current_doc_id += 1\n",
        "\n",
        "    for word in words:\n",
        "        if word not in WordDictionary:\n",
        "            WordDictionary[word] = current_word_id\n",
        "            output_lines.append(f\"{word}\\t{current_word_id}\")\n",
        "            current_word_id += 1\n",
        "\n",
        "# Ensure consistent word order\n",
        "WordDictionary = dict(sorted(WordDictionary.items()))\n",
        "FileDictionary = dict(sorted(FileDictionary.items()))\n",
        "\n",
        "# Save WordDictionary to file\n",
        "word_dict_path = \"/content/drive/MyDrive/InfoRetrieval/word_dictionary.txt\"\n",
        "with open(word_dict_path, \"w\") as word_file:\n",
        "    for word, word_id in WordDictionary.items():\n",
        "        word_file.write(f\"{word}\\t{word_id}\\n\")\n",
        "\n",
        "# Save FileDictionary to file\n",
        "file_dict_path = \"/content/drive/MyDrive/InfoRetrieval/file_dictionary.txt\"\n",
        "with open(file_dict_path, \"w\") as file_file:\n",
        "    for file_name, file_id in FileDictionary.items():\n",
        "        file_file.write(f\"{file_name}\\t{file_id}\\n\")\n",
        "\n",
        "print(f\"WordDictionary has been saved to {word_dict_path}\")\n",
        "print(f\"FileDictionary has been saved to {file_dict_path}\")\n",
        "\n",
        "# Print sample output\n",
        "print(\"Token and Token ID Mapping (First 100):\")\n",
        "for token, token_id in list(WordDictionary.items())[:100]:\n",
        "    print(f\"{token}\\t{token_id}\")\n",
        "\n",
        "print(f\"Sample document IDs: {list(FileDictionary.items())[:10]}\")\n",
        "print(f\"Sample tokens: {list(WordDictionary.items())[:20]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxVc6regs0Bw",
        "outputId": "e3d9120d-5ba5-49ca-fc64-4d76555fdd16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_1\n",
            "Parsed documents (first 10): ['FT911-1', 'FT911-2', 'FT911-3', 'FT911-4', 'FT911-5', 'FT911-6', 'FT911-7', 'FT911-8', 'FT911-9', 'FT911-10']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_2\n",
            "Parsed documents (first 10): ['FT911-376', 'FT911-377', 'FT911-378', 'FT911-379', 'FT911-380', 'FT911-381', 'FT911-382', 'FT911-383', 'FT911-384', 'FT911-385']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_3\n",
            "Parsed documents (first 10): ['FT911-722', 'FT911-723', 'FT911-724', 'FT911-725', 'FT911-726', 'FT911-727', 'FT911-728', 'FT911-729', 'FT911-730', 'FT911-731']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_4\n",
            "Parsed documents (first 10): ['FT911-1099', 'FT911-1100', 'FT911-1101', 'FT911-1102', 'FT911-1103', 'FT911-1104', 'FT911-1105', 'FT911-1106', 'FT911-1107', 'FT911-1108']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_5\n",
            "Parsed documents (first 10): ['FT911-1479', 'FT911-1480', 'FT911-1481', 'FT911-1482', 'FT911-1483', 'FT911-1484', 'FT911-1485', 'FT911-1486', 'FT911-1487', 'FT911-1488']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_6\n",
            "Parsed documents (first 10): ['FT911-1832', 'FT911-1833', 'FT911-1834', 'FT911-1835', 'FT911-1836', 'FT911-1837', 'FT911-1838', 'FT911-1839', 'FT911-1840', 'FT911-1841']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_7\n",
            "Parsed documents (first 10): ['FT911-2212', 'FT911-2213', 'FT911-2214', 'FT911-2215', 'FT911-2216', 'FT911-2217', 'FT911-2218', 'FT911-2219', 'FT911-2220', 'FT911-2221']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_8\n",
            "Parsed documents (first 10): ['FT911-2621', 'FT911-2622', 'FT911-2623', 'FT911-2624', 'FT911-2625', 'FT911-2626', 'FT911-2627', 'FT911-2628', 'FT911-2629', 'FT911-2630']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_9\n",
            "Parsed documents (first 10): ['FT911-2950', 'FT911-2951', 'FT911-2952', 'FT911-2953', 'FT911-2954', 'FT911-2955', 'FT911-2956', 'FT911-2957', 'FT911-2958', 'FT911-2959']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_10\n",
            "Parsed documents (first 10): ['FT911-3323', 'FT911-3324', 'FT911-3325', 'FT911-3326', 'FT911-3327', 'FT911-3328', 'FT911-3329', 'FT911-3330', 'FT911-3331', 'FT911-3332']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_11\n",
            "Parsed documents (first 10): ['FT911-3694', 'FT911-3695', 'FT911-3696', 'FT911-3697', 'FT911-3698', 'FT911-3699', 'FT911-3700', 'FT911-3701', 'FT911-3702', 'FT911-3703']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_12\n",
            "Parsed documents (first 10): ['FT911-4016', 'FT911-4017', 'FT911-4018', 'FT911-4019', 'FT911-4020', 'FT911-4021', 'FT911-4022', 'FT911-4023', 'FT911-4024', 'FT911-4025']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_13\n",
            "Parsed documents (first 10): ['FT911-4366', 'FT911-4367', 'FT911-4368', 'FT911-4369', 'FT911-4370', 'FT911-4371', 'FT911-4372', 'FT911-4373', 'FT911-4374', 'FT911-4375']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_15\n",
            "Parsed documents (first 10): ['FT911-5129', 'FT911-5130', 'FT911-5131', 'FT911-5132', 'FT911-5133', 'FT911-5134', 'FT911-5135', 'FT911-5136', 'FT911-5137', 'FT911-5138']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_14\n",
            "Parsed documents (first 10): ['FT911-4749', 'FT911-4750', 'FT911-4751', 'FT911-4752', 'FT911-4753', 'FT911-4754', 'FT911-4755', 'FT911-4756', 'FT911-4757', 'FT911-4758']\n",
            "WordDictionary has been saved to /content/drive/MyDrive/InfoRetrieval/word_dictionary.txt\n",
            "FileDictionary has been saved to /content/drive/MyDrive/InfoRetrieval/file_dictionary.txt\n",
            "Token and Token ID Mapping (First 100):\n",
            "aa\t6530\n",
            "aaa\t13313\n",
            "aachen\t13936\n",
            "aaf\t8268\n",
            "aah\t20455\n",
            "aakvaag\t32703\n",
            "aalborg\t30434\n",
            "aaron\t17933\n",
            "ab\t1016\n",
            "ababa\t22651\n",
            "aback\t13024\n",
            "abalkin\t3974\n",
            "abandon\t390\n",
            "abash\t15264\n",
            "abat\t15396\n",
            "abattoir\t22113\n",
            "abb\t4205\n",
            "abba\t22179\n",
            "abbacchio\t18245\n",
            "abbado\t6922\n",
            "abbatoir\t20349\n",
            "abbey\t11270\n",
            "abbot\t18730\n",
            "abbott\t923\n",
            "abbrevi\t20194\n",
            "abc\t14815\n",
            "abcc\t22751\n",
            "abcd\t30331\n",
            "abci\t27009\n",
            "abdali\t23047\n",
            "abdel\t21093\n",
            "abdelaziz\t24529\n",
            "abdic\t15804\n",
            "abduct\t7269\n",
            "abdul\t6810\n",
            "abdulla\t12153\n",
            "abdullah\t20274\n",
            "abe\t32960\n",
            "abel\t7042\n",
            "abela\t27132\n",
            "abercorn\t24021\n",
            "aberdeen\t4924\n",
            "aberdeenshir\t14726\n",
            "aberforth\t9459\n",
            "abergavenni\t12926\n",
            "aberr\t16785\n",
            "abet\t20983\n",
            "abey\t17054\n",
            "abhor\t2101\n",
            "abhorr\t17326\n",
            "abi\t30652\n",
            "abid\t9704\n",
            "abidin\t28714\n",
            "abil\t1429\n",
            "abingdon\t14899\n",
            "abington\t24389\n",
            "abingworth\t27431\n",
            "abitibi\t20809\n",
            "abitur\t31992\n",
            "abject\t17375\n",
            "abk\t30859\n",
            "ablaz\t18224\n",
            "abli\t15487\n",
            "ablitt\t26515\n",
            "ablut\t10456\n",
            "abn\t13229\n",
            "abnorm\t13927\n",
            "aboard\t18096\n",
            "abod\t29376\n",
            "abol\t28004\n",
            "abolhassan\t19623\n",
            "abolish\t3168\n",
            "abolit\t1725\n",
            "abolitionist\t26881\n",
            "abomin\t32251\n",
            "aborigin\t13858\n",
            "abort\t1682\n",
            "abouloff\t15924\n",
            "abound\t4634\n",
            "abouthow\t23620\n",
            "aboutth\t23185\n",
            "abpi\t31966\n",
            "abraham\t5471\n",
            "abram\t6106\n",
            "abramowitz\t18199\n",
            "abras\t12705\n",
            "abreast\t20229\n",
            "abridg\t7341\n",
            "abroad\t290\n",
            "abrog\t20237\n",
            "abrupt\t2885\n",
            "abruptli\t18040\n",
            "abruzzo\t31574\n",
            "absa\t16978\n",
            "abscond\t31613\n",
            "absenc\t399\n",
            "absent\t4155\n",
            "absente\t12099\n",
            "absentia\t27146\n",
            "absinth\t17797\n",
            "Sample document IDs: [('ft911_1-FT911-1', 1), ('ft911_1-FT911-10', 10), ('ft911_1-FT911-100', 100), ('ft911_1-FT911-101', 101), ('ft911_1-FT911-102', 102), ('ft911_1-FT911-103', 103), ('ft911_1-FT911-104', 104), ('ft911_1-FT911-105', 105), ('ft911_1-FT911-106', 106), ('ft911_1-FT911-107', 107)]\n",
            "Sample tokens: [('aa', 6530), ('aaa', 13313), ('aachen', 13936), ('aaf', 8268), ('aah', 20455), ('aakvaag', 32703), ('aalborg', 30434), ('aaron', 17933), ('ab', 1016), ('ababa', 22651), ('aback', 13024), ('abalkin', 3974), ('abandon', 390), ('abash', 15264), ('abat', 15396), ('abattoir', 22113), ('abb', 4205), ('abba', 22179), ('abbacchio', 18245), ('abbado', 6922)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Load stopwords\n",
        "stopwords_path = \"/content/drive/MyDrive/InfoRetrieval/stopwordlist.txt\"\n",
        "with open(stopwords_path, 'r', encoding='utf-8') as f:\n",
        "    stopwords = set(f.read().split())\n",
        "\n",
        "# Initialize the stemmer\n",
        "word_stemmer = PorterStemmer()\n",
        "\n",
        "docs_path = \"/content/drive/MyDrive/InfoRetrieval/ft911/\"\n",
        "processed_docs = {}\n",
        "\n",
        "# Process each file in the dataset\n",
        "for file_name in os.listdir(docs_path):\n",
        "    full_file_path = os.path.join(docs_path, file_name)\n",
        "\n",
        "    if os.path.isfile(full_file_path) and file_name.startswith(\"ft911_\"):\n",
        "        print(f\"Checking file: {full_file_path}\")  # Debugging line\n",
        "        with open(full_file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "            file_content = file.read()\n",
        "\n",
        "            # Parse TREC documents\n",
        "            parsed_docs = {}\n",
        "            current_doc_lines = []\n",
        "            current_doc_id = None\n",
        "            for line in file_content.splitlines():\n",
        "                if line.startswith(\"<DOCNO>\"):\n",
        "                    if current_doc_id is not None:\n",
        "                        parsed_docs[current_doc_id] = \" \".join(current_doc_lines)\n",
        "                    current_doc_id = line.replace(\"<DOCNO>\", \"\").replace(\"</DOCNO>\", \"\").strip()\n",
        "                    current_doc_lines = []\n",
        "                elif not line.startswith(\"<\") and current_doc_id:\n",
        "                    current_doc_lines.append(line.strip())\n",
        "            if current_doc_id is not None:\n",
        "                parsed_docs[current_doc_id] = \" \".join(current_doc_lines)\n",
        "\n",
        "            print(f\"Parsed documents (first 10): {list(parsed_docs.keys())[:10]}\")  # Debugging line\n",
        "\n",
        "            # Process text\n",
        "            for doc_id, doc_text in parsed_docs.items():\n",
        "                text = doc_text.lower()\n",
        "                text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
        "                tokens = nltk.word_tokenize(text)\n",
        "                tokens = [word for word in tokens if word.isalpha()]\n",
        "                filtered_tokens = [word for word in tokens if word not in stopwords]\n",
        "                stemmed_tokens = [word_stemmer.stem(word) for word in filtered_tokens]\n",
        "                formatted_doc_id = f\"{file_name.replace('_', '-')} {doc_id.split('-')[-1]}\"\n",
        "                processed_docs[formatted_doc_id] = stemmed_tokens\n",
        "\n",
        "# Create WordDictionary and FileDictionary\n",
        "WordDictionary = {}\n",
        "FileDictionary = {}\n",
        "output_lines = []\n",
        "\n",
        "current_doc_id = 1\n",
        "current_word_id = 1\n",
        "\n",
        "for doc_name, words in processed_docs.items():\n",
        "    if doc_name not in FileDictionary:\n",
        "        FileDictionary[doc_name] = current_doc_id\n",
        "        output_lines.append(f\"{doc_name}\\t{current_doc_id}\")\n",
        "        current_doc_id += 1\n",
        "\n",
        "    for word in words:\n",
        "        if word not in WordDictionary:\n",
        "            WordDictionary[word] = current_word_id\n",
        "            output_lines.append(f\"{word}\\t{current_word_id}\")\n",
        "            current_word_id += 1\n",
        "\n",
        "# Ensure consistent word order\n",
        "WordDictionary = dict(sorted(WordDictionary.items()))\n",
        "FileDictionary = dict(sorted(FileDictionary.items()))\n",
        "\n",
        "# Save WordDictionary to file\n",
        "word_dict_path = \"/content/drive/MyDrive/InfoRetrieval/word_dictionary.txt\"\n",
        "with open(word_dict_path, \"w\") as word_file:\n",
        "    for word, word_id in WordDictionary.items():\n",
        "        word_file.write(f\"{word}\\t{word_id}\\n\")\n",
        "\n",
        "# Save FileDictionary to file\n",
        "file_dict_path = \"/content/drive/MyDrive/InfoRetrieval/file_dictionary.txt\"\n",
        "with open(file_dict_path, \"w\") as file_file:\n",
        "    for file_name, file_id in FileDictionary.items():\n",
        "        file_file.write(f\"{file_name}\\t{file_id}\\n\")\n",
        "\n",
        "print(f\"WordDictionary has been saved to {word_dict_path}\")\n",
        "print(f\"FileDictionary has been saved to {file_dict_path}\")\n",
        "\n",
        "# Print sample output\n",
        "print(\"Token and Token ID Mapping (First 100):\")\n",
        "for token, token_id in list(WordDictionary.items())[:100]:\n",
        "    print(f\"{token}\\t{token_id}\")\n",
        "\n",
        "print(f\"Sample document IDs: {list(FileDictionary.items())[:10]}\")\n",
        "print(f\"Sample tokens: {list(WordDictionary.items())[:20]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGSvjnJQu0hB",
        "outputId": "92c5ad46-076e-4226-ade2-ff29aa86cd75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_1\n",
            "Parsed documents (first 10): ['FT911-1', 'FT911-2', 'FT911-3', 'FT911-4', 'FT911-5', 'FT911-6', 'FT911-7', 'FT911-8', 'FT911-9', 'FT911-10']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_2\n",
            "Parsed documents (first 10): ['FT911-376', 'FT911-377', 'FT911-378', 'FT911-379', 'FT911-380', 'FT911-381', 'FT911-382', 'FT911-383', 'FT911-384', 'FT911-385']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_3\n",
            "Parsed documents (first 10): ['FT911-722', 'FT911-723', 'FT911-724', 'FT911-725', 'FT911-726', 'FT911-727', 'FT911-728', 'FT911-729', 'FT911-730', 'FT911-731']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_4\n",
            "Parsed documents (first 10): ['FT911-1099', 'FT911-1100', 'FT911-1101', 'FT911-1102', 'FT911-1103', 'FT911-1104', 'FT911-1105', 'FT911-1106', 'FT911-1107', 'FT911-1108']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_5\n",
            "Parsed documents (first 10): ['FT911-1479', 'FT911-1480', 'FT911-1481', 'FT911-1482', 'FT911-1483', 'FT911-1484', 'FT911-1485', 'FT911-1486', 'FT911-1487', 'FT911-1488']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_6\n",
            "Parsed documents (first 10): ['FT911-1832', 'FT911-1833', 'FT911-1834', 'FT911-1835', 'FT911-1836', 'FT911-1837', 'FT911-1838', 'FT911-1839', 'FT911-1840', 'FT911-1841']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_7\n",
            "Parsed documents (first 10): ['FT911-2212', 'FT911-2213', 'FT911-2214', 'FT911-2215', 'FT911-2216', 'FT911-2217', 'FT911-2218', 'FT911-2219', 'FT911-2220', 'FT911-2221']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_8\n",
            "Parsed documents (first 10): ['FT911-2621', 'FT911-2622', 'FT911-2623', 'FT911-2624', 'FT911-2625', 'FT911-2626', 'FT911-2627', 'FT911-2628', 'FT911-2629', 'FT911-2630']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_9\n",
            "Parsed documents (first 10): ['FT911-2950', 'FT911-2951', 'FT911-2952', 'FT911-2953', 'FT911-2954', 'FT911-2955', 'FT911-2956', 'FT911-2957', 'FT911-2958', 'FT911-2959']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_10\n",
            "Parsed documents (first 10): ['FT911-3323', 'FT911-3324', 'FT911-3325', 'FT911-3326', 'FT911-3327', 'FT911-3328', 'FT911-3329', 'FT911-3330', 'FT911-3331', 'FT911-3332']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_11\n",
            "Parsed documents (first 10): ['FT911-3694', 'FT911-3695', 'FT911-3696', 'FT911-3697', 'FT911-3698', 'FT911-3699', 'FT911-3700', 'FT911-3701', 'FT911-3702', 'FT911-3703']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_12\n",
            "Parsed documents (first 10): ['FT911-4016', 'FT911-4017', 'FT911-4018', 'FT911-4019', 'FT911-4020', 'FT911-4021', 'FT911-4022', 'FT911-4023', 'FT911-4024', 'FT911-4025']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_13\n",
            "Parsed documents (first 10): ['FT911-4366', 'FT911-4367', 'FT911-4368', 'FT911-4369', 'FT911-4370', 'FT911-4371', 'FT911-4372', 'FT911-4373', 'FT911-4374', 'FT911-4375']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_15\n",
            "Parsed documents (first 10): ['FT911-5129', 'FT911-5130', 'FT911-5131', 'FT911-5132', 'FT911-5133', 'FT911-5134', 'FT911-5135', 'FT911-5136', 'FT911-5137', 'FT911-5138']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_14\n",
            "Parsed documents (first 10): ['FT911-4749', 'FT911-4750', 'FT911-4751', 'FT911-4752', 'FT911-4753', 'FT911-4754', 'FT911-4755', 'FT911-4756', 'FT911-4757', 'FT911-4758']\n",
            "WordDictionary has been saved to /content/drive/MyDrive/InfoRetrieval/word_dictionary.txt\n",
            "FileDictionary has been saved to /content/drive/MyDrive/InfoRetrieval/file_dictionary.txt\n",
            "Token and Token ID Mapping (First 100):\n",
            "aa\t6530\n",
            "aaa\t13313\n",
            "aachen\t13936\n",
            "aaf\t8268\n",
            "aah\t20455\n",
            "aakvaag\t32703\n",
            "aalborg\t30434\n",
            "aaron\t17933\n",
            "ab\t1016\n",
            "ababa\t22651\n",
            "aback\t13024\n",
            "abalkin\t3974\n",
            "abandon\t390\n",
            "abash\t15264\n",
            "abat\t15396\n",
            "abattoir\t22113\n",
            "abb\t4205\n",
            "abba\t22179\n",
            "abbacchio\t18245\n",
            "abbado\t6922\n",
            "abbatoir\t20349\n",
            "abbey\t11270\n",
            "abbot\t18730\n",
            "abbott\t923\n",
            "abbrevi\t20194\n",
            "abc\t14815\n",
            "abcc\t22751\n",
            "abcd\t30331\n",
            "abci\t27009\n",
            "abdali\t23047\n",
            "abdel\t21093\n",
            "abdelaziz\t24529\n",
            "abdic\t15804\n",
            "abduct\t7269\n",
            "abdul\t6810\n",
            "abdulla\t12153\n",
            "abdullah\t20274\n",
            "abe\t32960\n",
            "abel\t7042\n",
            "abela\t27132\n",
            "abercorn\t24021\n",
            "aberdeen\t4924\n",
            "aberdeenshir\t14726\n",
            "aberforth\t9459\n",
            "abergavenni\t12926\n",
            "aberr\t16785\n",
            "abet\t20983\n",
            "abey\t17054\n",
            "abhor\t2101\n",
            "abhorr\t17326\n",
            "abi\t30652\n",
            "abid\t9704\n",
            "abidin\t28714\n",
            "abil\t1429\n",
            "abingdon\t14899\n",
            "abington\t24389\n",
            "abingworth\t27431\n",
            "abitibi\t20809\n",
            "abitur\t31992\n",
            "abject\t17375\n",
            "abk\t30859\n",
            "ablaz\t18224\n",
            "abli\t15487\n",
            "ablitt\t26515\n",
            "ablut\t10456\n",
            "abn\t13229\n",
            "abnorm\t13927\n",
            "aboard\t18096\n",
            "abod\t29376\n",
            "abol\t28004\n",
            "abolhassan\t19623\n",
            "abolish\t3168\n",
            "abolit\t1725\n",
            "abolitionist\t26881\n",
            "abomin\t32251\n",
            "aborigin\t13858\n",
            "abort\t1682\n",
            "abouloff\t15924\n",
            "abound\t4634\n",
            "abouthow\t23620\n",
            "aboutth\t23185\n",
            "abpi\t31966\n",
            "abraham\t5471\n",
            "abram\t6106\n",
            "abramowitz\t18199\n",
            "abras\t12705\n",
            "abreast\t20229\n",
            "abridg\t7341\n",
            "abroad\t290\n",
            "abrog\t20237\n",
            "abrupt\t2885\n",
            "abruptli\t18040\n",
            "abruzzo\t31574\n",
            "absa\t16978\n",
            "abscond\t31613\n",
            "absenc\t399\n",
            "absent\t4155\n",
            "absente\t12099\n",
            "absentia\t27146\n",
            "absinth\t17797\n",
            "Sample document IDs: [('ft911-1 1', 1), ('ft911-1 10', 10), ('ft911-1 100', 100), ('ft911-1 101', 101), ('ft911-1 102', 102), ('ft911-1 103', 103), ('ft911-1 104', 104), ('ft911-1 105', 105), ('ft911-1 106', 106), ('ft911-1 107', 107)]\n",
            "Sample tokens: [('aa', 6530), ('aaa', 13313), ('aachen', 13936), ('aaf', 8268), ('aah', 20455), ('aakvaag', 32703), ('aalborg', 30434), ('aaron', 17933), ('ab', 1016), ('ababa', 22651), ('aback', 13024), ('abalkin', 3974), ('abandon', 390), ('abash', 15264), ('abat', 15396), ('abattoir', 22113), ('abb', 4205), ('abba', 22179), ('abbacchio', 18245), ('abbado', 6922)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Load stopwords\n",
        "stopwords_path = \"/content/drive/MyDrive/InfoRetrieval/stopwordlist.txt\"\n",
        "with open(stopwords_path, 'r', encoding='utf-8') as f:\n",
        "    stopwords = set(f.read().split())\n",
        "\n",
        "# Initialize the stemmer\n",
        "word_stemmer = PorterStemmer()\n",
        "\n",
        "docs_path = \"/content/drive/MyDrive/InfoRetrieval/ft911/\"\n",
        "processed_docs = {}\n",
        "\n",
        "doc_id_counter = 1\n",
        "\n",
        "\n",
        "# Process each file in the dataset\n",
        "for file_name in sorted(os.listdir(docs_path)):\n",
        "    full_file_path = os.path.join(docs_path, file_name)\n",
        "\n",
        "    if os.path.isfile(full_file_path) and file_name.startswith(\"ft911_\"):\n",
        "        print(f\"Checking file: {full_file_path}\")  # Debugging line\n",
        "        with open(full_file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "            file_content = file.read()\n",
        "\n",
        "            # Parse TREC documents\n",
        "            parsed_docs = {}\n",
        "            current_doc_lines = []\n",
        "            current_doc_id = None\n",
        "            for line in file_content.splitlines():\n",
        "                if line.startswith(\"<DOCNO>\"):\n",
        "                    if current_doc_id is not None:\n",
        "                        parsed_docs[current_doc_id] = \" \".join(current_doc_lines)\n",
        "                    current_doc_id = line.replace(\"<DOCNO>\", \"\").replace(\"</DOCNO>\", \"\").strip()\n",
        "                    current_doc_lines = []\n",
        "                elif not line.startswith(\"<\") and current_doc_id:\n",
        "                    current_doc_lines.append(line.strip())\n",
        "            if current_doc_id is not None:\n",
        "                parsed_docs[current_doc_id] = \" \".join(current_doc_lines)\n",
        "\n",
        "            print(f\"Parsed documents (first 10): {list(parsed_docs.keys())[:10]}\")  # Debugging line\n",
        "\n",
        "            # Process text\n",
        "            for doc_id, doc_text in sorted(parsed_docs.items()):\n",
        "                text = doc_text.lower()\n",
        "                text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
        "                tokens = nltk.word_tokenize(text)\n",
        "                tokens = [word for word in tokens if word.isalpha()]\n",
        "                filtered_tokens = [word for word in tokens if word not in stopwords]\n",
        "                stemmed_tokens = [word_stemmer.stem(word) for word in filtered_tokens]\n",
        "                formatted_doc_id = f\"{file_name} {doc_id.split('-')[-1]}\"\n",
        "                processed_docs[formatted_doc_id] = stemmed_tokens\n",
        "\n",
        "# Create WordDictionary and FileDictionary\n",
        "WordDictionary = {}\n",
        "FileDictionary = {}\n",
        "output_lines = []\n",
        "\n",
        "current_word_id = 1\n",
        "\n",
        "for doc_name, words in sorted(processed_docs.items()):\n",
        "    if doc_name not in FileDictionary:\n",
        "        FileDictionary[doc_name] = doc_id_counter\n",
        "        output_lines.append(f\"{doc_name}\\t{doc_id_counter}\")\n",
        "        doc_id_counter += 1\n",
        "\n",
        "    for word in words:\n",
        "        if word not in WordDictionary:\n",
        "            WordDictionary[word] = current_word_id\n",
        "            output_lines.append(f\"{word}\\t{current_word_id}\")\n",
        "            current_word_id += 1\n",
        "\n",
        "# Ensure consistent word order\n",
        "WordDictionary = dict(sorted(WordDictionary.items()))\n",
        "FileDictionary = dict(sorted(FileDictionary.items()))\n",
        "\n",
        "# Save WordDictionary to file\n",
        "word_dict_path = \"/content/drive/MyDrive/InfoRetrieval/word_dictionary.txt\"\n",
        "with open(word_dict_path, \"w\") as word_file:\n",
        "    for word, word_id in WordDictionary.items():\n",
        "        word_file.write(f\"{word}\\t{word_id}\\n\")\n",
        "\n",
        "# Save FileDictionary to file\n",
        "file_dict_path = \"/content/drive/MyDrive/InfoRetrieval/file_dictionary.txt\"\n",
        "with open(file_dict_path, \"w\") as file_file:\n",
        "    for file_name, file_id in FileDictionary.items():\n",
        "        file_file.write(f\"{file_name}\\t{file_id}\\n\")\n",
        "\n",
        "print(f\"WordDictionary has been saved to {word_dict_path}\")\n",
        "print(f\"FileDictionary has been saved to {file_dict_path}\")\n",
        "\n",
        "# Print sample output\n",
        "print(\"Token and Token ID Mapping (First 100):\")\n",
        "for token, token_id in list(WordDictionary.items())[:100]:\n",
        "    print(f\"{token}\\t{token_id}\")\n",
        "\n",
        "print(f\"Sample document IDs: {list(FileDictionary.items())[:10]}\")\n",
        "print(f\"Sample tokens: {list(WordDictionary.items())[:20]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUBh05AGwbck",
        "outputId": "d29d13fd-2d0c-4a90-901d-57d4cff98851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_1\n",
            "Parsed documents (first 10): ['FT911-1', 'FT911-2', 'FT911-3', 'FT911-4', 'FT911-5', 'FT911-6', 'FT911-7', 'FT911-8', 'FT911-9', 'FT911-10']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_10\n",
            "Parsed documents (first 10): ['FT911-3323', 'FT911-3324', 'FT911-3325', 'FT911-3326', 'FT911-3327', 'FT911-3328', 'FT911-3329', 'FT911-3330', 'FT911-3331', 'FT911-3332']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_11\n",
            "Parsed documents (first 10): ['FT911-3694', 'FT911-3695', 'FT911-3696', 'FT911-3697', 'FT911-3698', 'FT911-3699', 'FT911-3700', 'FT911-3701', 'FT911-3702', 'FT911-3703']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_12\n",
            "Parsed documents (first 10): ['FT911-4016', 'FT911-4017', 'FT911-4018', 'FT911-4019', 'FT911-4020', 'FT911-4021', 'FT911-4022', 'FT911-4023', 'FT911-4024', 'FT911-4025']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_13\n",
            "Parsed documents (first 10): ['FT911-4366', 'FT911-4367', 'FT911-4368', 'FT911-4369', 'FT911-4370', 'FT911-4371', 'FT911-4372', 'FT911-4373', 'FT911-4374', 'FT911-4375']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_14\n",
            "Parsed documents (first 10): ['FT911-4749', 'FT911-4750', 'FT911-4751', 'FT911-4752', 'FT911-4753', 'FT911-4754', 'FT911-4755', 'FT911-4756', 'FT911-4757', 'FT911-4758']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_15\n",
            "Parsed documents (first 10): ['FT911-5129', 'FT911-5130', 'FT911-5131', 'FT911-5132', 'FT911-5133', 'FT911-5134', 'FT911-5135', 'FT911-5136', 'FT911-5137', 'FT911-5138']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_2\n",
            "Parsed documents (first 10): ['FT911-376', 'FT911-377', 'FT911-378', 'FT911-379', 'FT911-380', 'FT911-381', 'FT911-382', 'FT911-383', 'FT911-384', 'FT911-385']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_3\n",
            "Parsed documents (first 10): ['FT911-722', 'FT911-723', 'FT911-724', 'FT911-725', 'FT911-726', 'FT911-727', 'FT911-728', 'FT911-729', 'FT911-730', 'FT911-731']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_4\n",
            "Parsed documents (first 10): ['FT911-1099', 'FT911-1100', 'FT911-1101', 'FT911-1102', 'FT911-1103', 'FT911-1104', 'FT911-1105', 'FT911-1106', 'FT911-1107', 'FT911-1108']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_5\n",
            "Parsed documents (first 10): ['FT911-1479', 'FT911-1480', 'FT911-1481', 'FT911-1482', 'FT911-1483', 'FT911-1484', 'FT911-1485', 'FT911-1486', 'FT911-1487', 'FT911-1488']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_6\n",
            "Parsed documents (first 10): ['FT911-1832', 'FT911-1833', 'FT911-1834', 'FT911-1835', 'FT911-1836', 'FT911-1837', 'FT911-1838', 'FT911-1839', 'FT911-1840', 'FT911-1841']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_7\n",
            "Parsed documents (first 10): ['FT911-2212', 'FT911-2213', 'FT911-2214', 'FT911-2215', 'FT911-2216', 'FT911-2217', 'FT911-2218', 'FT911-2219', 'FT911-2220', 'FT911-2221']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_8\n",
            "Parsed documents (first 10): ['FT911-2621', 'FT911-2622', 'FT911-2623', 'FT911-2624', 'FT911-2625', 'FT911-2626', 'FT911-2627', 'FT911-2628', 'FT911-2629', 'FT911-2630']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_9\n",
            "Parsed documents (first 10): ['FT911-2950', 'FT911-2951', 'FT911-2952', 'FT911-2953', 'FT911-2954', 'FT911-2955', 'FT911-2956', 'FT911-2957', 'FT911-2958', 'FT911-2959']\n",
            "WordDictionary has been saved to /content/drive/MyDrive/InfoRetrieval/word_dictionary.txt\n",
            "FileDictionary has been saved to /content/drive/MyDrive/InfoRetrieval/file_dictionary.txt\n",
            "Token and Token ID Mapping (First 100):\n",
            "aa\t5542\n",
            "aaa\t24229\n",
            "aachen\t20439\n",
            "aaf\t7625\n",
            "aah\t28473\n",
            "aakvaag\t20650\n",
            "aalborg\t17818\n",
            "aaron\t26803\n",
            "ab\t4195\n",
            "ababa\t29972\n",
            "aback\t20822\n",
            "abalkin\t1117\n",
            "abandon\t1408\n",
            "abash\t25643\n",
            "abat\t14067\n",
            "abattoir\t29601\n",
            "abb\t1742\n",
            "abba\t19835\n",
            "abbacchio\t26983\n",
            "abbado\t6021\n",
            "abbatoir\t21467\n",
            "abbey\t14332\n",
            "abbot\t20704\n",
            "abbott\t3736\n",
            "abbrevi\t10578\n",
            "abc\t8821\n",
            "abcc\t30027\n",
            "abcd\t17613\n",
            "abci\t9549\n",
            "abdali\t30235\n",
            "abdel\t28891\n",
            "abdelaziz\t31393\n",
            "abdic\t24568\n",
            "abduct\t6413\n",
            "abdul\t5892\n",
            "abdulla\t23739\n",
            "abdullah\t15511\n",
            "abe\t21126\n",
            "abel\t6148\n",
            "abela\t9896\n",
            "abercorn\t31032\n",
            "aberdeen\t3026\n",
            "aberdeenshir\t14982\n",
            "aberforth\t17589\n",
            "abergavenni\t24040\n",
            "aberr\t18489\n",
            "abet\t28817\n",
            "abey\t17621\n",
            "abhor\t8237\n",
            "abhorr\t21818\n",
            "abi\t18260\n",
            "abid\t10006\n",
            "abidin\t14196\n",
            "abil\t1673\n",
            "abingdon\t13957\n",
            "abington\t31291\n",
            "abingworth\t10901\n",
            "abitibi\t12448\n",
            "abitur\t20466\n",
            "abject\t26432\n",
            "abk\t18658\n",
            "ablaz\t26971\n",
            "abli\t24375\n",
            "ablitt\t32820\n",
            "ablut\t22868\n",
            "abn\t18887\n",
            "abnorm\t9597\n",
            "aboard\t9780\n",
            "abod\t15567\n",
            "abol\t12578\n",
            "abolhassan\t27919\n",
            "abolish\t6745\n",
            "abolit\t8151\n",
            "abolitionist\t9202\n",
            "abomin\t19847\n",
            "aborigin\t24934\n",
            "abort\t5298\n",
            "abouloff\t24641\n",
            "abound\t2537\n",
            "abouthow\t30718\n",
            "aboutth\t30346\n",
            "abpi\t21826\n",
            "abraham\t3935\n",
            "abram\t4987\n",
            "abramowitz\t26957\n",
            "abras\t20490\n",
            "abreast\t10343\n",
            "abridg\t6520\n",
            "abroad\t344\n",
            "abrog\t28314\n",
            "abrupt\t4555\n",
            "abruptli\t14632\n",
            "abruzzo\t21212\n",
            "absa\t15872\n",
            "abscond\t21278\n",
            "absenc\t1413\n",
            "absent\t1645\n",
            "absente\t12334\n",
            "absentia\t9925\n",
            "absinth\t26700\n",
            "Sample document IDs: [('ft911_1 1', 1), ('ft911_1 10', 2), ('ft911_1 100', 3), ('ft911_1 101', 4), ('ft911_1 102', 5), ('ft911_1 103', 6), ('ft911_1 104', 7), ('ft911_1 105', 8), ('ft911_1 106', 9), ('ft911_1 107', 10)]\n",
            "Sample tokens: [('aa', 5542), ('aaa', 24229), ('aachen', 20439), ('aaf', 7625), ('aah', 28473), ('aakvaag', 20650), ('aalborg', 17818), ('aaron', 26803), ('ab', 4195), ('ababa', 29972), ('aback', 20822), ('abalkin', 1117), ('abandon', 1408), ('abash', 25643), ('abat', 14067), ('abattoir', 29601), ('abb', 1742), ('abba', 19835), ('abbacchio', 26983), ('abbado', 6021)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load stopwords\n",
        "stopwords_path = \"/content/drive/MyDrive/InfoRetrieval/stopwordlist.txt\"\n",
        "with open(stopwords_path, 'r', encoding='utf-8') as f:\n",
        "    stopwords = set(f.read().split())\n",
        "\n",
        "# Initialize the stemmer\n",
        "word_stemmer = PorterStemmer()\n",
        "\n",
        "docs_path = \"/content/drive/MyDrive/InfoRetrieval/ft911/\"\n",
        "processed_docs = {}\n",
        "\n",
        "# Process each file in the dataset\n",
        "for file_name in sorted(os.listdir(docs_path)):  # Ensure correct file order\n",
        "    full_file_path = os.path.join(docs_path, file_name)\n",
        "\n",
        "    if os.path.isfile(full_file_path) and file_name.startswith(\"ft911_\"):\n",
        "        print(f\"Checking file: {full_file_path}\")  # Debugging line\n",
        "        with open(full_file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "            file_content = file.read()\n",
        "\n",
        "            # Parse TREC documents\n",
        "            parsed_docs = {}\n",
        "            current_doc_lines = []\n",
        "            current_doc_id = None\n",
        "            for line in file_content.splitlines():\n",
        "                if line.startswith(\"<DOCNO>\"):\n",
        "                    if current_doc_id is not None:\n",
        "                        parsed_docs[current_doc_id] = \" \".join(current_doc_lines)\n",
        "                    current_doc_id = line.replace(\"<DOCNO>\", \"\").replace(\"</DOCNO>\", \"\").strip()\n",
        "                    current_doc_lines = []\n",
        "                elif not line.startswith(\"<\") and current_doc_id:\n",
        "                    current_doc_lines.append(line.strip())\n",
        "            if current_doc_id is not None:\n",
        "                parsed_docs[current_doc_id] = \" \".join(current_doc_lines)\n",
        "\n",
        "            print(f\"Parsed documents (first 10): {list(parsed_docs.keys())[:10]}\")  # Debugging line\n",
        "\n",
        "            # Process text\n",
        "            doc_counter = 1  # Reset document counter per file\n",
        "            for doc_id, doc_text in sorted(parsed_docs.items(), key=lambda x: int(x[0].split('-')[-1])):\n",
        "                text = doc_text.lower()\n",
        "                text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
        "                tokens = nltk.word_tokenize(text)\n",
        "                tokens = [word for word in tokens if word.isalpha()]\n",
        "                filtered_tokens = [word for word in tokens if word not in stopwords]\n",
        "                stemmed_tokens = [word_stemmer.stem(word) for word in filtered_tokens]\n",
        "                formatted_doc_id = f\"{file_name} {doc_counter}\"  # Ensure sequential numbering\n",
        "                processed_docs[formatted_doc_id] = stemmed_tokens\n",
        "                doc_counter += 1\n",
        "\n",
        "# Create WordDictionary and FileDictionary\n",
        "WordDictionary = {}\n",
        "FileDictionary = {}\n",
        "output_lines = []\n",
        "\n",
        "current_word_id = 1\n",
        "current_doc_id = 1  # Sequential document numbering\n",
        "\n",
        "for doc_name, words in sorted(processed_docs.items(), key=lambda x: (x[0].split()[0], int(x[0].split()[1]))):\n",
        "    if doc_name not in FileDictionary:\n",
        "        FileDictionary[doc_name] = current_doc_id\n",
        "        output_lines.append(f\"{doc_name}\\t{current_doc_id}\")\n",
        "        current_doc_id += 1\n",
        "\n",
        "    for word in words:\n",
        "        if word not in WordDictionary:\n",
        "            WordDictionary[word] = current_word_id\n",
        "            output_lines.append(f\"{word}\\t{current_word_id}\")\n",
        "            current_word_id += 1\n",
        "\n",
        "# Ensure consistent word order\n",
        "WordDictionary = dict(sorted(WordDictionary.items()))\n",
        "FileDictionary = dict(sorted(FileDictionary.items(), key=lambda x: (x[0].split()[0], int(x[0].split()[1]))))\n",
        "\n",
        "# Save WordDictionary to file\n",
        "word_dict_path = \"/content/drive/MyDrive/InfoRetrieval/word_dictionary.txt\"\n",
        "with open(word_dict_path, \"w\") as word_file:\n",
        "    for word, word_id in WordDictionary.items():\n",
        "        word_file.write(f\"{word}\\t{word_id}\\n\")\n",
        "\n",
        "# Save FileDictionary to file\n",
        "file_dict_path = \"/content/drive/MyDrive/InfoRetrieval/file_dictionary.txt\"\n",
        "with open(file_dict_path, \"w\") as file_file:\n",
        "    for file_name, file_id in FileDictionary.items():\n",
        "        file_file.write(f\"{file_name}\\t{file_id}\\n\")\n",
        "\n",
        "print(f\"WordDictionary has been saved to {word_dict_path}\")\n",
        "print(f\"FileDictionary has been saved to {file_dict_path}\")\n",
        "\n",
        "# Print sample output\n",
        "print(\"Token and Token ID Mapping (First 100):\")\n",
        "for token, token_id in list(WordDictionary.items())[:100]:\n",
        "    print(f\"{token}\\t{token_id}\")\n",
        "\n",
        "print(f\"Sample document IDs: {list(FileDictionary.items())[:10]}\")\n",
        "print(f\"Sample tokens: {list(WordDictionary.items())[:20]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 789
        },
        "id": "JaB8IA97x0t3",
        "outputId": "4013944e-3731-4170-ddae-6663fc271df3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_1\n",
            "Parsed documents (first 10): ['FT911-1', 'FT911-2', 'FT911-3', 'FT911-4', 'FT911-5', 'FT911-6', 'FT911-7', 'FT911-8', 'FT911-9', 'FT911-10']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b51c81721fa4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[^a-zA-Z\\s]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m                 \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mfiltered_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     return [\n\u001b[1;32m    144\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load stopwords\n",
        "stopwords_path = \"/content/drive/MyDrive/InfoRetrieval/stopwordlist.txt\"\n",
        "with open(stopwords_path, 'r', encoding='utf-8') as f:\n",
        "    stopwords = set(f.read().split())\n",
        "\n",
        "# Initialize the stemmer\n",
        "word_stemmer = PorterStemmer()\n",
        "\n",
        "docs_path = \"/content/drive/MyDrive/InfoRetrieval/ft911/\"\n",
        "processed_docs = {}\n",
        "FTFileMapping = {}  # Mapping: file_name -> list of DOCNOs\n",
        "FileDictionary = {}  # Mapping: (file_name, local_doc_num) -> global_doc_id\n",
        "DocumentToFileMapping = []  # NEW: list of (file_name, DOCNO)\n",
        "\n",
        "# Sort files numerically based on suffix\n",
        "file_list = sorted(\n",
        "    [f for f in os.listdir(docs_path) if f.startswith(\"ft911_\")],\n",
        "    key=lambda x: int(re.findall(r'\\d+', x)[0])\n",
        ")\n",
        "\n",
        "# Process each file\n",
        "global_doc_id = 1\n",
        "for file_name in file_list:\n",
        "    full_file_path = os.path.join(docs_path, file_name)\n",
        "\n",
        "    if os.path.isfile(full_file_path):\n",
        "        print(f\"Checking file: {full_file_path}\")\n",
        "        with open(full_file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "            file_content = file.read()\n",
        "\n",
        "            # Parse TREC documents\n",
        "            parsed_docs = {}\n",
        "            current_doc_lines = []\n",
        "            current_doc_id = None\n",
        "            for line in file_content.splitlines():\n",
        "                if line.startswith(\"<DOCNO>\"):\n",
        "                    if current_doc_id is not None:\n",
        "                        parsed_docs[current_doc_id] = \" \".join(current_doc_lines)\n",
        "                    current_doc_id = line.replace(\"<DOCNO>\", \"\").replace(\"</DOCNO>\", \"\").strip()\n",
        "                    current_doc_lines = []\n",
        "                elif not line.startswith(\"<\") and current_doc_id:\n",
        "                    current_doc_lines.append(line.strip())\n",
        "            if current_doc_id is not None:\n",
        "                parsed_docs[current_doc_id] = \" \".join(current_doc_lines)\n",
        "\n",
        "            print(f\"Parsed documents (first 10): {list(parsed_docs.keys())[:10]}\")\n",
        "\n",
        "            # Save file -> doc list\n",
        "            FTFileMapping[file_name] = list(parsed_docs.keys())\n",
        "\n",
        "            # Save (file_name, DOCNO) mapping\n",
        "            for docno in parsed_docs.keys():\n",
        "                DocumentToFileMapping.append((file_name, docno))\n",
        "\n",
        "            # Assign global and local doc IDs\n",
        "            local_id = 1\n",
        "            for doc_id in parsed_docs:\n",
        "                FileDictionary[(file_name, local_id)] = global_doc_id\n",
        "                local_id += 1\n",
        "                global_doc_id += 1\n",
        "\n",
        "            # Process text\n",
        "            for doc_id, doc_text in sorted(parsed_docs.items()):\n",
        "                text = doc_text.lower()\n",
        "                text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
        "                tokens = nltk.word_tokenize(text)\n",
        "                tokens = [word for word in tokens if word.isalpha()]\n",
        "                filtered_tokens = [word for word in tokens if word not in stopwords]\n",
        "                stemmed_tokens = [word_stemmer.stem(word) for word in filtered_tokens]\n",
        "                processed_docs[doc_id] = stemmed_tokens\n",
        "\n",
        "# Create WordDictionary\n",
        "WordDictionary = {}\n",
        "current_word_id = 1\n",
        "for words in processed_docs.values():\n",
        "    for word in words:\n",
        "        if word not in WordDictionary:\n",
        "            WordDictionary[word] = current_word_id\n",
        "            current_word_id += 1\n",
        "\n",
        "# Sort dictionaries\n",
        "WordDictionary = dict(sorted(WordDictionary.items()))\n",
        "FileDictionary = dict(sorted(FileDictionary.items()))\n",
        "\n",
        "# Save WordDictionary\n",
        "word_dict_path = \"/content/drive/MyDrive/InfoRetrieval/word_dictionary.txt\"\n",
        "with open(word_dict_path, \"w\") as word_file:\n",
        "    for word, word_id in WordDictionary.items():\n",
        "        word_file.write(f\"{word}\\t{word_id}\\n\")\n",
        "\n",
        "# Save FileDictionary (filename + local doc number)\n",
        "file_dict_path = \"/content/drive/MyDrive/InfoRetrieval/file_dictionary.txt\"\n",
        "with open(file_dict_path, \"w\") as file_file:\n",
        "    for (file_name, local_doc_num), global_id in FileDictionary.items():\n",
        "        file_file.write(f\"{file_name}\\t{local_doc_num}\\t{global_id}\\n\")\n",
        "\n",
        "# Save FTFileMapping (file -> list of DOCNOs)\n",
        "ft_map_path = \"/content/drive/MyDrive/InfoRetrieval/ft_file_mapping.txt\"\n",
        "with open(ft_map_path, \"w\") as map_file:\n",
        "    for file, doc_ids in FTFileMapping.items():\n",
        "        map_file.write(f\"{file}:\\t{', '.join(doc_ids)}\\n\")\n",
        "\n",
        "# Save DocumentToFileMapping (each line: file_name<TAB>DOCNO)\n",
        "doc_file_map_path = \"/content/drive/MyDrive/InfoRetrieval/doc_to_file_mapping.txt\"\n",
        "with open(doc_file_map_path, \"w\") as out_file:\n",
        "    for file_name, docno in DocumentToFileMapping:\n",
        "        docno=docno.split(\"-\")[-1]\n",
        "        out_file.write(f\"{file_name}\\t{docno}\\n\")\n",
        "\n",
        "\n",
        "# Logs\n",
        "print(f\"WordDictionary saved to {word_dict_path}\")\n",
        "print(f\"FileDictionary saved to {file_dict_path}\")\n",
        "print(f\"FTFileMapping saved to {ft_map_path}\")\n",
        "print(f\"DocumentToFileMapping saved to {doc_file_map_path}\")\n",
        "\n",
        "print(\"Token and Token ID Mapping (First 100):\")\n",
        "for token, token_id in list(WordDictionary.items())[:100]:\n",
        "    print(f\"{token}\\t{token_id}\")\n",
        "\n",
        "print(\"DocumentToFileMapping (First 1000):\")\n",
        "for entry in DocumentToFileMapping[721:1100]:\n",
        "    print(entry)\n"
      ],
      "metadata": {
        "id": "h37NBxYLzUKe",
        "outputId": "d4efce1a-4313-4935-c126-47abc7628c27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_1\n",
            "Parsed documents (first 10): ['FT911-1', 'FT911-2', 'FT911-3', 'FT911-4', 'FT911-5', 'FT911-6', 'FT911-7', 'FT911-8', 'FT911-9', 'FT911-10']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_2\n",
            "Parsed documents (first 10): ['FT911-376', 'FT911-377', 'FT911-378', 'FT911-379', 'FT911-380', 'FT911-381', 'FT911-382', 'FT911-383', 'FT911-384', 'FT911-385']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_3\n",
            "Parsed documents (first 10): ['FT911-722', 'FT911-723', 'FT911-724', 'FT911-725', 'FT911-726', 'FT911-727', 'FT911-728', 'FT911-729', 'FT911-730', 'FT911-731']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_4\n",
            "Parsed documents (first 10): ['FT911-1099', 'FT911-1100', 'FT911-1101', 'FT911-1102', 'FT911-1103', 'FT911-1104', 'FT911-1105', 'FT911-1106', 'FT911-1107', 'FT911-1108']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_5\n",
            "Parsed documents (first 10): ['FT911-1479', 'FT911-1480', 'FT911-1481', 'FT911-1482', 'FT911-1483', 'FT911-1484', 'FT911-1485', 'FT911-1486', 'FT911-1487', 'FT911-1488']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_6\n",
            "Parsed documents (first 10): ['FT911-1832', 'FT911-1833', 'FT911-1834', 'FT911-1835', 'FT911-1836', 'FT911-1837', 'FT911-1838', 'FT911-1839', 'FT911-1840', 'FT911-1841']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_7\n",
            "Parsed documents (first 10): ['FT911-2212', 'FT911-2213', 'FT911-2214', 'FT911-2215', 'FT911-2216', 'FT911-2217', 'FT911-2218', 'FT911-2219', 'FT911-2220', 'FT911-2221']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_8\n",
            "Parsed documents (first 10): ['FT911-2621', 'FT911-2622', 'FT911-2623', 'FT911-2624', 'FT911-2625', 'FT911-2626', 'FT911-2627', 'FT911-2628', 'FT911-2629', 'FT911-2630']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_9\n",
            "Parsed documents (first 10): ['FT911-2950', 'FT911-2951', 'FT911-2952', 'FT911-2953', 'FT911-2954', 'FT911-2955', 'FT911-2956', 'FT911-2957', 'FT911-2958', 'FT911-2959']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_10\n",
            "Parsed documents (first 10): ['FT911-3323', 'FT911-3324', 'FT911-3325', 'FT911-3326', 'FT911-3327', 'FT911-3328', 'FT911-3329', 'FT911-3330', 'FT911-3331', 'FT911-3332']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_11\n",
            "Parsed documents (first 10): ['FT911-3694', 'FT911-3695', 'FT911-3696', 'FT911-3697', 'FT911-3698', 'FT911-3699', 'FT911-3700', 'FT911-3701', 'FT911-3702', 'FT911-3703']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_12\n",
            "Parsed documents (first 10): ['FT911-4016', 'FT911-4017', 'FT911-4018', 'FT911-4019', 'FT911-4020', 'FT911-4021', 'FT911-4022', 'FT911-4023', 'FT911-4024', 'FT911-4025']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_13\n",
            "Parsed documents (first 10): ['FT911-4366', 'FT911-4367', 'FT911-4368', 'FT911-4369', 'FT911-4370', 'FT911-4371', 'FT911-4372', 'FT911-4373', 'FT911-4374', 'FT911-4375']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_15\n",
            "Parsed documents (first 10): ['FT911-5129', 'FT911-5130', 'FT911-5131', 'FT911-5132', 'FT911-5133', 'FT911-5134', 'FT911-5135', 'FT911-5136', 'FT911-5137', 'FT911-5138']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_14\n",
            "Parsed documents (first 10): ['FT911-4749', 'FT911-4750', 'FT911-4751', 'FT911-4752', 'FT911-4753', 'FT911-4754', 'FT911-4755', 'FT911-4756', 'FT911-4757', 'FT911-4758']\n",
            "WordDictionary saved to /content/drive/MyDrive/InfoRetrieval/word_dictionary.txt\n",
            "FileDictionary saved to /content/drive/MyDrive/InfoRetrieval/file_dictionary.txt\n",
            "FTFileMapping saved to /content/drive/MyDrive/InfoRetrieval/ft_file_mapping.txt\n",
            "DocumentToFileMapping saved to /content/drive/MyDrive/InfoRetrieval/doc_to_file_mapping.txt\n",
            "Token and Token ID Mapping (First 100):\n",
            "aa\t5542\n",
            "aaa\t13313\n",
            "aachen\t14777\n",
            "aaf\t7625\n",
            "aah\t20455\n",
            "aakvaag\t32703\n",
            "aalborg\t30434\n",
            "aaron\t17933\n",
            "ab\t4195\n",
            "ababa\t22651\n",
            "aback\t13024\n",
            "abalkin\t1117\n",
            "abandon\t1408\n",
            "abash\t15986\n",
            "abat\t13479\n",
            "abattoir\t22113\n",
            "abb\t1742\n",
            "abba\t22179\n",
            "abbacchio\t18245\n",
            "abbado\t6021\n",
            "abbatoir\t20349\n",
            "abbey\t11270\n",
            "abbot\t18730\n",
            "abbott\t3736\n",
            "abbrevi\t20194\n",
            "abc\t15580\n",
            "abcc\t22751\n",
            "abcd\t30331\n",
            "abci\t27009\n",
            "abdali\t23047\n",
            "abdel\t21093\n",
            "abdelaziz\t24529\n",
            "abdic\t13992\n",
            "abduct\t6413\n",
            "abdul\t5892\n",
            "abdulla\t12153\n",
            "abdullah\t20274\n",
            "abe\t32960\n",
            "abel\t6148\n",
            "abela\t27132\n",
            "abercorn\t24021\n",
            "aberdeen\t3026\n",
            "aberdeenshir\t15507\n",
            "aberforth\t9459\n",
            "abergavenni\t12926\n",
            "aberr\t16785\n",
            "abet\t20983\n",
            "abey\t17054\n",
            "abhor\t8237\n",
            "abhorr\t17326\n",
            "abi\t30652\n",
            "abid\t9704\n",
            "abidin\t28714\n",
            "abil\t1673\n",
            "abingdon\t15659\n",
            "abington\t24389\n",
            "abingworth\t27431\n",
            "abitibi\t20809\n",
            "abitur\t31992\n",
            "abject\t17375\n",
            "abk\t30859\n",
            "ablaz\t18224\n",
            "abli\t13581\n",
            "ablitt\t26515\n",
            "ablut\t10456\n",
            "abn\t13229\n",
            "abnorm\t14074\n",
            "aboard\t18096\n",
            "abod\t29376\n",
            "abol\t28004\n",
            "abolhassan\t19623\n",
            "abolish\t6745\n",
            "abolit\t8151\n",
            "abolitionist\t26881\n",
            "abomin\t32251\n",
            "aborigin\t14709\n",
            "abort\t5298\n",
            "abouloff\t14142\n",
            "abound\t2537\n",
            "abouthow\t23620\n",
            "aboutth\t23185\n",
            "abpi\t31966\n",
            "abraham\t3935\n",
            "abram\t4987\n",
            "abramowitz\t18199\n",
            "abras\t12705\n",
            "abreast\t20229\n",
            "abridg\t6520\n",
            "abroad\t344\n",
            "abrog\t20237\n",
            "abrupt\t4555\n",
            "abruptli\t18040\n",
            "abruzzo\t31574\n",
            "absa\t16978\n",
            "abscond\t31613\n",
            "absenc\t1413\n",
            "absent\t1645\n",
            "absente\t12099\n",
            "absentia\t27146\n",
            "absinth\t17797\n",
            "DocumentToFileMapping (First 1000):\n",
            "('ft911_3', 'FT911-722')\n",
            "('ft911_3', 'FT911-723')\n",
            "('ft911_3', 'FT911-724')\n",
            "('ft911_3', 'FT911-725')\n",
            "('ft911_3', 'FT911-726')\n",
            "('ft911_3', 'FT911-727')\n",
            "('ft911_3', 'FT911-728')\n",
            "('ft911_3', 'FT911-729')\n",
            "('ft911_3', 'FT911-730')\n",
            "('ft911_3', 'FT911-731')\n",
            "('ft911_3', 'FT911-732')\n",
            "('ft911_3', 'FT911-733')\n",
            "('ft911_3', 'FT911-734')\n",
            "('ft911_3', 'FT911-735')\n",
            "('ft911_3', 'FT911-736')\n",
            "('ft911_3', 'FT911-737')\n",
            "('ft911_3', 'FT911-738')\n",
            "('ft911_3', 'FT911-739')\n",
            "('ft911_3', 'FT911-740')\n",
            "('ft911_3', 'FT911-741')\n",
            "('ft911_3', 'FT911-742')\n",
            "('ft911_3', 'FT911-743')\n",
            "('ft911_3', 'FT911-744')\n",
            "('ft911_3', 'FT911-745')\n",
            "('ft911_3', 'FT911-746')\n",
            "('ft911_3', 'FT911-747')\n",
            "('ft911_3', 'FT911-748')\n",
            "('ft911_3', 'FT911-749')\n",
            "('ft911_3', 'FT911-750')\n",
            "('ft911_3', 'FT911-751')\n",
            "('ft911_3', 'FT911-752')\n",
            "('ft911_3', 'FT911-753')\n",
            "('ft911_3', 'FT911-754')\n",
            "('ft911_3', 'FT911-755')\n",
            "('ft911_3', 'FT911-756')\n",
            "('ft911_3', 'FT911-757')\n",
            "('ft911_3', 'FT911-758')\n",
            "('ft911_3', 'FT911-759')\n",
            "('ft911_3', 'FT911-760')\n",
            "('ft911_3', 'FT911-761')\n",
            "('ft911_3', 'FT911-762')\n",
            "('ft911_3', 'FT911-763')\n",
            "('ft911_3', 'FT911-764')\n",
            "('ft911_3', 'FT911-765')\n",
            "('ft911_3', 'FT911-766')\n",
            "('ft911_3', 'FT911-767')\n",
            "('ft911_3', 'FT911-768')\n",
            "('ft911_3', 'FT911-769')\n",
            "('ft911_3', 'FT911-770')\n",
            "('ft911_3', 'FT911-771')\n",
            "('ft911_3', 'FT911-772')\n",
            "('ft911_3', 'FT911-773')\n",
            "('ft911_3', 'FT911-774')\n",
            "('ft911_3', 'FT911-775')\n",
            "('ft911_3', 'FT911-776')\n",
            "('ft911_3', 'FT911-777')\n",
            "('ft911_3', 'FT911-778')\n",
            "('ft911_3', 'FT911-779')\n",
            "('ft911_3', 'FT911-780')\n",
            "('ft911_3', 'FT911-781')\n",
            "('ft911_3', 'FT911-782')\n",
            "('ft911_3', 'FT911-783')\n",
            "('ft911_3', 'FT911-784')\n",
            "('ft911_3', 'FT911-785')\n",
            "('ft911_3', 'FT911-786')\n",
            "('ft911_3', 'FT911-787')\n",
            "('ft911_3', 'FT911-788')\n",
            "('ft911_3', 'FT911-789')\n",
            "('ft911_3', 'FT911-790')\n",
            "('ft911_3', 'FT911-791')\n",
            "('ft911_3', 'FT911-792')\n",
            "('ft911_3', 'FT911-793')\n",
            "('ft911_3', 'FT911-794')\n",
            "('ft911_3', 'FT911-795')\n",
            "('ft911_3', 'FT911-796')\n",
            "('ft911_3', 'FT911-797')\n",
            "('ft911_3', 'FT911-798')\n",
            "('ft911_3', 'FT911-799')\n",
            "('ft911_3', 'FT911-800')\n",
            "('ft911_3', 'FT911-801')\n",
            "('ft911_3', 'FT911-802')\n",
            "('ft911_3', 'FT911-803')\n",
            "('ft911_3', 'FT911-804')\n",
            "('ft911_3', 'FT911-805')\n",
            "('ft911_3', 'FT911-806')\n",
            "('ft911_3', 'FT911-807')\n",
            "('ft911_3', 'FT911-808')\n",
            "('ft911_3', 'FT911-809')\n",
            "('ft911_3', 'FT911-810')\n",
            "('ft911_3', 'FT911-811')\n",
            "('ft911_3', 'FT911-812')\n",
            "('ft911_3', 'FT911-813')\n",
            "('ft911_3', 'FT911-814')\n",
            "('ft911_3', 'FT911-815')\n",
            "('ft911_3', 'FT911-816')\n",
            "('ft911_3', 'FT911-817')\n",
            "('ft911_3', 'FT911-818')\n",
            "('ft911_3', 'FT911-819')\n",
            "('ft911_3', 'FT911-820')\n",
            "('ft911_3', 'FT911-821')\n",
            "('ft911_3', 'FT911-822')\n",
            "('ft911_3', 'FT911-823')\n",
            "('ft911_3', 'FT911-824')\n",
            "('ft911_3', 'FT911-825')\n",
            "('ft911_3', 'FT911-826')\n",
            "('ft911_3', 'FT911-827')\n",
            "('ft911_3', 'FT911-828')\n",
            "('ft911_3', 'FT911-829')\n",
            "('ft911_3', 'FT911-830')\n",
            "('ft911_3', 'FT911-831')\n",
            "('ft911_3', 'FT911-832')\n",
            "('ft911_3', 'FT911-833')\n",
            "('ft911_3', 'FT911-834')\n",
            "('ft911_3', 'FT911-835')\n",
            "('ft911_3', 'FT911-836')\n",
            "('ft911_3', 'FT911-837')\n",
            "('ft911_3', 'FT911-838')\n",
            "('ft911_3', 'FT911-839')\n",
            "('ft911_3', 'FT911-840')\n",
            "('ft911_3', 'FT911-841')\n",
            "('ft911_3', 'FT911-842')\n",
            "('ft911_3', 'FT911-843')\n",
            "('ft911_3', 'FT911-844')\n",
            "('ft911_3', 'FT911-845')\n",
            "('ft911_3', 'FT911-846')\n",
            "('ft911_3', 'FT911-847')\n",
            "('ft911_3', 'FT911-848')\n",
            "('ft911_3', 'FT911-849')\n",
            "('ft911_3', 'FT911-850')\n",
            "('ft911_3', 'FT911-851')\n",
            "('ft911_3', 'FT911-852')\n",
            "('ft911_3', 'FT911-853')\n",
            "('ft911_3', 'FT911-854')\n",
            "('ft911_3', 'FT911-855')\n",
            "('ft911_3', 'FT911-856')\n",
            "('ft911_3', 'FT911-857')\n",
            "('ft911_3', 'FT911-858')\n",
            "('ft911_3', 'FT911-859')\n",
            "('ft911_3', 'FT911-860')\n",
            "('ft911_3', 'FT911-861')\n",
            "('ft911_3', 'FT911-862')\n",
            "('ft911_3', 'FT911-863')\n",
            "('ft911_3', 'FT911-864')\n",
            "('ft911_3', 'FT911-865')\n",
            "('ft911_3', 'FT911-866')\n",
            "('ft911_3', 'FT911-867')\n",
            "('ft911_3', 'FT911-868')\n",
            "('ft911_3', 'FT911-869')\n",
            "('ft911_3', 'FT911-870')\n",
            "('ft911_3', 'FT911-871')\n",
            "('ft911_3', 'FT911-872')\n",
            "('ft911_3', 'FT911-873')\n",
            "('ft911_3', 'FT911-874')\n",
            "('ft911_3', 'FT911-875')\n",
            "('ft911_3', 'FT911-876')\n",
            "('ft911_3', 'FT911-877')\n",
            "('ft911_3', 'FT911-878')\n",
            "('ft911_3', 'FT911-879')\n",
            "('ft911_3', 'FT911-880')\n",
            "('ft911_3', 'FT911-881')\n",
            "('ft911_3', 'FT911-882')\n",
            "('ft911_3', 'FT911-883')\n",
            "('ft911_3', 'FT911-884')\n",
            "('ft911_3', 'FT911-885')\n",
            "('ft911_3', 'FT911-886')\n",
            "('ft911_3', 'FT911-887')\n",
            "('ft911_3', 'FT911-888')\n",
            "('ft911_3', 'FT911-889')\n",
            "('ft911_3', 'FT911-890')\n",
            "('ft911_3', 'FT911-891')\n",
            "('ft911_3', 'FT911-892')\n",
            "('ft911_3', 'FT911-893')\n",
            "('ft911_3', 'FT911-894')\n",
            "('ft911_3', 'FT911-895')\n",
            "('ft911_3', 'FT911-896')\n",
            "('ft911_3', 'FT911-897')\n",
            "('ft911_3', 'FT911-898')\n",
            "('ft911_3', 'FT911-899')\n",
            "('ft911_3', 'FT911-900')\n",
            "('ft911_3', 'FT911-901')\n",
            "('ft911_3', 'FT911-902')\n",
            "('ft911_3', 'FT911-903')\n",
            "('ft911_3', 'FT911-904')\n",
            "('ft911_3', 'FT911-905')\n",
            "('ft911_3', 'FT911-906')\n",
            "('ft911_3', 'FT911-907')\n",
            "('ft911_3', 'FT911-908')\n",
            "('ft911_3', 'FT911-909')\n",
            "('ft911_3', 'FT911-910')\n",
            "('ft911_3', 'FT911-911')\n",
            "('ft911_3', 'FT911-912')\n",
            "('ft911_3', 'FT911-913')\n",
            "('ft911_3', 'FT911-914')\n",
            "('ft911_3', 'FT911-915')\n",
            "('ft911_3', 'FT911-916')\n",
            "('ft911_3', 'FT911-917')\n",
            "('ft911_3', 'FT911-918')\n",
            "('ft911_3', 'FT911-919')\n",
            "('ft911_3', 'FT911-920')\n",
            "('ft911_3', 'FT911-921')\n",
            "('ft911_3', 'FT911-922')\n",
            "('ft911_3', 'FT911-923')\n",
            "('ft911_3', 'FT911-924')\n",
            "('ft911_3', 'FT911-925')\n",
            "('ft911_3', 'FT911-926')\n",
            "('ft911_3', 'FT911-927')\n",
            "('ft911_3', 'FT911-928')\n",
            "('ft911_3', 'FT911-929')\n",
            "('ft911_3', 'FT911-930')\n",
            "('ft911_3', 'FT911-931')\n",
            "('ft911_3', 'FT911-932')\n",
            "('ft911_3', 'FT911-933')\n",
            "('ft911_3', 'FT911-934')\n",
            "('ft911_3', 'FT911-935')\n",
            "('ft911_3', 'FT911-936')\n",
            "('ft911_3', 'FT911-937')\n",
            "('ft911_3', 'FT911-938')\n",
            "('ft911_3', 'FT911-939')\n",
            "('ft911_3', 'FT911-940')\n",
            "('ft911_3', 'FT911-941')\n",
            "('ft911_3', 'FT911-942')\n",
            "('ft911_3', 'FT911-943')\n",
            "('ft911_3', 'FT911-944')\n",
            "('ft911_3', 'FT911-945')\n",
            "('ft911_3', 'FT911-946')\n",
            "('ft911_3', 'FT911-947')\n",
            "('ft911_3', 'FT911-948')\n",
            "('ft911_3', 'FT911-949')\n",
            "('ft911_3', 'FT911-950')\n",
            "('ft911_3', 'FT911-951')\n",
            "('ft911_3', 'FT911-952')\n",
            "('ft911_3', 'FT911-953')\n",
            "('ft911_3', 'FT911-954')\n",
            "('ft911_3', 'FT911-955')\n",
            "('ft911_3', 'FT911-956')\n",
            "('ft911_3', 'FT911-957')\n",
            "('ft911_3', 'FT911-958')\n",
            "('ft911_3', 'FT911-959')\n",
            "('ft911_3', 'FT911-960')\n",
            "('ft911_3', 'FT911-961')\n",
            "('ft911_3', 'FT911-962')\n",
            "('ft911_3', 'FT911-963')\n",
            "('ft911_3', 'FT911-964')\n",
            "('ft911_3', 'FT911-965')\n",
            "('ft911_3', 'FT911-966')\n",
            "('ft911_3', 'FT911-967')\n",
            "('ft911_3', 'FT911-968')\n",
            "('ft911_3', 'FT911-969')\n",
            "('ft911_3', 'FT911-970')\n",
            "('ft911_3', 'FT911-971')\n",
            "('ft911_3', 'FT911-972')\n",
            "('ft911_3', 'FT911-973')\n",
            "('ft911_3', 'FT911-974')\n",
            "('ft911_3', 'FT911-975')\n",
            "('ft911_3', 'FT911-976')\n",
            "('ft911_3', 'FT911-977')\n",
            "('ft911_3', 'FT911-978')\n",
            "('ft911_3', 'FT911-979')\n",
            "('ft911_3', 'FT911-980')\n",
            "('ft911_3', 'FT911-981')\n",
            "('ft911_3', 'FT911-982')\n",
            "('ft911_3', 'FT911-983')\n",
            "('ft911_3', 'FT911-984')\n",
            "('ft911_3', 'FT911-985')\n",
            "('ft911_3', 'FT911-986')\n",
            "('ft911_3', 'FT911-987')\n",
            "('ft911_3', 'FT911-988')\n",
            "('ft911_3', 'FT911-989')\n",
            "('ft911_3', 'FT911-990')\n",
            "('ft911_3', 'FT911-991')\n",
            "('ft911_3', 'FT911-992')\n",
            "('ft911_3', 'FT911-993')\n",
            "('ft911_3', 'FT911-994')\n",
            "('ft911_3', 'FT911-995')\n",
            "('ft911_3', 'FT911-996')\n",
            "('ft911_3', 'FT911-997')\n",
            "('ft911_3', 'FT911-998')\n",
            "('ft911_3', 'FT911-999')\n",
            "('ft911_3', 'FT911-1000')\n",
            "('ft911_3', 'FT911-1001')\n",
            "('ft911_3', 'FT911-1002')\n",
            "('ft911_3', 'FT911-1003')\n",
            "('ft911_3', 'FT911-1004')\n",
            "('ft911_3', 'FT911-1005')\n",
            "('ft911_3', 'FT911-1006')\n",
            "('ft911_3', 'FT911-1007')\n",
            "('ft911_3', 'FT911-1008')\n",
            "('ft911_3', 'FT911-1009')\n",
            "('ft911_3', 'FT911-1010')\n",
            "('ft911_3', 'FT911-1011')\n",
            "('ft911_3', 'FT911-1012')\n",
            "('ft911_3', 'FT911-1013')\n",
            "('ft911_3', 'FT911-1014')\n",
            "('ft911_3', 'FT911-1015')\n",
            "('ft911_3', 'FT911-1016')\n",
            "('ft911_3', 'FT911-1017')\n",
            "('ft911_3', 'FT911-1018')\n",
            "('ft911_3', 'FT911-1019')\n",
            "('ft911_3', 'FT911-1020')\n",
            "('ft911_3', 'FT911-1021')\n",
            "('ft911_3', 'FT911-1022')\n",
            "('ft911_3', 'FT911-1023')\n",
            "('ft911_3', 'FT911-1024')\n",
            "('ft911_3', 'FT911-1025')\n",
            "('ft911_3', 'FT911-1026')\n",
            "('ft911_3', 'FT911-1027')\n",
            "('ft911_3', 'FT911-1028')\n",
            "('ft911_3', 'FT911-1029')\n",
            "('ft911_3', 'FT911-1030')\n",
            "('ft911_3', 'FT911-1031')\n",
            "('ft911_3', 'FT911-1032')\n",
            "('ft911_3', 'FT911-1033')\n",
            "('ft911_3', 'FT911-1034')\n",
            "('ft911_3', 'FT911-1035')\n",
            "('ft911_3', 'FT911-1036')\n",
            "('ft911_3', 'FT911-1037')\n",
            "('ft911_3', 'FT911-1038')\n",
            "('ft911_3', 'FT911-1039')\n",
            "('ft911_3', 'FT911-1040')\n",
            "('ft911_3', 'FT911-1041')\n",
            "('ft911_3', 'FT911-1042')\n",
            "('ft911_3', 'FT911-1043')\n",
            "('ft911_3', 'FT911-1044')\n",
            "('ft911_3', 'FT911-1045')\n",
            "('ft911_3', 'FT911-1046')\n",
            "('ft911_3', 'FT911-1047')\n",
            "('ft911_3', 'FT911-1048')\n",
            "('ft911_3', 'FT911-1049')\n",
            "('ft911_3', 'FT911-1050')\n",
            "('ft911_3', 'FT911-1051')\n",
            "('ft911_3', 'FT911-1052')\n",
            "('ft911_3', 'FT911-1053')\n",
            "('ft911_3', 'FT911-1054')\n",
            "('ft911_3', 'FT911-1055')\n",
            "('ft911_3', 'FT911-1056')\n",
            "('ft911_3', 'FT911-1057')\n",
            "('ft911_3', 'FT911-1058')\n",
            "('ft911_3', 'FT911-1059')\n",
            "('ft911_3', 'FT911-1060')\n",
            "('ft911_3', 'FT911-1061')\n",
            "('ft911_3', 'FT911-1062')\n",
            "('ft911_3', 'FT911-1063')\n",
            "('ft911_3', 'FT911-1064')\n",
            "('ft911_3', 'FT911-1065')\n",
            "('ft911_3', 'FT911-1066')\n",
            "('ft911_3', 'FT911-1067')\n",
            "('ft911_3', 'FT911-1068')\n",
            "('ft911_3', 'FT911-1069')\n",
            "('ft911_3', 'FT911-1070')\n",
            "('ft911_3', 'FT911-1071')\n",
            "('ft911_3', 'FT911-1072')\n",
            "('ft911_3', 'FT911-1073')\n",
            "('ft911_3', 'FT911-1074')\n",
            "('ft911_3', 'FT911-1075')\n",
            "('ft911_3', 'FT911-1076')\n",
            "('ft911_3', 'FT911-1077')\n",
            "('ft911_3', 'FT911-1078')\n",
            "('ft911_3', 'FT911-1079')\n",
            "('ft911_3', 'FT911-1080')\n",
            "('ft911_3', 'FT911-1081')\n",
            "('ft911_3', 'FT911-1082')\n",
            "('ft911_3', 'FT911-1083')\n",
            "('ft911_3', 'FT911-1084')\n",
            "('ft911_3', 'FT911-1085')\n",
            "('ft911_3', 'FT911-1086')\n",
            "('ft911_3', 'FT911-1087')\n",
            "('ft911_3', 'FT911-1088')\n",
            "('ft911_3', 'FT911-1089')\n",
            "('ft911_3', 'FT911-1090')\n",
            "('ft911_3', 'FT911-1091')\n",
            "('ft911_3', 'FT911-1092')\n",
            "('ft911_3', 'FT911-1093')\n",
            "('ft911_3', 'FT911-1094')\n",
            "('ft911_3', 'FT911-1095')\n",
            "('ft911_3', 'FT911-1096')\n",
            "('ft911_3', 'FT911-1097')\n",
            "('ft911_3', 'FT911-1098')\n",
            "('ft911_4', 'FT911-1099')\n",
            "('ft911_4', 'FT911-1100')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load stopwords\n",
        "stopwords_path = \"/content/drive/MyDrive/InfoRetrieval/stopwordlist.txt\"\n",
        "with open(stopwords_path, 'r', encoding='utf-8') as f:\n",
        "    stopwords = set(f.read().split())\n",
        "\n",
        "# Initialize the stemmer\n",
        "word_stemmer = PorterStemmer()\n",
        "\n",
        "docs_path = \"/content/drive/MyDrive/InfoRetrieval/ft911/\"\n",
        "processed_docs = {}\n",
        "FTFileMapping = {}  # Mapping: file_name -> list of DOCNOs\n",
        "FileDictionary = {}  # Mapping: (file_name, local_doc_num) -> global_doc_id\n",
        "DocumentToFileMapping = []  # list of (file_name, numeric_DOCNO)\n",
        "\n",
        "# Sort files numerically based on suffix\n",
        "file_list = sorted(\n",
        "    [f for f in os.listdir(docs_path) if f.startswith(\"ft911_\")],\n",
        "    key=lambda x: int(re.findall(r'\\d+', x)[0])\n",
        ")\n",
        "\n",
        "# Process each file\n",
        "global_doc_id = 1\n",
        "for file_name in file_list:\n",
        "    full_file_path = os.path.join(docs_path, file_name)\n",
        "\n",
        "    if os.path.isfile(full_file_path):\n",
        "        print(f\"Checking file: {full_file_path}\")\n",
        "        with open(full_file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "            file_content = file.read()\n",
        "\n",
        "            # Parse TREC documents\n",
        "            parsed_docs = {}\n",
        "            current_doc_lines = []\n",
        "            current_doc_id = None\n",
        "            for line in file_content.splitlines():\n",
        "                if line.startswith(\"<DOCNO>\"):\n",
        "                    if current_doc_id is not None:\n",
        "                        parsed_docs[current_doc_id] = \" \".join(current_doc_lines)\n",
        "                    current_doc_id = line.replace(\"<DOCNO>\", \"\").replace(\"</DOCNO>\", \"\").strip()\n",
        "                    current_doc_lines = []\n",
        "                elif not line.startswith(\"<\") and current_doc_id:\n",
        "                    current_doc_lines.append(line.strip())\n",
        "            if current_doc_id is not None:\n",
        "                parsed_docs[current_doc_id] = \" \".join(current_doc_lines)\n",
        "\n",
        "            print(f\"Parsed documents (first 10): {list(parsed_docs.keys())[:10]}\")\n",
        "\n",
        "            # Save file -> DOCNOs\n",
        "            FTFileMapping[file_name] = list(parsed_docs.keys())\n",
        "\n",
        "            # Save cleaned (file_name, numeric_DOCNO)\n",
        "            for docno in parsed_docs.keys():\n",
        "                numeric_docno = docno.split(\"-\")[-1]\n",
        "                DocumentToFileMapping.append((file_name, numeric_docno))\n",
        "\n",
        "            # Assign global and local doc IDs\n",
        "            local_id = 1\n",
        "            for doc_id in parsed_docs:\n",
        "                FileDictionary[(file_name, local_id)] = global_doc_id\n",
        "                local_id += 1\n",
        "                global_doc_id += 1\n",
        "\n",
        "            # Process text\n",
        "            for doc_id, doc_text in sorted(parsed_docs.items()):\n",
        "                text = doc_text.lower()\n",
        "                text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
        "                tokens = nltk.word_tokenize(text)\n",
        "                tokens = [word for word in tokens if word.isalpha()]\n",
        "                filtered_tokens = [word for word in tokens if word not in stopwords]\n",
        "                stemmed_tokens = [word_stemmer.stem(word) for word in filtered_tokens]\n",
        "                processed_docs[doc_id] = stemmed_tokens\n",
        "\n",
        "# Create WordDictionary\n",
        "WordDictionary = {}\n",
        "current_word_id = 1\n",
        "for words in processed_docs.values():\n",
        "    for word in words:\n",
        "        if word not in WordDictionary:\n",
        "            WordDictionary[word] = current_word_id\n",
        "            current_word_id += 1\n",
        "\n",
        "# Sort dictionaries\n",
        "WordDictionary = dict(sorted(WordDictionary.items()))\n",
        "FileDictionary = dict(sorted(FileDictionary.items()))\n",
        "\n",
        "# Save WordDictionary\n",
        "word_dict_path = \"/content/drive/MyDrive/InfoRetrieval/word_dictionary.txt\"\n",
        "with open(word_dict_path, \"w\") as word_file:\n",
        "    for word, word_id in WordDictionary.items():\n",
        "        word_file.write(f\"{word}\\t{word_id}\\n\")\n",
        "\n",
        "# Save FileDictionary (filename + local doc number)\n",
        "file_dict_path = \"/content/drive/MyDrive/InfoRetrieval/file_dictionary.txt\"\n",
        "with open(file_dict_path, \"w\") as file_file:\n",
        "    for (file_name, local_doc_num), global_id in FileDictionary.items():\n",
        "        file_file.write(f\"{file_name}\\t{local_doc_num}\\t{global_id}\\n\")\n",
        "\n",
        "# Save FTFileMapping (file -> list of DOCNOs)\n",
        "ft_map_path = \"/content/drive/MyDrive/InfoRetrieval/ft_file_mapping.txt\"\n",
        "with open(ft_map_path, \"w\") as map_file:\n",
        "    for file, doc_ids in FTFileMapping.items():\n",
        "        map_file.write(f\"{file}:\\t{', '.join(doc_ids)}\\n\")\n",
        "\n",
        "# Save DocumentToFileMapping with numeric DOCNOs\n",
        "doc_file_map_path = \"/content/drive/MyDrive/InfoRetrieval/doc_to_file_mapping.txt\"\n",
        "with open(doc_file_map_path, \"w\") as out_file:\n",
        "    for file_name, numeric_docno in DocumentToFileMapping:\n",
        "        out_file.write(f\"{file_name}\\t{numeric_docno}\\n\")\n",
        "\n",
        "# Logs\n",
        "print(f\"WordDictionary saved to {word_dict_path}\")\n",
        "print(f\"FileDictionary saved to {file_dict_path}\")\n",
        "print(f\"FTFileMapping saved to {ft_map_path}\")\n",
        "print(f\"DocumentToFileMapping saved to {doc_file_map_path}\")\n",
        "\n",
        "print(\"Token and Token ID Mapping (First 100):\")\n",
        "for token, token_id in list(WordDictionary.items())[:100]:\n",
        "    print(f\"{token}\\t{token_id}\")\n",
        "\n",
        "# Print cleaned DocumentToFileMapping\n",
        "print(\"DocumentToFileMapping (First 1000):\")\n",
        "for file_name, numeric_docno in DocumentToFileMapping[718:1100]:\n",
        "    print((file_name, numeric_docno))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYe28QuEnH_s",
        "outputId": "45fc2edd-419e-4d9a-eeaf-9f2817c01ee6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_1\n",
            "Parsed documents (first 10): ['FT911-1', 'FT911-2', 'FT911-3', 'FT911-4', 'FT911-5', 'FT911-6', 'FT911-7', 'FT911-8', 'FT911-9', 'FT911-10']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_2\n",
            "Parsed documents (first 10): ['FT911-376', 'FT911-377', 'FT911-378', 'FT911-379', 'FT911-380', 'FT911-381', 'FT911-382', 'FT911-383', 'FT911-384', 'FT911-385']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_3\n",
            "Parsed documents (first 10): ['FT911-722', 'FT911-723', 'FT911-724', 'FT911-725', 'FT911-726', 'FT911-727', 'FT911-728', 'FT911-729', 'FT911-730', 'FT911-731']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_4\n",
            "Parsed documents (first 10): ['FT911-1099', 'FT911-1100', 'FT911-1101', 'FT911-1102', 'FT911-1103', 'FT911-1104', 'FT911-1105', 'FT911-1106', 'FT911-1107', 'FT911-1108']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_5\n",
            "Parsed documents (first 10): ['FT911-1479', 'FT911-1480', 'FT911-1481', 'FT911-1482', 'FT911-1483', 'FT911-1484', 'FT911-1485', 'FT911-1486', 'FT911-1487', 'FT911-1488']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_6\n",
            "Parsed documents (first 10): ['FT911-1832', 'FT911-1833', 'FT911-1834', 'FT911-1835', 'FT911-1836', 'FT911-1837', 'FT911-1838', 'FT911-1839', 'FT911-1840', 'FT911-1841']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_7\n",
            "Parsed documents (first 10): ['FT911-2212', 'FT911-2213', 'FT911-2214', 'FT911-2215', 'FT911-2216', 'FT911-2217', 'FT911-2218', 'FT911-2219', 'FT911-2220', 'FT911-2221']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_8\n",
            "Parsed documents (first 10): ['FT911-2621', 'FT911-2622', 'FT911-2623', 'FT911-2624', 'FT911-2625', 'FT911-2626', 'FT911-2627', 'FT911-2628', 'FT911-2629', 'FT911-2630']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_9\n",
            "Parsed documents (first 10): ['FT911-2950', 'FT911-2951', 'FT911-2952', 'FT911-2953', 'FT911-2954', 'FT911-2955', 'FT911-2956', 'FT911-2957', 'FT911-2958', 'FT911-2959']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_10\n",
            "Parsed documents (first 10): ['FT911-3323', 'FT911-3324', 'FT911-3325', 'FT911-3326', 'FT911-3327', 'FT911-3328', 'FT911-3329', 'FT911-3330', 'FT911-3331', 'FT911-3332']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_11\n",
            "Parsed documents (first 10): ['FT911-3694', 'FT911-3695', 'FT911-3696', 'FT911-3697', 'FT911-3698', 'FT911-3699', 'FT911-3700', 'FT911-3701', 'FT911-3702', 'FT911-3703']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_12\n",
            "Parsed documents (first 10): ['FT911-4016', 'FT911-4017', 'FT911-4018', 'FT911-4019', 'FT911-4020', 'FT911-4021', 'FT911-4022', 'FT911-4023', 'FT911-4024', 'FT911-4025']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_13\n",
            "Parsed documents (first 10): ['FT911-4366', 'FT911-4367', 'FT911-4368', 'FT911-4369', 'FT911-4370', 'FT911-4371', 'FT911-4372', 'FT911-4373', 'FT911-4374', 'FT911-4375']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_15\n",
            "Parsed documents (first 10): ['FT911-5129', 'FT911-5130', 'FT911-5131', 'FT911-5132', 'FT911-5133', 'FT911-5134', 'FT911-5135', 'FT911-5136', 'FT911-5137', 'FT911-5138']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_14\n",
            "Parsed documents (first 10): ['FT911-4749', 'FT911-4750', 'FT911-4751', 'FT911-4752', 'FT911-4753', 'FT911-4754', 'FT911-4755', 'FT911-4756', 'FT911-4757', 'FT911-4758']\n",
            "WordDictionary saved to /content/drive/MyDrive/InfoRetrieval/word_dictionary.txt\n",
            "FileDictionary saved to /content/drive/MyDrive/InfoRetrieval/file_dictionary.txt\n",
            "FTFileMapping saved to /content/drive/MyDrive/InfoRetrieval/ft_file_mapping.txt\n",
            "DocumentToFileMapping saved to /content/drive/MyDrive/InfoRetrieval/doc_to_file_mapping.txt\n",
            "Token and Token ID Mapping (First 100):\n",
            "aa\t5542\n",
            "aaa\t13313\n",
            "aachen\t14777\n",
            "aaf\t7625\n",
            "aah\t20455\n",
            "aakvaag\t32703\n",
            "aalborg\t30434\n",
            "aaron\t17933\n",
            "ab\t4195\n",
            "ababa\t22651\n",
            "aback\t13024\n",
            "abalkin\t1117\n",
            "abandon\t1408\n",
            "abash\t15986\n",
            "abat\t13479\n",
            "abattoir\t22113\n",
            "abb\t1742\n",
            "abba\t22179\n",
            "abbacchio\t18245\n",
            "abbado\t6021\n",
            "abbatoir\t20349\n",
            "abbey\t11270\n",
            "abbot\t18730\n",
            "abbott\t3736\n",
            "abbrevi\t20194\n",
            "abc\t15580\n",
            "abcc\t22751\n",
            "abcd\t30331\n",
            "abci\t27009\n",
            "abdali\t23047\n",
            "abdel\t21093\n",
            "abdelaziz\t24529\n",
            "abdic\t13992\n",
            "abduct\t6413\n",
            "abdul\t5892\n",
            "abdulla\t12153\n",
            "abdullah\t20274\n",
            "abe\t32960\n",
            "abel\t6148\n",
            "abela\t27132\n",
            "abercorn\t24021\n",
            "aberdeen\t3026\n",
            "aberdeenshir\t15507\n",
            "aberforth\t9459\n",
            "abergavenni\t12926\n",
            "aberr\t16785\n",
            "abet\t20983\n",
            "abey\t17054\n",
            "abhor\t8237\n",
            "abhorr\t17326\n",
            "abi\t30652\n",
            "abid\t9704\n",
            "abidin\t28714\n",
            "abil\t1673\n",
            "abingdon\t15659\n",
            "abington\t24389\n",
            "abingworth\t27431\n",
            "abitibi\t20809\n",
            "abitur\t31992\n",
            "abject\t17375\n",
            "abk\t30859\n",
            "ablaz\t18224\n",
            "abli\t13581\n",
            "ablitt\t26515\n",
            "ablut\t10456\n",
            "abn\t13229\n",
            "abnorm\t14074\n",
            "aboard\t18096\n",
            "abod\t29376\n",
            "abol\t28004\n",
            "abolhassan\t19623\n",
            "abolish\t6745\n",
            "abolit\t8151\n",
            "abolitionist\t26881\n",
            "abomin\t32251\n",
            "aborigin\t14709\n",
            "abort\t5298\n",
            "abouloff\t14142\n",
            "abound\t2537\n",
            "abouthow\t23620\n",
            "aboutth\t23185\n",
            "abpi\t31966\n",
            "abraham\t3935\n",
            "abram\t4987\n",
            "abramowitz\t18199\n",
            "abras\t12705\n",
            "abreast\t20229\n",
            "abridg\t6520\n",
            "abroad\t344\n",
            "abrog\t20237\n",
            "abrupt\t4555\n",
            "abruptli\t18040\n",
            "abruzzo\t31574\n",
            "absa\t16978\n",
            "abscond\t31613\n",
            "absenc\t1413\n",
            "absent\t1645\n",
            "absente\t12099\n",
            "absentia\t27146\n",
            "absinth\t17797\n",
            "DocumentToFileMapping (First 1000):\n",
            "('ft911_2', '719')\n",
            "('ft911_2', '720')\n",
            "('ft911_2', '721')\n",
            "('ft911_3', '722')\n",
            "('ft911_3', '723')\n",
            "('ft911_3', '724')\n",
            "('ft911_3', '725')\n",
            "('ft911_3', '726')\n",
            "('ft911_3', '727')\n",
            "('ft911_3', '728')\n",
            "('ft911_3', '729')\n",
            "('ft911_3', '730')\n",
            "('ft911_3', '731')\n",
            "('ft911_3', '732')\n",
            "('ft911_3', '733')\n",
            "('ft911_3', '734')\n",
            "('ft911_3', '735')\n",
            "('ft911_3', '736')\n",
            "('ft911_3', '737')\n",
            "('ft911_3', '738')\n",
            "('ft911_3', '739')\n",
            "('ft911_3', '740')\n",
            "('ft911_3', '741')\n",
            "('ft911_3', '742')\n",
            "('ft911_3', '743')\n",
            "('ft911_3', '744')\n",
            "('ft911_3', '745')\n",
            "('ft911_3', '746')\n",
            "('ft911_3', '747')\n",
            "('ft911_3', '748')\n",
            "('ft911_3', '749')\n",
            "('ft911_3', '750')\n",
            "('ft911_3', '751')\n",
            "('ft911_3', '752')\n",
            "('ft911_3', '753')\n",
            "('ft911_3', '754')\n",
            "('ft911_3', '755')\n",
            "('ft911_3', '756')\n",
            "('ft911_3', '757')\n",
            "('ft911_3', '758')\n",
            "('ft911_3', '759')\n",
            "('ft911_3', '760')\n",
            "('ft911_3', '761')\n",
            "('ft911_3', '762')\n",
            "('ft911_3', '763')\n",
            "('ft911_3', '764')\n",
            "('ft911_3', '765')\n",
            "('ft911_3', '766')\n",
            "('ft911_3', '767')\n",
            "('ft911_3', '768')\n",
            "('ft911_3', '769')\n",
            "('ft911_3', '770')\n",
            "('ft911_3', '771')\n",
            "('ft911_3', '772')\n",
            "('ft911_3', '773')\n",
            "('ft911_3', '774')\n",
            "('ft911_3', '775')\n",
            "('ft911_3', '776')\n",
            "('ft911_3', '777')\n",
            "('ft911_3', '778')\n",
            "('ft911_3', '779')\n",
            "('ft911_3', '780')\n",
            "('ft911_3', '781')\n",
            "('ft911_3', '782')\n",
            "('ft911_3', '783')\n",
            "('ft911_3', '784')\n",
            "('ft911_3', '785')\n",
            "('ft911_3', '786')\n",
            "('ft911_3', '787')\n",
            "('ft911_3', '788')\n",
            "('ft911_3', '789')\n",
            "('ft911_3', '790')\n",
            "('ft911_3', '791')\n",
            "('ft911_3', '792')\n",
            "('ft911_3', '793')\n",
            "('ft911_3', '794')\n",
            "('ft911_3', '795')\n",
            "('ft911_3', '796')\n",
            "('ft911_3', '797')\n",
            "('ft911_3', '798')\n",
            "('ft911_3', '799')\n",
            "('ft911_3', '800')\n",
            "('ft911_3', '801')\n",
            "('ft911_3', '802')\n",
            "('ft911_3', '803')\n",
            "('ft911_3', '804')\n",
            "('ft911_3', '805')\n",
            "('ft911_3', '806')\n",
            "('ft911_3', '807')\n",
            "('ft911_3', '808')\n",
            "('ft911_3', '809')\n",
            "('ft911_3', '810')\n",
            "('ft911_3', '811')\n",
            "('ft911_3', '812')\n",
            "('ft911_3', '813')\n",
            "('ft911_3', '814')\n",
            "('ft911_3', '815')\n",
            "('ft911_3', '816')\n",
            "('ft911_3', '817')\n",
            "('ft911_3', '818')\n",
            "('ft911_3', '819')\n",
            "('ft911_3', '820')\n",
            "('ft911_3', '821')\n",
            "('ft911_3', '822')\n",
            "('ft911_3', '823')\n",
            "('ft911_3', '824')\n",
            "('ft911_3', '825')\n",
            "('ft911_3', '826')\n",
            "('ft911_3', '827')\n",
            "('ft911_3', '828')\n",
            "('ft911_3', '829')\n",
            "('ft911_3', '830')\n",
            "('ft911_3', '831')\n",
            "('ft911_3', '832')\n",
            "('ft911_3', '833')\n",
            "('ft911_3', '834')\n",
            "('ft911_3', '835')\n",
            "('ft911_3', '836')\n",
            "('ft911_3', '837')\n",
            "('ft911_3', '838')\n",
            "('ft911_3', '839')\n",
            "('ft911_3', '840')\n",
            "('ft911_3', '841')\n",
            "('ft911_3', '842')\n",
            "('ft911_3', '843')\n",
            "('ft911_3', '844')\n",
            "('ft911_3', '845')\n",
            "('ft911_3', '846')\n",
            "('ft911_3', '847')\n",
            "('ft911_3', '848')\n",
            "('ft911_3', '849')\n",
            "('ft911_3', '850')\n",
            "('ft911_3', '851')\n",
            "('ft911_3', '852')\n",
            "('ft911_3', '853')\n",
            "('ft911_3', '854')\n",
            "('ft911_3', '855')\n",
            "('ft911_3', '856')\n",
            "('ft911_3', '857')\n",
            "('ft911_3', '858')\n",
            "('ft911_3', '859')\n",
            "('ft911_3', '860')\n",
            "('ft911_3', '861')\n",
            "('ft911_3', '862')\n",
            "('ft911_3', '863')\n",
            "('ft911_3', '864')\n",
            "('ft911_3', '865')\n",
            "('ft911_3', '866')\n",
            "('ft911_3', '867')\n",
            "('ft911_3', '868')\n",
            "('ft911_3', '869')\n",
            "('ft911_3', '870')\n",
            "('ft911_3', '871')\n",
            "('ft911_3', '872')\n",
            "('ft911_3', '873')\n",
            "('ft911_3', '874')\n",
            "('ft911_3', '875')\n",
            "('ft911_3', '876')\n",
            "('ft911_3', '877')\n",
            "('ft911_3', '878')\n",
            "('ft911_3', '879')\n",
            "('ft911_3', '880')\n",
            "('ft911_3', '881')\n",
            "('ft911_3', '882')\n",
            "('ft911_3', '883')\n",
            "('ft911_3', '884')\n",
            "('ft911_3', '885')\n",
            "('ft911_3', '886')\n",
            "('ft911_3', '887')\n",
            "('ft911_3', '888')\n",
            "('ft911_3', '889')\n",
            "('ft911_3', '890')\n",
            "('ft911_3', '891')\n",
            "('ft911_3', '892')\n",
            "('ft911_3', '893')\n",
            "('ft911_3', '894')\n",
            "('ft911_3', '895')\n",
            "('ft911_3', '896')\n",
            "('ft911_3', '897')\n",
            "('ft911_3', '898')\n",
            "('ft911_3', '899')\n",
            "('ft911_3', '900')\n",
            "('ft911_3', '901')\n",
            "('ft911_3', '902')\n",
            "('ft911_3', '903')\n",
            "('ft911_3', '904')\n",
            "('ft911_3', '905')\n",
            "('ft911_3', '906')\n",
            "('ft911_3', '907')\n",
            "('ft911_3', '908')\n",
            "('ft911_3', '909')\n",
            "('ft911_3', '910')\n",
            "('ft911_3', '911')\n",
            "('ft911_3', '912')\n",
            "('ft911_3', '913')\n",
            "('ft911_3', '914')\n",
            "('ft911_3', '915')\n",
            "('ft911_3', '916')\n",
            "('ft911_3', '917')\n",
            "('ft911_3', '918')\n",
            "('ft911_3', '919')\n",
            "('ft911_3', '920')\n",
            "('ft911_3', '921')\n",
            "('ft911_3', '922')\n",
            "('ft911_3', '923')\n",
            "('ft911_3', '924')\n",
            "('ft911_3', '925')\n",
            "('ft911_3', '926')\n",
            "('ft911_3', '927')\n",
            "('ft911_3', '928')\n",
            "('ft911_3', '929')\n",
            "('ft911_3', '930')\n",
            "('ft911_3', '931')\n",
            "('ft911_3', '932')\n",
            "('ft911_3', '933')\n",
            "('ft911_3', '934')\n",
            "('ft911_3', '935')\n",
            "('ft911_3', '936')\n",
            "('ft911_3', '937')\n",
            "('ft911_3', '938')\n",
            "('ft911_3', '939')\n",
            "('ft911_3', '940')\n",
            "('ft911_3', '941')\n",
            "('ft911_3', '942')\n",
            "('ft911_3', '943')\n",
            "('ft911_3', '944')\n",
            "('ft911_3', '945')\n",
            "('ft911_3', '946')\n",
            "('ft911_3', '947')\n",
            "('ft911_3', '948')\n",
            "('ft911_3', '949')\n",
            "('ft911_3', '950')\n",
            "('ft911_3', '951')\n",
            "('ft911_3', '952')\n",
            "('ft911_3', '953')\n",
            "('ft911_3', '954')\n",
            "('ft911_3', '955')\n",
            "('ft911_3', '956')\n",
            "('ft911_3', '957')\n",
            "('ft911_3', '958')\n",
            "('ft911_3', '959')\n",
            "('ft911_3', '960')\n",
            "('ft911_3', '961')\n",
            "('ft911_3', '962')\n",
            "('ft911_3', '963')\n",
            "('ft911_3', '964')\n",
            "('ft911_3', '965')\n",
            "('ft911_3', '966')\n",
            "('ft911_3', '967')\n",
            "('ft911_3', '968')\n",
            "('ft911_3', '969')\n",
            "('ft911_3', '970')\n",
            "('ft911_3', '971')\n",
            "('ft911_3', '972')\n",
            "('ft911_3', '973')\n",
            "('ft911_3', '974')\n",
            "('ft911_3', '975')\n",
            "('ft911_3', '976')\n",
            "('ft911_3', '977')\n",
            "('ft911_3', '978')\n",
            "('ft911_3', '979')\n",
            "('ft911_3', '980')\n",
            "('ft911_3', '981')\n",
            "('ft911_3', '982')\n",
            "('ft911_3', '983')\n",
            "('ft911_3', '984')\n",
            "('ft911_3', '985')\n",
            "('ft911_3', '986')\n",
            "('ft911_3', '987')\n",
            "('ft911_3', '988')\n",
            "('ft911_3', '989')\n",
            "('ft911_3', '990')\n",
            "('ft911_3', '991')\n",
            "('ft911_3', '992')\n",
            "('ft911_3', '993')\n",
            "('ft911_3', '994')\n",
            "('ft911_3', '995')\n",
            "('ft911_3', '996')\n",
            "('ft911_3', '997')\n",
            "('ft911_3', '998')\n",
            "('ft911_3', '999')\n",
            "('ft911_3', '1000')\n",
            "('ft911_3', '1001')\n",
            "('ft911_3', '1002')\n",
            "('ft911_3', '1003')\n",
            "('ft911_3', '1004')\n",
            "('ft911_3', '1005')\n",
            "('ft911_3', '1006')\n",
            "('ft911_3', '1007')\n",
            "('ft911_3', '1008')\n",
            "('ft911_3', '1009')\n",
            "('ft911_3', '1010')\n",
            "('ft911_3', '1011')\n",
            "('ft911_3', '1012')\n",
            "('ft911_3', '1013')\n",
            "('ft911_3', '1014')\n",
            "('ft911_3', '1015')\n",
            "('ft911_3', '1016')\n",
            "('ft911_3', '1017')\n",
            "('ft911_3', '1018')\n",
            "('ft911_3', '1019')\n",
            "('ft911_3', '1020')\n",
            "('ft911_3', '1021')\n",
            "('ft911_3', '1022')\n",
            "('ft911_3', '1023')\n",
            "('ft911_3', '1024')\n",
            "('ft911_3', '1025')\n",
            "('ft911_3', '1026')\n",
            "('ft911_3', '1027')\n",
            "('ft911_3', '1028')\n",
            "('ft911_3', '1029')\n",
            "('ft911_3', '1030')\n",
            "('ft911_3', '1031')\n",
            "('ft911_3', '1032')\n",
            "('ft911_3', '1033')\n",
            "('ft911_3', '1034')\n",
            "('ft911_3', '1035')\n",
            "('ft911_3', '1036')\n",
            "('ft911_3', '1037')\n",
            "('ft911_3', '1038')\n",
            "('ft911_3', '1039')\n",
            "('ft911_3', '1040')\n",
            "('ft911_3', '1041')\n",
            "('ft911_3', '1042')\n",
            "('ft911_3', '1043')\n",
            "('ft911_3', '1044')\n",
            "('ft911_3', '1045')\n",
            "('ft911_3', '1046')\n",
            "('ft911_3', '1047')\n",
            "('ft911_3', '1048')\n",
            "('ft911_3', '1049')\n",
            "('ft911_3', '1050')\n",
            "('ft911_3', '1051')\n",
            "('ft911_3', '1052')\n",
            "('ft911_3', '1053')\n",
            "('ft911_3', '1054')\n",
            "('ft911_3', '1055')\n",
            "('ft911_3', '1056')\n",
            "('ft911_3', '1057')\n",
            "('ft911_3', '1058')\n",
            "('ft911_3', '1059')\n",
            "('ft911_3', '1060')\n",
            "('ft911_3', '1061')\n",
            "('ft911_3', '1062')\n",
            "('ft911_3', '1063')\n",
            "('ft911_3', '1064')\n",
            "('ft911_3', '1065')\n",
            "('ft911_3', '1066')\n",
            "('ft911_3', '1067')\n",
            "('ft911_3', '1068')\n",
            "('ft911_3', '1069')\n",
            "('ft911_3', '1070')\n",
            "('ft911_3', '1071')\n",
            "('ft911_3', '1072')\n",
            "('ft911_3', '1073')\n",
            "('ft911_3', '1074')\n",
            "('ft911_3', '1075')\n",
            "('ft911_3', '1076')\n",
            "('ft911_3', '1077')\n",
            "('ft911_3', '1078')\n",
            "('ft911_3', '1079')\n",
            "('ft911_3', '1080')\n",
            "('ft911_3', '1081')\n",
            "('ft911_3', '1082')\n",
            "('ft911_3', '1083')\n",
            "('ft911_3', '1084')\n",
            "('ft911_3', '1085')\n",
            "('ft911_3', '1086')\n",
            "('ft911_3', '1087')\n",
            "('ft911_3', '1088')\n",
            "('ft911_3', '1089')\n",
            "('ft911_3', '1090')\n",
            "('ft911_3', '1091')\n",
            "('ft911_3', '1092')\n",
            "('ft911_3', '1093')\n",
            "('ft911_3', '1094')\n",
            "('ft911_3', '1095')\n",
            "('ft911_3', '1096')\n",
            "('ft911_3', '1097')\n",
            "('ft911_3', '1098')\n",
            "('ft911_4', '1099')\n",
            "('ft911_4', '1100')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load stopwords\n",
        "stopwords_path = \"/content/drive/MyDrive/InfoRetrieval/stopwordlist.txt\"\n",
        "with open(stopwords_path, 'r', encoding='utf-8') as f:\n",
        "    stopwords = set(f.read().split())\n",
        "\n",
        "# Initialize the stemmer\n",
        "word_stemmer = PorterStemmer()\n",
        "\n",
        "docs_path = \"/content/drive/MyDrive/InfoRetrieval/ft911/\"\n",
        "processed_docs = {}\n",
        "FTFileMapping = {}  # Mapping: file_name -> list of DOCNOs\n",
        "FileDictionary = {}  # Mapping: (file_name, local_doc_num) -> global_doc_id\n",
        "DocumentToFileMapping = []  # list of (file_name, numeric_DOCNO)\n",
        "\n",
        "# Sort files numerically based on suffix\n",
        "file_list = sorted(\n",
        "    [f for f in os.listdir(docs_path) if f.startswith(\"ft911_\")],\n",
        "    key=lambda x: int(re.findall(r'\\d+', x)[0])\n",
        ")\n",
        "\n",
        "# Process each file\n",
        "global_doc_id = 1\n",
        "for file_name in file_list:\n",
        "    full_file_path = os.path.join(docs_path, file_name)\n",
        "\n",
        "    if os.path.isfile(full_file_path):\n",
        "        print(f\"Checking file: {full_file_path}\")\n",
        "        with open(full_file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "            file_content = file.read()\n",
        "\n",
        "            # Parse TREC documents\n",
        "            parsed_docs = {}\n",
        "            current_doc_lines = []\n",
        "            current_doc_id = None\n",
        "            for line in file_content.splitlines():\n",
        "                if line.startswith(\"<DOCNO>\"):\n",
        "                    if current_doc_id is not None:\n",
        "                        parsed_docs[current_doc_id] = \" \".join(current_doc_lines)\n",
        "                    current_doc_id = line.replace(\"<DOCNO>\", \"\").replace(\"</DOCNO>\", \"\").strip()\n",
        "                    current_doc_lines = []\n",
        "                elif not line.startswith(\"<\") and current_doc_id:\n",
        "                    current_doc_lines.append(line.strip())\n",
        "            if current_doc_id is not None:\n",
        "                parsed_docs[current_doc_id] = \" \".join(current_doc_lines)\n",
        "\n",
        "            print(f\"Parsed documents (first 10): {list(parsed_docs.keys())[:10]}\")\n",
        "\n",
        "            # Save file -> DOCNOs\n",
        "            FTFileMapping[file_name] = list(parsed_docs.keys())\n",
        "\n",
        "            # Save cleaned (FT911-X, numeric_DOCNO)\n",
        "            for docno in parsed_docs.keys():\n",
        "                numeric_docno = docno.split(\"-\")[-1]\n",
        "                ft_formatted = f\"FT911-{file_name.split('_')[-1]}\"\n",
        "                DocumentToFileMapping.append((ft_formatted, numeric_docno))\n",
        "\n",
        "            # Assign global and local doc IDs\n",
        "            local_id = 1\n",
        "            for doc_id in parsed_docs:\n",
        "                FileDictionary[(file_name, local_id)] = global_doc_id\n",
        "                local_id += 1\n",
        "                global_doc_id += 1\n",
        "\n",
        "            # Process text\n",
        "            for doc_id, doc_text in sorted(parsed_docs.items()):\n",
        "                text = doc_text.lower()\n",
        "                text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
        "                tokens = nltk.word_tokenize(text)\n",
        "                tokens = [word for word in tokens if word.isalpha()]\n",
        "                filtered_tokens = [word for word in tokens if word not in stopwords]\n",
        "                stemmed_tokens = [word_stemmer.stem(word) for word in filtered_tokens]\n",
        "                processed_docs[doc_id] = stemmed_tokens\n",
        "\n",
        "# Create WordDictionary\n",
        "WordDictionary = {}\n",
        "current_word_id = 1\n",
        "for words in processed_docs.values():\n",
        "    for word in words:\n",
        "        if word not in WordDictionary:\n",
        "            WordDictionary[word] = current_word_id\n",
        "            current_word_id += 1\n",
        "\n",
        "# Sort dictionaries\n",
        "WordDictionary = dict(sorted(WordDictionary.items()))\n",
        "FileDictionary = dict(sorted(FileDictionary.items()))\n",
        "\n",
        "# Save WordDictionary\n",
        "word_dict_path = \"/content/drive/MyDrive/InfoRetrieval/word_dictionary.txt\"\n",
        "with open(word_dict_path, \"w\") as word_file:\n",
        "    for word, word_id in WordDictionary.items():\n",
        "        word_file.write(f\"{word}\\t{word_id}\\n\")\n",
        "\n",
        "# Save FileDictionary (filename + local doc number)\n",
        "file_dict_path = \"/content/drive/MyDrive/InfoRetrieval/file_dictionary.txt\"\n",
        "with open(file_dict_path, \"w\") as file_file:\n",
        "    for (file_name, local_doc_num), global_id in FileDictionary.items():\n",
        "        file_file.write(f\"{file_name}\\t{local_doc_num}\\t{global_id}\\n\")\n",
        "\n",
        "# Save FTFileMapping (file -> list of DOCNOs)\n",
        "ft_map_path = \"/content/drive/MyDrive/InfoRetrieval/ft_file_mapping.txt\"\n",
        "with open(ft_map_path, \"w\") as map_file:\n",
        "    for file, doc_ids in FTFileMapping.items():\n",
        "        map_file.write(f\"{file}:\\t{', '.join(doc_ids)}\\n\")\n",
        "\n",
        "# Save final formatted DocumentToFileMapping\n",
        "doc_file_map_path = \"/content/drive/MyDrive/InfoRetrieval/doc_to_file_mapping.txt\"\n",
        "with open(doc_file_map_path, \"w\") as out_file:\n",
        "    for formatted_file, numeric_docno in DocumentToFileMapping:\n",
        "        out_file.write(f\"{formatted_file}\\t{numeric_docno}\\n\")\n",
        "\n",
        "# Logs\n",
        "print(f\"WordDictionary saved to {word_dict_path}\")\n",
        "print(f\"FileDictionary saved to {file_dict_path}\")\n",
        "print(f\"FTFileMapping saved to {ft_map_path}\")\n",
        "print(f\"DocumentToFileMapping saved to {doc_file_map_path}\")\n",
        "\n",
        "print(\"Token and Token ID Mapping (First 100):\")\n",
        "for token, token_id in list(WordDictionary.items())[:100]:\n",
        "    print(f\"{token}\\t{token_id}\")\n",
        "\n",
        "# Print the final formatted output\n",
        "print(\"DocumentToFileMapping (First 1000):\")\n",
        "for formatted_file, numeric_docno in DocumentToFileMapping[717:1100]:\n",
        "    print((formatted_file, numeric_docno))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QksyK75tK2X",
        "outputId": "baf1188a-0640-47e8-e6fa-f588073dfcd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_1\n",
            "Parsed documents (first 10): ['FT911-1', 'FT911-2', 'FT911-3', 'FT911-4', 'FT911-5', 'FT911-6', 'FT911-7', 'FT911-8', 'FT911-9', 'FT911-10']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_2\n",
            "Parsed documents (first 10): ['FT911-376', 'FT911-377', 'FT911-378', 'FT911-379', 'FT911-380', 'FT911-381', 'FT911-382', 'FT911-383', 'FT911-384', 'FT911-385']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_3\n",
            "Parsed documents (first 10): ['FT911-722', 'FT911-723', 'FT911-724', 'FT911-725', 'FT911-726', 'FT911-727', 'FT911-728', 'FT911-729', 'FT911-730', 'FT911-731']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_4\n",
            "Parsed documents (first 10): ['FT911-1099', 'FT911-1100', 'FT911-1101', 'FT911-1102', 'FT911-1103', 'FT911-1104', 'FT911-1105', 'FT911-1106', 'FT911-1107', 'FT911-1108']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_5\n",
            "Parsed documents (first 10): ['FT911-1479', 'FT911-1480', 'FT911-1481', 'FT911-1482', 'FT911-1483', 'FT911-1484', 'FT911-1485', 'FT911-1486', 'FT911-1487', 'FT911-1488']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_6\n",
            "Parsed documents (first 10): ['FT911-1832', 'FT911-1833', 'FT911-1834', 'FT911-1835', 'FT911-1836', 'FT911-1837', 'FT911-1838', 'FT911-1839', 'FT911-1840', 'FT911-1841']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_7\n",
            "Parsed documents (first 10): ['FT911-2212', 'FT911-2213', 'FT911-2214', 'FT911-2215', 'FT911-2216', 'FT911-2217', 'FT911-2218', 'FT911-2219', 'FT911-2220', 'FT911-2221']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_8\n",
            "Parsed documents (first 10): ['FT911-2621', 'FT911-2622', 'FT911-2623', 'FT911-2624', 'FT911-2625', 'FT911-2626', 'FT911-2627', 'FT911-2628', 'FT911-2629', 'FT911-2630']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_9\n",
            "Parsed documents (first 10): ['FT911-2950', 'FT911-2951', 'FT911-2952', 'FT911-2953', 'FT911-2954', 'FT911-2955', 'FT911-2956', 'FT911-2957', 'FT911-2958', 'FT911-2959']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_10\n",
            "Parsed documents (first 10): ['FT911-3323', 'FT911-3324', 'FT911-3325', 'FT911-3326', 'FT911-3327', 'FT911-3328', 'FT911-3329', 'FT911-3330', 'FT911-3331', 'FT911-3332']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_11\n",
            "Parsed documents (first 10): ['FT911-3694', 'FT911-3695', 'FT911-3696', 'FT911-3697', 'FT911-3698', 'FT911-3699', 'FT911-3700', 'FT911-3701', 'FT911-3702', 'FT911-3703']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_12\n",
            "Parsed documents (first 10): ['FT911-4016', 'FT911-4017', 'FT911-4018', 'FT911-4019', 'FT911-4020', 'FT911-4021', 'FT911-4022', 'FT911-4023', 'FT911-4024', 'FT911-4025']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_13\n",
            "Parsed documents (first 10): ['FT911-4366', 'FT911-4367', 'FT911-4368', 'FT911-4369', 'FT911-4370', 'FT911-4371', 'FT911-4372', 'FT911-4373', 'FT911-4374', 'FT911-4375']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_15\n",
            "Parsed documents (first 10): ['FT911-5129', 'FT911-5130', 'FT911-5131', 'FT911-5132', 'FT911-5133', 'FT911-5134', 'FT911-5135', 'FT911-5136', 'FT911-5137', 'FT911-5138']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_14\n",
            "Parsed documents (first 10): ['FT911-4749', 'FT911-4750', 'FT911-4751', 'FT911-4752', 'FT911-4753', 'FT911-4754', 'FT911-4755', 'FT911-4756', 'FT911-4757', 'FT911-4758']\n",
            "WordDictionary saved to /content/drive/MyDrive/InfoRetrieval/word_dictionary.txt\n",
            "FileDictionary saved to /content/drive/MyDrive/InfoRetrieval/file_dictionary.txt\n",
            "FTFileMapping saved to /content/drive/MyDrive/InfoRetrieval/ft_file_mapping.txt\n",
            "DocumentToFileMapping saved to /content/drive/MyDrive/InfoRetrieval/doc_to_file_mapping.txt\n",
            "Token and Token ID Mapping (First 100):\n",
            "aa\t5542\n",
            "aaa\t13313\n",
            "aachen\t14777\n",
            "aaf\t7625\n",
            "aah\t20455\n",
            "aakvaag\t32703\n",
            "aalborg\t30434\n",
            "aaron\t17933\n",
            "ab\t4195\n",
            "ababa\t22651\n",
            "aback\t13024\n",
            "abalkin\t1117\n",
            "abandon\t1408\n",
            "abash\t15986\n",
            "abat\t13479\n",
            "abattoir\t22113\n",
            "abb\t1742\n",
            "abba\t22179\n",
            "abbacchio\t18245\n",
            "abbado\t6021\n",
            "abbatoir\t20349\n",
            "abbey\t11270\n",
            "abbot\t18730\n",
            "abbott\t3736\n",
            "abbrevi\t20194\n",
            "abc\t15580\n",
            "abcc\t22751\n",
            "abcd\t30331\n",
            "abci\t27009\n",
            "abdali\t23047\n",
            "abdel\t21093\n",
            "abdelaziz\t24529\n",
            "abdic\t13992\n",
            "abduct\t6413\n",
            "abdul\t5892\n",
            "abdulla\t12153\n",
            "abdullah\t20274\n",
            "abe\t32960\n",
            "abel\t6148\n",
            "abela\t27132\n",
            "abercorn\t24021\n",
            "aberdeen\t3026\n",
            "aberdeenshir\t15507\n",
            "aberforth\t9459\n",
            "abergavenni\t12926\n",
            "aberr\t16785\n",
            "abet\t20983\n",
            "abey\t17054\n",
            "abhor\t8237\n",
            "abhorr\t17326\n",
            "abi\t30652\n",
            "abid\t9704\n",
            "abidin\t28714\n",
            "abil\t1673\n",
            "abingdon\t15659\n",
            "abington\t24389\n",
            "abingworth\t27431\n",
            "abitibi\t20809\n",
            "abitur\t31992\n",
            "abject\t17375\n",
            "abk\t30859\n",
            "ablaz\t18224\n",
            "abli\t13581\n",
            "ablitt\t26515\n",
            "ablut\t10456\n",
            "abn\t13229\n",
            "abnorm\t14074\n",
            "aboard\t18096\n",
            "abod\t29376\n",
            "abol\t28004\n",
            "abolhassan\t19623\n",
            "abolish\t6745\n",
            "abolit\t8151\n",
            "abolitionist\t26881\n",
            "abomin\t32251\n",
            "aborigin\t14709\n",
            "abort\t5298\n",
            "abouloff\t14142\n",
            "abound\t2537\n",
            "abouthow\t23620\n",
            "aboutth\t23185\n",
            "abpi\t31966\n",
            "abraham\t3935\n",
            "abram\t4987\n",
            "abramowitz\t18199\n",
            "abras\t12705\n",
            "abreast\t20229\n",
            "abridg\t6520\n",
            "abroad\t344\n",
            "abrog\t20237\n",
            "abrupt\t4555\n",
            "abruptli\t18040\n",
            "abruzzo\t31574\n",
            "absa\t16978\n",
            "abscond\t31613\n",
            "absenc\t1413\n",
            "absent\t1645\n",
            "absente\t12099\n",
            "absentia\t27146\n",
            "absinth\t17797\n",
            "DocumentToFileMapping (First 1000):\n",
            "('FT911-2', '718')\n",
            "('FT911-2', '719')\n",
            "('FT911-2', '720')\n",
            "('FT911-2', '721')\n",
            "('FT911-3', '722')\n",
            "('FT911-3', '723')\n",
            "('FT911-3', '724')\n",
            "('FT911-3', '725')\n",
            "('FT911-3', '726')\n",
            "('FT911-3', '727')\n",
            "('FT911-3', '728')\n",
            "('FT911-3', '729')\n",
            "('FT911-3', '730')\n",
            "('FT911-3', '731')\n",
            "('FT911-3', '732')\n",
            "('FT911-3', '733')\n",
            "('FT911-3', '734')\n",
            "('FT911-3', '735')\n",
            "('FT911-3', '736')\n",
            "('FT911-3', '737')\n",
            "('FT911-3', '738')\n",
            "('FT911-3', '739')\n",
            "('FT911-3', '740')\n",
            "('FT911-3', '741')\n",
            "('FT911-3', '742')\n",
            "('FT911-3', '743')\n",
            "('FT911-3', '744')\n",
            "('FT911-3', '745')\n",
            "('FT911-3', '746')\n",
            "('FT911-3', '747')\n",
            "('FT911-3', '748')\n",
            "('FT911-3', '749')\n",
            "('FT911-3', '750')\n",
            "('FT911-3', '751')\n",
            "('FT911-3', '752')\n",
            "('FT911-3', '753')\n",
            "('FT911-3', '754')\n",
            "('FT911-3', '755')\n",
            "('FT911-3', '756')\n",
            "('FT911-3', '757')\n",
            "('FT911-3', '758')\n",
            "('FT911-3', '759')\n",
            "('FT911-3', '760')\n",
            "('FT911-3', '761')\n",
            "('FT911-3', '762')\n",
            "('FT911-3', '763')\n",
            "('FT911-3', '764')\n",
            "('FT911-3', '765')\n",
            "('FT911-3', '766')\n",
            "('FT911-3', '767')\n",
            "('FT911-3', '768')\n",
            "('FT911-3', '769')\n",
            "('FT911-3', '770')\n",
            "('FT911-3', '771')\n",
            "('FT911-3', '772')\n",
            "('FT911-3', '773')\n",
            "('FT911-3', '774')\n",
            "('FT911-3', '775')\n",
            "('FT911-3', '776')\n",
            "('FT911-3', '777')\n",
            "('FT911-3', '778')\n",
            "('FT911-3', '779')\n",
            "('FT911-3', '780')\n",
            "('FT911-3', '781')\n",
            "('FT911-3', '782')\n",
            "('FT911-3', '783')\n",
            "('FT911-3', '784')\n",
            "('FT911-3', '785')\n",
            "('FT911-3', '786')\n",
            "('FT911-3', '787')\n",
            "('FT911-3', '788')\n",
            "('FT911-3', '789')\n",
            "('FT911-3', '790')\n",
            "('FT911-3', '791')\n",
            "('FT911-3', '792')\n",
            "('FT911-3', '793')\n",
            "('FT911-3', '794')\n",
            "('FT911-3', '795')\n",
            "('FT911-3', '796')\n",
            "('FT911-3', '797')\n",
            "('FT911-3', '798')\n",
            "('FT911-3', '799')\n",
            "('FT911-3', '800')\n",
            "('FT911-3', '801')\n",
            "('FT911-3', '802')\n",
            "('FT911-3', '803')\n",
            "('FT911-3', '804')\n",
            "('FT911-3', '805')\n",
            "('FT911-3', '806')\n",
            "('FT911-3', '807')\n",
            "('FT911-3', '808')\n",
            "('FT911-3', '809')\n",
            "('FT911-3', '810')\n",
            "('FT911-3', '811')\n",
            "('FT911-3', '812')\n",
            "('FT911-3', '813')\n",
            "('FT911-3', '814')\n",
            "('FT911-3', '815')\n",
            "('FT911-3', '816')\n",
            "('FT911-3', '817')\n",
            "('FT911-3', '818')\n",
            "('FT911-3', '819')\n",
            "('FT911-3', '820')\n",
            "('FT911-3', '821')\n",
            "('FT911-3', '822')\n",
            "('FT911-3', '823')\n",
            "('FT911-3', '824')\n",
            "('FT911-3', '825')\n",
            "('FT911-3', '826')\n",
            "('FT911-3', '827')\n",
            "('FT911-3', '828')\n",
            "('FT911-3', '829')\n",
            "('FT911-3', '830')\n",
            "('FT911-3', '831')\n",
            "('FT911-3', '832')\n",
            "('FT911-3', '833')\n",
            "('FT911-3', '834')\n",
            "('FT911-3', '835')\n",
            "('FT911-3', '836')\n",
            "('FT911-3', '837')\n",
            "('FT911-3', '838')\n",
            "('FT911-3', '839')\n",
            "('FT911-3', '840')\n",
            "('FT911-3', '841')\n",
            "('FT911-3', '842')\n",
            "('FT911-3', '843')\n",
            "('FT911-3', '844')\n",
            "('FT911-3', '845')\n",
            "('FT911-3', '846')\n",
            "('FT911-3', '847')\n",
            "('FT911-3', '848')\n",
            "('FT911-3', '849')\n",
            "('FT911-3', '850')\n",
            "('FT911-3', '851')\n",
            "('FT911-3', '852')\n",
            "('FT911-3', '853')\n",
            "('FT911-3', '854')\n",
            "('FT911-3', '855')\n",
            "('FT911-3', '856')\n",
            "('FT911-3', '857')\n",
            "('FT911-3', '858')\n",
            "('FT911-3', '859')\n",
            "('FT911-3', '860')\n",
            "('FT911-3', '861')\n",
            "('FT911-3', '862')\n",
            "('FT911-3', '863')\n",
            "('FT911-3', '864')\n",
            "('FT911-3', '865')\n",
            "('FT911-3', '866')\n",
            "('FT911-3', '867')\n",
            "('FT911-3', '868')\n",
            "('FT911-3', '869')\n",
            "('FT911-3', '870')\n",
            "('FT911-3', '871')\n",
            "('FT911-3', '872')\n",
            "('FT911-3', '873')\n",
            "('FT911-3', '874')\n",
            "('FT911-3', '875')\n",
            "('FT911-3', '876')\n",
            "('FT911-3', '877')\n",
            "('FT911-3', '878')\n",
            "('FT911-3', '879')\n",
            "('FT911-3', '880')\n",
            "('FT911-3', '881')\n",
            "('FT911-3', '882')\n",
            "('FT911-3', '883')\n",
            "('FT911-3', '884')\n",
            "('FT911-3', '885')\n",
            "('FT911-3', '886')\n",
            "('FT911-3', '887')\n",
            "('FT911-3', '888')\n",
            "('FT911-3', '889')\n",
            "('FT911-3', '890')\n",
            "('FT911-3', '891')\n",
            "('FT911-3', '892')\n",
            "('FT911-3', '893')\n",
            "('FT911-3', '894')\n",
            "('FT911-3', '895')\n",
            "('FT911-3', '896')\n",
            "('FT911-3', '897')\n",
            "('FT911-3', '898')\n",
            "('FT911-3', '899')\n",
            "('FT911-3', '900')\n",
            "('FT911-3', '901')\n",
            "('FT911-3', '902')\n",
            "('FT911-3', '903')\n",
            "('FT911-3', '904')\n",
            "('FT911-3', '905')\n",
            "('FT911-3', '906')\n",
            "('FT911-3', '907')\n",
            "('FT911-3', '908')\n",
            "('FT911-3', '909')\n",
            "('FT911-3', '910')\n",
            "('FT911-3', '911')\n",
            "('FT911-3', '912')\n",
            "('FT911-3', '913')\n",
            "('FT911-3', '914')\n",
            "('FT911-3', '915')\n",
            "('FT911-3', '916')\n",
            "('FT911-3', '917')\n",
            "('FT911-3', '918')\n",
            "('FT911-3', '919')\n",
            "('FT911-3', '920')\n",
            "('FT911-3', '921')\n",
            "('FT911-3', '922')\n",
            "('FT911-3', '923')\n",
            "('FT911-3', '924')\n",
            "('FT911-3', '925')\n",
            "('FT911-3', '926')\n",
            "('FT911-3', '927')\n",
            "('FT911-3', '928')\n",
            "('FT911-3', '929')\n",
            "('FT911-3', '930')\n",
            "('FT911-3', '931')\n",
            "('FT911-3', '932')\n",
            "('FT911-3', '933')\n",
            "('FT911-3', '934')\n",
            "('FT911-3', '935')\n",
            "('FT911-3', '936')\n",
            "('FT911-3', '937')\n",
            "('FT911-3', '938')\n",
            "('FT911-3', '939')\n",
            "('FT911-3', '940')\n",
            "('FT911-3', '941')\n",
            "('FT911-3', '942')\n",
            "('FT911-3', '943')\n",
            "('FT911-3', '944')\n",
            "('FT911-3', '945')\n",
            "('FT911-3', '946')\n",
            "('FT911-3', '947')\n",
            "('FT911-3', '948')\n",
            "('FT911-3', '949')\n",
            "('FT911-3', '950')\n",
            "('FT911-3', '951')\n",
            "('FT911-3', '952')\n",
            "('FT911-3', '953')\n",
            "('FT911-3', '954')\n",
            "('FT911-3', '955')\n",
            "('FT911-3', '956')\n",
            "('FT911-3', '957')\n",
            "('FT911-3', '958')\n",
            "('FT911-3', '959')\n",
            "('FT911-3', '960')\n",
            "('FT911-3', '961')\n",
            "('FT911-3', '962')\n",
            "('FT911-3', '963')\n",
            "('FT911-3', '964')\n",
            "('FT911-3', '965')\n",
            "('FT911-3', '966')\n",
            "('FT911-3', '967')\n",
            "('FT911-3', '968')\n",
            "('FT911-3', '969')\n",
            "('FT911-3', '970')\n",
            "('FT911-3', '971')\n",
            "('FT911-3', '972')\n",
            "('FT911-3', '973')\n",
            "('FT911-3', '974')\n",
            "('FT911-3', '975')\n",
            "('FT911-3', '976')\n",
            "('FT911-3', '977')\n",
            "('FT911-3', '978')\n",
            "('FT911-3', '979')\n",
            "('FT911-3', '980')\n",
            "('FT911-3', '981')\n",
            "('FT911-3', '982')\n",
            "('FT911-3', '983')\n",
            "('FT911-3', '984')\n",
            "('FT911-3', '985')\n",
            "('FT911-3', '986')\n",
            "('FT911-3', '987')\n",
            "('FT911-3', '988')\n",
            "('FT911-3', '989')\n",
            "('FT911-3', '990')\n",
            "('FT911-3', '991')\n",
            "('FT911-3', '992')\n",
            "('FT911-3', '993')\n",
            "('FT911-3', '994')\n",
            "('FT911-3', '995')\n",
            "('FT911-3', '996')\n",
            "('FT911-3', '997')\n",
            "('FT911-3', '998')\n",
            "('FT911-3', '999')\n",
            "('FT911-3', '1000')\n",
            "('FT911-3', '1001')\n",
            "('FT911-3', '1002')\n",
            "('FT911-3', '1003')\n",
            "('FT911-3', '1004')\n",
            "('FT911-3', '1005')\n",
            "('FT911-3', '1006')\n",
            "('FT911-3', '1007')\n",
            "('FT911-3', '1008')\n",
            "('FT911-3', '1009')\n",
            "('FT911-3', '1010')\n",
            "('FT911-3', '1011')\n",
            "('FT911-3', '1012')\n",
            "('FT911-3', '1013')\n",
            "('FT911-3', '1014')\n",
            "('FT911-3', '1015')\n",
            "('FT911-3', '1016')\n",
            "('FT911-3', '1017')\n",
            "('FT911-3', '1018')\n",
            "('FT911-3', '1019')\n",
            "('FT911-3', '1020')\n",
            "('FT911-3', '1021')\n",
            "('FT911-3', '1022')\n",
            "('FT911-3', '1023')\n",
            "('FT911-3', '1024')\n",
            "('FT911-3', '1025')\n",
            "('FT911-3', '1026')\n",
            "('FT911-3', '1027')\n",
            "('FT911-3', '1028')\n",
            "('FT911-3', '1029')\n",
            "('FT911-3', '1030')\n",
            "('FT911-3', '1031')\n",
            "('FT911-3', '1032')\n",
            "('FT911-3', '1033')\n",
            "('FT911-3', '1034')\n",
            "('FT911-3', '1035')\n",
            "('FT911-3', '1036')\n",
            "('FT911-3', '1037')\n",
            "('FT911-3', '1038')\n",
            "('FT911-3', '1039')\n",
            "('FT911-3', '1040')\n",
            "('FT911-3', '1041')\n",
            "('FT911-3', '1042')\n",
            "('FT911-3', '1043')\n",
            "('FT911-3', '1044')\n",
            "('FT911-3', '1045')\n",
            "('FT911-3', '1046')\n",
            "('FT911-3', '1047')\n",
            "('FT911-3', '1048')\n",
            "('FT911-3', '1049')\n",
            "('FT911-3', '1050')\n",
            "('FT911-3', '1051')\n",
            "('FT911-3', '1052')\n",
            "('FT911-3', '1053')\n",
            "('FT911-3', '1054')\n",
            "('FT911-3', '1055')\n",
            "('FT911-3', '1056')\n",
            "('FT911-3', '1057')\n",
            "('FT911-3', '1058')\n",
            "('FT911-3', '1059')\n",
            "('FT911-3', '1060')\n",
            "('FT911-3', '1061')\n",
            "('FT911-3', '1062')\n",
            "('FT911-3', '1063')\n",
            "('FT911-3', '1064')\n",
            "('FT911-3', '1065')\n",
            "('FT911-3', '1066')\n",
            "('FT911-3', '1067')\n",
            "('FT911-3', '1068')\n",
            "('FT911-3', '1069')\n",
            "('FT911-3', '1070')\n",
            "('FT911-3', '1071')\n",
            "('FT911-3', '1072')\n",
            "('FT911-3', '1073')\n",
            "('FT911-3', '1074')\n",
            "('FT911-3', '1075')\n",
            "('FT911-3', '1076')\n",
            "('FT911-3', '1077')\n",
            "('FT911-3', '1078')\n",
            "('FT911-3', '1079')\n",
            "('FT911-3', '1080')\n",
            "('FT911-3', '1081')\n",
            "('FT911-3', '1082')\n",
            "('FT911-3', '1083')\n",
            "('FT911-3', '1084')\n",
            "('FT911-3', '1085')\n",
            "('FT911-3', '1086')\n",
            "('FT911-3', '1087')\n",
            "('FT911-3', '1088')\n",
            "('FT911-3', '1089')\n",
            "('FT911-3', '1090')\n",
            "('FT911-3', '1091')\n",
            "('FT911-3', '1092')\n",
            "('FT911-3', '1093')\n",
            "('FT911-3', '1094')\n",
            "('FT911-3', '1095')\n",
            "('FT911-3', '1096')\n",
            "('FT911-3', '1097')\n",
            "('FT911-3', '1098')\n",
            "('FT911-4', '1099')\n",
            "('FT911-4', '1100')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine word_dictionary.txt and doc_to_file_mapping.txt into parser_output.txt\n",
        "parser_output_path = \"/content/drive/MyDrive/InfoRetrieval/parser_output.txt\"\n",
        "word_dict_path = \"/content/drive/MyDrive/InfoRetrieval/word_dictionary.txt\"\n",
        "doc_file_map_path = \"/content/drive/MyDrive/InfoRetrieval/doc_to_file_mapping.txt\"\n",
        "\n",
        "with open(parser_output_path, \"w\") as output_file:\n",
        "    # Write word dictionary first\n",
        "    with open(word_dict_path, \"r\") as word_file:\n",
        "        output_file.write(\"### Word Dictionary ###\\n\")\n",
        "        output_file.writelines(word_file.readlines())\n",
        "        output_file.write(\"\\n\")\n",
        "\n",
        "    # Write document-to-file mapping next\n",
        "    with open(doc_file_map_path, \"r\") as map_file:\n",
        "        output_file.write(\"### Document to File Mapping ###\\n\")\n",
        "        output_file.writelines(map_file.readlines())\n",
        "\n",
        "print(f\"Combined output saved to {parser_output_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2W-XY8Suqur",
        "outputId": "058dee33-a36f-44b0-bbab-cbf138a07d96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined output saved to /content/drive/MyDrive/InfoRetrieval/parser_output.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load stopwords\n",
        "stopwords_path = \"/content/drive/MyDrive/InfoRetrieval/stopwordlist.txt\"\n",
        "with open(stopwords_path, 'r', encoding='utf-8') as f:\n",
        "    stopwords = set(f.read().split())\n",
        "\n",
        "# Initialize the stemmer\n",
        "word_stemmer = PorterStemmer()\n",
        "\n",
        "docs_path = \"/content/drive/MyDrive/InfoRetrieval/ft911/\"\n",
        "processed_docs = {}\n",
        "FTFileMapping = {}\n",
        "FileDictionary = {}\n",
        "DocumentToFileMapping = []\n",
        "\n",
        "# Sort files numerically based on suffix\n",
        "file_list = sorted(\n",
        "    [f for f in os.listdir(docs_path) if f.startswith(\"ft911_\")],\n",
        "    key=lambda x: int(re.findall(r'\\d+', x)[0])\n",
        ")\n",
        "\n",
        "# Process each file\n",
        "global_doc_id = 1\n",
        "for file_name in file_list:\n",
        "    full_file_path = os.path.join(docs_path, file_name)\n",
        "\n",
        "    if os.path.isfile(full_file_path):\n",
        "        print(f\"Checking file: {full_file_path}\")\n",
        "        with open(full_file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "            file_content = file.read()\n",
        "\n",
        "            # Parse TREC documents\n",
        "            parsed_docs = {}\n",
        "            current_doc_lines = []\n",
        "            current_doc_id = None\n",
        "            for line in file_content.splitlines():\n",
        "                if line.startswith(\"<DOCNO>\"):\n",
        "                    if current_doc_id is not None:\n",
        "                        parsed_docs[current_doc_id] = \" \".join(current_doc_lines)\n",
        "                    current_doc_id = line.replace(\"<DOCNO>\", \"\").replace(\"</DOCNO>\", \"\").strip()\n",
        "                    current_doc_lines = []\n",
        "                elif not line.startswith(\"<\") and current_doc_id:\n",
        "                    current_doc_lines.append(line.strip())\n",
        "            if current_doc_id is not None:\n",
        "                parsed_docs[current_doc_id] = \" \".join(current_doc_lines)\n",
        "\n",
        "            print(f\"Parsed documents (first 10): {list(parsed_docs.keys())[:10]}\")\n",
        "\n",
        "            # Save file -> DOCNOs\n",
        "            FTFileMapping[file_name] = list(parsed_docs.keys())\n",
        "\n",
        "            # Save cleaned (FT911-X, numeric_DOCNO)\n",
        "            for docno in parsed_docs.keys():\n",
        "                numeric_docno = docno.split(\"-\")[-1]\n",
        "                ft_formatted = f\"FT911-{file_name.split('_')[-1]}\"\n",
        "                DocumentToFileMapping.append((ft_formatted, numeric_docno))\n",
        "\n",
        "            # Assign global and local doc IDs\n",
        "            local_id = 1\n",
        "            for doc_id in parsed_docs:\n",
        "                FileDictionary[(file_name, local_id)] = global_doc_id\n",
        "                local_id += 1\n",
        "                global_doc_id += 1\n",
        "\n",
        "            # Process text\n",
        "            for doc_id, doc_text in sorted(parsed_docs.items()):\n",
        "                text = doc_text.lower()\n",
        "                text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
        "                tokens = nltk.word_tokenize(text)\n",
        "                tokens = [word for word in tokens if word.isalpha()]\n",
        "                filtered_tokens = [word for word in tokens if word not in stopwords]\n",
        "                stemmed_tokens = [word_stemmer.stem(word) for word in filtered_tokens]\n",
        "                processed_docs[doc_id] = stemmed_tokens\n",
        "\n",
        "# Create WordDictionary\n",
        "WordDictionary = {}\n",
        "current_word_id = 1\n",
        "for words in processed_docs.values():\n",
        "    for word in words:\n",
        "        if word not in WordDictionary:\n",
        "            WordDictionary[word] = current_word_id\n",
        "            current_word_id += 1\n",
        "\n",
        "# Sort dictionaries\n",
        "WordDictionary = dict(sorted(WordDictionary.items()))\n",
        "FileDictionary = dict(sorted(FileDictionary.items()))\n",
        "\n",
        "# Save WordDictionary\n",
        "word_dict_path = \"/content/drive/MyDrive/InfoRetrieval/word_dictionary.txt\"\n",
        "with open(word_dict_path, \"w\") as word_file:\n",
        "    for word, word_id in WordDictionary.items():\n",
        "        word_file.write(f\"{word}\\t{word_id}\\n\")\n",
        "\n",
        "# Save FileDictionary\n",
        "file_dict_path = \"/content/drive/MyDrive/InfoRetrieval/file_dictionary.txt\"\n",
        "with open(file_dict_path, \"w\") as file_file:\n",
        "    for (file_name, local_doc_num), global_id in FileDictionary.items():\n",
        "        file_file.write(f\"{file_name}\\t{local_doc_num}\\t{global_id}\\n\")\n",
        "\n",
        "# Save FTFileMapping\n",
        "ft_map_path = \"/content/drive/MyDrive/InfoRetrieval/ft_file_mapping.txt\"\n",
        "with open(ft_map_path, \"w\") as map_file:\n",
        "    for file, doc_ids in FTFileMapping.items():\n",
        "        map_file.write(f\"{file}:\\t{', '.join(doc_ids)}\\n\")\n",
        "\n",
        "# ✅ Sort DocumentToFileMapping numerically by FT file number and then by docno\n",
        "def sort_key(item):\n",
        "    ft_number = int(item[0].split(\"-\")[-1])\n",
        "    doc_number = int(item[1])\n",
        "    return (ft_number, doc_number)\n",
        "\n",
        "DocumentToFileMapping.sort(key=sort_key)\n",
        "\n",
        "# Save final formatted DocumentToFileMapping\n",
        "doc_file_map_path = \"/content/drive/MyDrive/InfoRetrieval/doc_to_file_mapping.txt\"\n",
        "with open(doc_file_map_path, \"w\") as out_file:\n",
        "    for formatted_file, numeric_docno in DocumentToFileMapping:\n",
        "        out_file.write(f\"{formatted_file}\\t{numeric_docno}\\n\")\n",
        "\n",
        "# ✅ Combine into parser_output.txt\n",
        "parser_output_path = \"/content/drive/MyDrive/InfoRetrieval/parser_output.txt\"\n",
        "with open(parser_output_path, \"w\") as output_file:\n",
        "    # Write word dictionary\n",
        "    output_file.write(\"### token:  token ID ###\\n\")\n",
        "    with open(word_dict_path, \"r\") as word_file:\n",
        "        output_file.writelines(word_file.readlines())\n",
        "    output_file.write(\"\\n\")\n",
        "\n",
        "    # Write sorted document-to-file mapping\n",
        "    output_file.write(\"### document name: doc ID ###\\n\")\n",
        "    with open(doc_file_map_path, \"r\") as map_file:\n",
        "        output_file.writelines(map_file.readlines())\n",
        "\n",
        "# ✅ Logs\n",
        "print(f\"WordDictionary saved to {word_dict_path}\")\n",
        "print(f\"FileDictionary saved to {file_dict_path}\")\n",
        "print(f\"FTFileMapping saved to {ft_map_path}\")\n",
        "print(f\"DocumentToFileMapping saved to {doc_file_map_path}\")\n",
        "print(f\"Combined parser_output saved to {parser_output_path}\")\n",
        "\n",
        "# Optional print\n",
        "print(\"DocumentToFileMapping (Sample):\")\n",
        "for entry in DocumentToFileMapping[-10:]:\n",
        "    print(entry)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcEGcneDw5en",
        "outputId": "4cd1d309-f2bd-47e9-ac90-4cbb9b5a56e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_1\n",
            "Parsed documents (first 10): ['FT911-1', 'FT911-2', 'FT911-3', 'FT911-4', 'FT911-5', 'FT911-6', 'FT911-7', 'FT911-8', 'FT911-9', 'FT911-10']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_2\n",
            "Parsed documents (first 10): ['FT911-376', 'FT911-377', 'FT911-378', 'FT911-379', 'FT911-380', 'FT911-381', 'FT911-382', 'FT911-383', 'FT911-384', 'FT911-385']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_3\n",
            "Parsed documents (first 10): ['FT911-722', 'FT911-723', 'FT911-724', 'FT911-725', 'FT911-726', 'FT911-727', 'FT911-728', 'FT911-729', 'FT911-730', 'FT911-731']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_4\n",
            "Parsed documents (first 10): ['FT911-1099', 'FT911-1100', 'FT911-1101', 'FT911-1102', 'FT911-1103', 'FT911-1104', 'FT911-1105', 'FT911-1106', 'FT911-1107', 'FT911-1108']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_5\n",
            "Parsed documents (first 10): ['FT911-1479', 'FT911-1480', 'FT911-1481', 'FT911-1482', 'FT911-1483', 'FT911-1484', 'FT911-1485', 'FT911-1486', 'FT911-1487', 'FT911-1488']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_6\n",
            "Parsed documents (first 10): ['FT911-1832', 'FT911-1833', 'FT911-1834', 'FT911-1835', 'FT911-1836', 'FT911-1837', 'FT911-1838', 'FT911-1839', 'FT911-1840', 'FT911-1841']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_7\n",
            "Parsed documents (first 10): ['FT911-2212', 'FT911-2213', 'FT911-2214', 'FT911-2215', 'FT911-2216', 'FT911-2217', 'FT911-2218', 'FT911-2219', 'FT911-2220', 'FT911-2221']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_8\n",
            "Parsed documents (first 10): ['FT911-2621', 'FT911-2622', 'FT911-2623', 'FT911-2624', 'FT911-2625', 'FT911-2626', 'FT911-2627', 'FT911-2628', 'FT911-2629', 'FT911-2630']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_9\n",
            "Parsed documents (first 10): ['FT911-2950', 'FT911-2951', 'FT911-2952', 'FT911-2953', 'FT911-2954', 'FT911-2955', 'FT911-2956', 'FT911-2957', 'FT911-2958', 'FT911-2959']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_10\n",
            "Parsed documents (first 10): ['FT911-3323', 'FT911-3324', 'FT911-3325', 'FT911-3326', 'FT911-3327', 'FT911-3328', 'FT911-3329', 'FT911-3330', 'FT911-3331', 'FT911-3332']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_11\n",
            "Parsed documents (first 10): ['FT911-3694', 'FT911-3695', 'FT911-3696', 'FT911-3697', 'FT911-3698', 'FT911-3699', 'FT911-3700', 'FT911-3701', 'FT911-3702', 'FT911-3703']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_12\n",
            "Parsed documents (first 10): ['FT911-4016', 'FT911-4017', 'FT911-4018', 'FT911-4019', 'FT911-4020', 'FT911-4021', 'FT911-4022', 'FT911-4023', 'FT911-4024', 'FT911-4025']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_13\n",
            "Parsed documents (first 10): ['FT911-4366', 'FT911-4367', 'FT911-4368', 'FT911-4369', 'FT911-4370', 'FT911-4371', 'FT911-4372', 'FT911-4373', 'FT911-4374', 'FT911-4375']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_15\n",
            "Parsed documents (first 10): ['FT911-5129', 'FT911-5130', 'FT911-5131', 'FT911-5132', 'FT911-5133', 'FT911-5134', 'FT911-5135', 'FT911-5136', 'FT911-5137', 'FT911-5138']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_14\n",
            "Parsed documents (first 10): ['FT911-4749', 'FT911-4750', 'FT911-4751', 'FT911-4752', 'FT911-4753', 'FT911-4754', 'FT911-4755', 'FT911-4756', 'FT911-4757', 'FT911-4758']\n",
            "WordDictionary saved to /content/drive/MyDrive/InfoRetrieval/word_dictionary.txt\n",
            "FileDictionary saved to /content/drive/MyDrive/InfoRetrieval/file_dictionary.txt\n",
            "FTFileMapping saved to /content/drive/MyDrive/InfoRetrieval/ft_file_mapping.txt\n",
            "DocumentToFileMapping saved to /content/drive/MyDrive/InfoRetrieval/doc_to_file_mapping.txt\n",
            "Combined parser_output saved to /content/drive/MyDrive/InfoRetrieval/parser_output.txt\n",
            "DocumentToFileMapping (Sample):\n",
            "('FT911-15', '5359')\n",
            "('FT911-15', '5360')\n",
            "('FT911-15', '5361')\n",
            "('FT911-15', '5362')\n",
            "('FT911-15', '5363')\n",
            "('FT911-15', '5364')\n",
            "('FT911-15', '5365')\n",
            "('FT911-15', '5366')\n",
            "('FT911-15', '5367')\n",
            "('FT911-15', '5368')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load stopwords\n",
        "stopwords_path = \"/content/drive/MyDrive/InfoRetrieval/stopwordlist.txt\"\n",
        "with open(stopwords_path, 'r', encoding='utf-8') as f:\n",
        "    stopwords = set(f.read().split())\n",
        "\n",
        "# Initialize the stemmer\n",
        "word_stemmer = PorterStemmer()\n",
        "\n",
        "docs_path = \"/content/drive/MyDrive/InfoRetrieval/ft911/\"\n",
        "processed_docs = {}\n",
        "FTFileMapping = {}  # Mapping: file_name -> list of DOCNOs\n",
        "FileDictionary = {}  # Mapping: (file_name, local_doc_num) -> global_doc_id\n",
        "DocumentToFileMapping = []  # list of (file_name, numeric_DOCNO)\n",
        "\n",
        "# Sort files numerically based on suffix\n",
        "file_list = sorted(\n",
        "    [f for f in os.listdir(docs_path) if f.startswith(\"ft911_\")],\n",
        "    key=lambda x: int(re.findall(r'\\d+', x)[0])\n",
        ")\n",
        "\n",
        "# Process each file\n",
        "global_doc_id = 1\n",
        "for file_name in file_list:\n",
        "    full_file_path = os.path.join(docs_path, file_name)\n",
        "\n",
        "    if os.path.isfile(full_file_path):\n",
        "        print(f\"Checking file: {full_file_path}\")\n",
        "        with open(full_file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "            file_content = file.read()\n",
        "\n",
        "            # Parse TREC documents\n",
        "            parsed_docs = {}\n",
        "            current_doc_lines = []\n",
        "            current_doc_id = None\n",
        "            for line in file_content.splitlines():\n",
        "                if line.startswith(\"<DOCNO>\"):\n",
        "                    if current_doc_id is not None:\n",
        "                        parsed_docs[current_doc_id] = \" \".join(current_doc_lines)\n",
        "                    current_doc_id = line.replace(\"<DOCNO>\", \"\").replace(\"</DOCNO>\", \"\").strip()\n",
        "                    current_doc_lines = []\n",
        "                elif not line.startswith(\"<\") and current_doc_id:\n",
        "                    current_doc_lines.append(line.strip())\n",
        "            if current_doc_id is not None:\n",
        "                parsed_docs[current_doc_id] = \" \".join(current_doc_lines)\n",
        "\n",
        "            print(f\"Parsed documents (first 10): {list(parsed_docs.keys())[:10]}\")\n",
        "\n",
        "            # Save file -> DOCNOs\n",
        "            FTFileMapping[file_name] = list(parsed_docs.keys())\n",
        "\n",
        "            # Save cleaned (FT911-X, numeric_DOCNO)\n",
        "            for docno in parsed_docs.keys():\n",
        "                numeric_docno = docno.split(\"-\")[-1]\n",
        "                ft_formatted = f\"FT911-{file_name.split('_')[-1]}\"\n",
        "                DocumentToFileMapping.append((ft_formatted, numeric_docno))\n",
        "\n",
        "            # Assign global and local doc IDs\n",
        "            local_id = 1\n",
        "            for doc_id in parsed_docs:\n",
        "                FileDictionary[(file_name, local_id)] = global_doc_id\n",
        "                local_id += 1\n",
        "                global_doc_id += 1\n",
        "\n",
        "            # Process text\n",
        "            for doc_id, doc_text in sorted(parsed_docs.items()):\n",
        "                text = doc_text.lower()\n",
        "                text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
        "                tokens = nltk.word_tokenize(text)\n",
        "                tokens = [word for word in tokens if word.isalpha()]\n",
        "                filtered_tokens = [word for word in tokens if word not in stopwords]\n",
        "                stemmed_tokens = [word_stemmer.stem(word) for word in filtered_tokens]\n",
        "                processed_docs[doc_id] = stemmed_tokens\n",
        "# Create WordDictionary: sort all unique words alphabetically, then assign IDs\n",
        "all_words = set()\n",
        "for words in processed_docs.values():\n",
        "    all_words.update(words)\n",
        "\n",
        "WordDictionary = {}\n",
        "for idx, word in enumerate(sorted(all_words), start=1):\n",
        "    WordDictionary[word] = idx\n",
        "\n",
        "# Sort FileDictionary\n",
        "FileDictionary = dict(sorted(FileDictionary.items()))\n",
        "\n",
        "# Sort DocumentToFileMapping: sort FT911-14 before FT911-15\n",
        "def sort_key(doc):\n",
        "    prefix, number = doc[0].split(\"-\")\n",
        "    return int(number), int(doc[1])\n",
        "\n",
        "DocumentToFileMapping = sorted(DocumentToFileMapping, key=sort_key)\n",
        "\n",
        "# Save WordDictionary\n",
        "word_dict_path = \"/content/drive/MyDrive/InfoRetrieval/word_dictionary.txt\"\n",
        "with open(word_dict_path, \"w\") as word_file:\n",
        "    for word, word_id in WordDictionary.items():\n",
        "        word_file.write(f\"{word}\\t{word_id}\\n\")\n",
        "\n",
        "# Save FileDictionary (filename + local doc number)\n",
        "file_dict_path = \"/content/drive/MyDrive/InfoRetrieval/file_dictionary.txt\"\n",
        "with open(file_dict_path, \"w\") as file_file:\n",
        "    for (file_name, local_doc_num), global_id in FileDictionary.items():\n",
        "        file_file.write(f\"{file_name}\\t{local_doc_num}\\t{global_id}\\n\")\n",
        "\n",
        "# Save FTFileMapping (file -> list of DOCNOs)\n",
        "ft_map_path = \"/content/drive/MyDrive/InfoRetrieval/ft_file_mapping.txt\"\n",
        "with open(ft_map_path, \"w\") as map_file:\n",
        "    for file, doc_ids in FTFileMapping.items():\n",
        "        map_file.write(f\"{file}:\\t{', '.join(doc_ids)}\\n\")\n",
        "\n",
        "# Save final formatted DocumentToFileMapping\n",
        "doc_file_map_path = \"/content/drive/MyDrive/InfoRetrieval/doc_to_file_mapping.txt\"\n",
        "with open(doc_file_map_path, \"w\") as out_file:\n",
        "    for formatted_file, numeric_docno in DocumentToFileMapping:\n",
        "        out_file.write(f\"{formatted_file}\\t{numeric_docno}\\n\")\n",
        "\n",
        "# Create parser_output.txt with token dictionary and document ID mapping\n",
        "parser_output_path = \"/content/drive/MyDrive/InfoRetrieval/parser_output.txt\"\n",
        "with open(parser_output_path, \"w\") as out_file:\n",
        "    # First print word -> ID mapping\n",
        "    for word, word_id in WordDictionary.items():\n",
        "        out_file.write(f\"{word}\\t\\t{word_id}\\n\")\n",
        "\n",
        "    out_file.write(\"\\n\")\n",
        "\n",
        "    # Then print document -> ID mapping\n",
        "    for idx, (formatted_file, numeric_docno) in enumerate(DocumentToFileMapping, start=1):\n",
        "        out_file.write(f\"{formatted_file}\\t{numeric_docno}\\n\")\n",
        "\n",
        "# Logs\n",
        "print(f\"WordDictionary saved to {word_dict_path}\")\n",
        "print(f\"FileDictionary saved to {file_dict_path}\")\n",
        "print(f\"FTFileMapping saved to {ft_map_path}\")\n",
        "print(f\"DocumentToFileMapping saved to {doc_file_map_path}\")\n",
        "print(f\"parser_output.txt saved to {parser_output_path}\")\n",
        "\n",
        "# Print sample of token to ID mapping\n",
        "print(\"Token and Token ID Mapping (First 20):\")\n",
        "for token, token_id in list(WordDictionary.items())[:20]:\n",
        "    print(f\"{token}\\t{token_id}\")\n",
        "\n",
        "# Print DocumentToFileMapping like your format\n",
        "print(\"DocumentToFileMapping (Sample):\")\n",
        "for pair in DocumentToFileMapping[717:1100]:\n",
        "    print(pair)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Xc_jFTHkLJF",
        "outputId": "a4b29cef-562f-487a-e99a-f372f7f6d28b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_1\n",
            "Parsed documents (first 10): ['FT911-1', 'FT911-2', 'FT911-3', 'FT911-4', 'FT911-5', 'FT911-6', 'FT911-7', 'FT911-8', 'FT911-9', 'FT911-10']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_2\n",
            "Parsed documents (first 10): ['FT911-376', 'FT911-377', 'FT911-378', 'FT911-379', 'FT911-380', 'FT911-381', 'FT911-382', 'FT911-383', 'FT911-384', 'FT911-385']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_3\n",
            "Parsed documents (first 10): ['FT911-722', 'FT911-723', 'FT911-724', 'FT911-725', 'FT911-726', 'FT911-727', 'FT911-728', 'FT911-729', 'FT911-730', 'FT911-731']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_4\n",
            "Parsed documents (first 10): ['FT911-1099', 'FT911-1100', 'FT911-1101', 'FT911-1102', 'FT911-1103', 'FT911-1104', 'FT911-1105', 'FT911-1106', 'FT911-1107', 'FT911-1108']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_5\n",
            "Parsed documents (first 10): ['FT911-1479', 'FT911-1480', 'FT911-1481', 'FT911-1482', 'FT911-1483', 'FT911-1484', 'FT911-1485', 'FT911-1486', 'FT911-1487', 'FT911-1488']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_6\n",
            "Parsed documents (first 10): ['FT911-1832', 'FT911-1833', 'FT911-1834', 'FT911-1835', 'FT911-1836', 'FT911-1837', 'FT911-1838', 'FT911-1839', 'FT911-1840', 'FT911-1841']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_7\n",
            "Parsed documents (first 10): ['FT911-2212', 'FT911-2213', 'FT911-2214', 'FT911-2215', 'FT911-2216', 'FT911-2217', 'FT911-2218', 'FT911-2219', 'FT911-2220', 'FT911-2221']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_8\n",
            "Parsed documents (first 10): ['FT911-2621', 'FT911-2622', 'FT911-2623', 'FT911-2624', 'FT911-2625', 'FT911-2626', 'FT911-2627', 'FT911-2628', 'FT911-2629', 'FT911-2630']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_9\n",
            "Parsed documents (first 10): ['FT911-2950', 'FT911-2951', 'FT911-2952', 'FT911-2953', 'FT911-2954', 'FT911-2955', 'FT911-2956', 'FT911-2957', 'FT911-2958', 'FT911-2959']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_10\n",
            "Parsed documents (first 10): ['FT911-3323', 'FT911-3324', 'FT911-3325', 'FT911-3326', 'FT911-3327', 'FT911-3328', 'FT911-3329', 'FT911-3330', 'FT911-3331', 'FT911-3332']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_11\n",
            "Parsed documents (first 10): ['FT911-3694', 'FT911-3695', 'FT911-3696', 'FT911-3697', 'FT911-3698', 'FT911-3699', 'FT911-3700', 'FT911-3701', 'FT911-3702', 'FT911-3703']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_12\n",
            "Parsed documents (first 10): ['FT911-4016', 'FT911-4017', 'FT911-4018', 'FT911-4019', 'FT911-4020', 'FT911-4021', 'FT911-4022', 'FT911-4023', 'FT911-4024', 'FT911-4025']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_13\n",
            "Parsed documents (first 10): ['FT911-4366', 'FT911-4367', 'FT911-4368', 'FT911-4369', 'FT911-4370', 'FT911-4371', 'FT911-4372', 'FT911-4373', 'FT911-4374', 'FT911-4375']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_15\n",
            "Parsed documents (first 10): ['FT911-5129', 'FT911-5130', 'FT911-5131', 'FT911-5132', 'FT911-5133', 'FT911-5134', 'FT911-5135', 'FT911-5136', 'FT911-5137', 'FT911-5138']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_14\n",
            "Parsed documents (first 10): ['FT911-4749', 'FT911-4750', 'FT911-4751', 'FT911-4752', 'FT911-4753', 'FT911-4754', 'FT911-4755', 'FT911-4756', 'FT911-4757', 'FT911-4758']\n",
            "WordDictionary saved to /content/drive/MyDrive/InfoRetrieval/word_dictionary.txt\n",
            "FileDictionary saved to /content/drive/MyDrive/InfoRetrieval/file_dictionary.txt\n",
            "FTFileMapping saved to /content/drive/MyDrive/InfoRetrieval/ft_file_mapping.txt\n",
            "DocumentToFileMapping saved to /content/drive/MyDrive/InfoRetrieval/doc_to_file_mapping.txt\n",
            "parser_output.txt saved to /content/drive/MyDrive/InfoRetrieval/parser_output.txt\n",
            "Token and Token ID Mapping (First 20):\n",
            "aa\t1\n",
            "aaa\t2\n",
            "aachen\t3\n",
            "aaf\t4\n",
            "aah\t5\n",
            "aakvaag\t6\n",
            "aalborg\t7\n",
            "aaron\t8\n",
            "ab\t9\n",
            "ababa\t10\n",
            "aback\t11\n",
            "abalkin\t12\n",
            "abandon\t13\n",
            "abash\t14\n",
            "abat\t15\n",
            "abattoir\t16\n",
            "abb\t17\n",
            "abba\t18\n",
            "abbacchio\t19\n",
            "abbado\t20\n",
            "DocumentToFileMapping (Sample):\n",
            "('FT911-2', '718')\n",
            "('FT911-2', '719')\n",
            "('FT911-2', '720')\n",
            "('FT911-2', '721')\n",
            "('FT911-3', '722')\n",
            "('FT911-3', '723')\n",
            "('FT911-3', '724')\n",
            "('FT911-3', '725')\n",
            "('FT911-3', '726')\n",
            "('FT911-3', '727')\n",
            "('FT911-3', '728')\n",
            "('FT911-3', '729')\n",
            "('FT911-3', '730')\n",
            "('FT911-3', '731')\n",
            "('FT911-3', '732')\n",
            "('FT911-3', '733')\n",
            "('FT911-3', '734')\n",
            "('FT911-3', '735')\n",
            "('FT911-3', '736')\n",
            "('FT911-3', '737')\n",
            "('FT911-3', '738')\n",
            "('FT911-3', '739')\n",
            "('FT911-3', '740')\n",
            "('FT911-3', '741')\n",
            "('FT911-3', '742')\n",
            "('FT911-3', '743')\n",
            "('FT911-3', '744')\n",
            "('FT911-3', '745')\n",
            "('FT911-3', '746')\n",
            "('FT911-3', '747')\n",
            "('FT911-3', '748')\n",
            "('FT911-3', '749')\n",
            "('FT911-3', '750')\n",
            "('FT911-3', '751')\n",
            "('FT911-3', '752')\n",
            "('FT911-3', '753')\n",
            "('FT911-3', '754')\n",
            "('FT911-3', '755')\n",
            "('FT911-3', '756')\n",
            "('FT911-3', '757')\n",
            "('FT911-3', '758')\n",
            "('FT911-3', '759')\n",
            "('FT911-3', '760')\n",
            "('FT911-3', '761')\n",
            "('FT911-3', '762')\n",
            "('FT911-3', '763')\n",
            "('FT911-3', '764')\n",
            "('FT911-3', '765')\n",
            "('FT911-3', '766')\n",
            "('FT911-3', '767')\n",
            "('FT911-3', '768')\n",
            "('FT911-3', '769')\n",
            "('FT911-3', '770')\n",
            "('FT911-3', '771')\n",
            "('FT911-3', '772')\n",
            "('FT911-3', '773')\n",
            "('FT911-3', '774')\n",
            "('FT911-3', '775')\n",
            "('FT911-3', '776')\n",
            "('FT911-3', '777')\n",
            "('FT911-3', '778')\n",
            "('FT911-3', '779')\n",
            "('FT911-3', '780')\n",
            "('FT911-3', '781')\n",
            "('FT911-3', '782')\n",
            "('FT911-3', '783')\n",
            "('FT911-3', '784')\n",
            "('FT911-3', '785')\n",
            "('FT911-3', '786')\n",
            "('FT911-3', '787')\n",
            "('FT911-3', '788')\n",
            "('FT911-3', '789')\n",
            "('FT911-3', '790')\n",
            "('FT911-3', '791')\n",
            "('FT911-3', '792')\n",
            "('FT911-3', '793')\n",
            "('FT911-3', '794')\n",
            "('FT911-3', '795')\n",
            "('FT911-3', '796')\n",
            "('FT911-3', '797')\n",
            "('FT911-3', '798')\n",
            "('FT911-3', '799')\n",
            "('FT911-3', '800')\n",
            "('FT911-3', '801')\n",
            "('FT911-3', '802')\n",
            "('FT911-3', '803')\n",
            "('FT911-3', '804')\n",
            "('FT911-3', '805')\n",
            "('FT911-3', '806')\n",
            "('FT911-3', '807')\n",
            "('FT911-3', '808')\n",
            "('FT911-3', '809')\n",
            "('FT911-3', '810')\n",
            "('FT911-3', '811')\n",
            "('FT911-3', '812')\n",
            "('FT911-3', '813')\n",
            "('FT911-3', '814')\n",
            "('FT911-3', '815')\n",
            "('FT911-3', '816')\n",
            "('FT911-3', '817')\n",
            "('FT911-3', '818')\n",
            "('FT911-3', '819')\n",
            "('FT911-3', '820')\n",
            "('FT911-3', '821')\n",
            "('FT911-3', '822')\n",
            "('FT911-3', '823')\n",
            "('FT911-3', '824')\n",
            "('FT911-3', '825')\n",
            "('FT911-3', '826')\n",
            "('FT911-3', '827')\n",
            "('FT911-3', '828')\n",
            "('FT911-3', '829')\n",
            "('FT911-3', '830')\n",
            "('FT911-3', '831')\n",
            "('FT911-3', '832')\n",
            "('FT911-3', '833')\n",
            "('FT911-3', '834')\n",
            "('FT911-3', '835')\n",
            "('FT911-3', '836')\n",
            "('FT911-3', '837')\n",
            "('FT911-3', '838')\n",
            "('FT911-3', '839')\n",
            "('FT911-3', '840')\n",
            "('FT911-3', '841')\n",
            "('FT911-3', '842')\n",
            "('FT911-3', '843')\n",
            "('FT911-3', '844')\n",
            "('FT911-3', '845')\n",
            "('FT911-3', '846')\n",
            "('FT911-3', '847')\n",
            "('FT911-3', '848')\n",
            "('FT911-3', '849')\n",
            "('FT911-3', '850')\n",
            "('FT911-3', '851')\n",
            "('FT911-3', '852')\n",
            "('FT911-3', '853')\n",
            "('FT911-3', '854')\n",
            "('FT911-3', '855')\n",
            "('FT911-3', '856')\n",
            "('FT911-3', '857')\n",
            "('FT911-3', '858')\n",
            "('FT911-3', '859')\n",
            "('FT911-3', '860')\n",
            "('FT911-3', '861')\n",
            "('FT911-3', '862')\n",
            "('FT911-3', '863')\n",
            "('FT911-3', '864')\n",
            "('FT911-3', '865')\n",
            "('FT911-3', '866')\n",
            "('FT911-3', '867')\n",
            "('FT911-3', '868')\n",
            "('FT911-3', '869')\n",
            "('FT911-3', '870')\n",
            "('FT911-3', '871')\n",
            "('FT911-3', '872')\n",
            "('FT911-3', '873')\n",
            "('FT911-3', '874')\n",
            "('FT911-3', '875')\n",
            "('FT911-3', '876')\n",
            "('FT911-3', '877')\n",
            "('FT911-3', '878')\n",
            "('FT911-3', '879')\n",
            "('FT911-3', '880')\n",
            "('FT911-3', '881')\n",
            "('FT911-3', '882')\n",
            "('FT911-3', '883')\n",
            "('FT911-3', '884')\n",
            "('FT911-3', '885')\n",
            "('FT911-3', '886')\n",
            "('FT911-3', '887')\n",
            "('FT911-3', '888')\n",
            "('FT911-3', '889')\n",
            "('FT911-3', '890')\n",
            "('FT911-3', '891')\n",
            "('FT911-3', '892')\n",
            "('FT911-3', '893')\n",
            "('FT911-3', '894')\n",
            "('FT911-3', '895')\n",
            "('FT911-3', '896')\n",
            "('FT911-3', '897')\n",
            "('FT911-3', '898')\n",
            "('FT911-3', '899')\n",
            "('FT911-3', '900')\n",
            "('FT911-3', '901')\n",
            "('FT911-3', '902')\n",
            "('FT911-3', '903')\n",
            "('FT911-3', '904')\n",
            "('FT911-3', '905')\n",
            "('FT911-3', '906')\n",
            "('FT911-3', '907')\n",
            "('FT911-3', '908')\n",
            "('FT911-3', '909')\n",
            "('FT911-3', '910')\n",
            "('FT911-3', '911')\n",
            "('FT911-3', '912')\n",
            "('FT911-3', '913')\n",
            "('FT911-3', '914')\n",
            "('FT911-3', '915')\n",
            "('FT911-3', '916')\n",
            "('FT911-3', '917')\n",
            "('FT911-3', '918')\n",
            "('FT911-3', '919')\n",
            "('FT911-3', '920')\n",
            "('FT911-3', '921')\n",
            "('FT911-3', '922')\n",
            "('FT911-3', '923')\n",
            "('FT911-3', '924')\n",
            "('FT911-3', '925')\n",
            "('FT911-3', '926')\n",
            "('FT911-3', '927')\n",
            "('FT911-3', '928')\n",
            "('FT911-3', '929')\n",
            "('FT911-3', '930')\n",
            "('FT911-3', '931')\n",
            "('FT911-3', '932')\n",
            "('FT911-3', '933')\n",
            "('FT911-3', '934')\n",
            "('FT911-3', '935')\n",
            "('FT911-3', '936')\n",
            "('FT911-3', '937')\n",
            "('FT911-3', '938')\n",
            "('FT911-3', '939')\n",
            "('FT911-3', '940')\n",
            "('FT911-3', '941')\n",
            "('FT911-3', '942')\n",
            "('FT911-3', '943')\n",
            "('FT911-3', '944')\n",
            "('FT911-3', '945')\n",
            "('FT911-3', '946')\n",
            "('FT911-3', '947')\n",
            "('FT911-3', '948')\n",
            "('FT911-3', '949')\n",
            "('FT911-3', '950')\n",
            "('FT911-3', '951')\n",
            "('FT911-3', '952')\n",
            "('FT911-3', '953')\n",
            "('FT911-3', '954')\n",
            "('FT911-3', '955')\n",
            "('FT911-3', '956')\n",
            "('FT911-3', '957')\n",
            "('FT911-3', '958')\n",
            "('FT911-3', '959')\n",
            "('FT911-3', '960')\n",
            "('FT911-3', '961')\n",
            "('FT911-3', '962')\n",
            "('FT911-3', '963')\n",
            "('FT911-3', '964')\n",
            "('FT911-3', '965')\n",
            "('FT911-3', '966')\n",
            "('FT911-3', '967')\n",
            "('FT911-3', '968')\n",
            "('FT911-3', '969')\n",
            "('FT911-3', '970')\n",
            "('FT911-3', '971')\n",
            "('FT911-3', '972')\n",
            "('FT911-3', '973')\n",
            "('FT911-3', '974')\n",
            "('FT911-3', '975')\n",
            "('FT911-3', '976')\n",
            "('FT911-3', '977')\n",
            "('FT911-3', '978')\n",
            "('FT911-3', '979')\n",
            "('FT911-3', '980')\n",
            "('FT911-3', '981')\n",
            "('FT911-3', '982')\n",
            "('FT911-3', '983')\n",
            "('FT911-3', '984')\n",
            "('FT911-3', '985')\n",
            "('FT911-3', '986')\n",
            "('FT911-3', '987')\n",
            "('FT911-3', '988')\n",
            "('FT911-3', '989')\n",
            "('FT911-3', '990')\n",
            "('FT911-3', '991')\n",
            "('FT911-3', '992')\n",
            "('FT911-3', '993')\n",
            "('FT911-3', '994')\n",
            "('FT911-3', '995')\n",
            "('FT911-3', '996')\n",
            "('FT911-3', '997')\n",
            "('FT911-3', '998')\n",
            "('FT911-3', '999')\n",
            "('FT911-3', '1000')\n",
            "('FT911-3', '1001')\n",
            "('FT911-3', '1002')\n",
            "('FT911-3', '1003')\n",
            "('FT911-3', '1004')\n",
            "('FT911-3', '1005')\n",
            "('FT911-3', '1006')\n",
            "('FT911-3', '1007')\n",
            "('FT911-3', '1008')\n",
            "('FT911-3', '1009')\n",
            "('FT911-3', '1010')\n",
            "('FT911-3', '1011')\n",
            "('FT911-3', '1012')\n",
            "('FT911-3', '1013')\n",
            "('FT911-3', '1014')\n",
            "('FT911-3', '1015')\n",
            "('FT911-3', '1016')\n",
            "('FT911-3', '1017')\n",
            "('FT911-3', '1018')\n",
            "('FT911-3', '1019')\n",
            "('FT911-3', '1020')\n",
            "('FT911-3', '1021')\n",
            "('FT911-3', '1022')\n",
            "('FT911-3', '1023')\n",
            "('FT911-3', '1024')\n",
            "('FT911-3', '1025')\n",
            "('FT911-3', '1026')\n",
            "('FT911-3', '1027')\n",
            "('FT911-3', '1028')\n",
            "('FT911-3', '1029')\n",
            "('FT911-3', '1030')\n",
            "('FT911-3', '1031')\n",
            "('FT911-3', '1032')\n",
            "('FT911-3', '1033')\n",
            "('FT911-3', '1034')\n",
            "('FT911-3', '1035')\n",
            "('FT911-3', '1036')\n",
            "('FT911-3', '1037')\n",
            "('FT911-3', '1038')\n",
            "('FT911-3', '1039')\n",
            "('FT911-3', '1040')\n",
            "('FT911-3', '1041')\n",
            "('FT911-3', '1042')\n",
            "('FT911-3', '1043')\n",
            "('FT911-3', '1044')\n",
            "('FT911-3', '1045')\n",
            "('FT911-3', '1046')\n",
            "('FT911-3', '1047')\n",
            "('FT911-3', '1048')\n",
            "('FT911-3', '1049')\n",
            "('FT911-3', '1050')\n",
            "('FT911-3', '1051')\n",
            "('FT911-3', '1052')\n",
            "('FT911-3', '1053')\n",
            "('FT911-3', '1054')\n",
            "('FT911-3', '1055')\n",
            "('FT911-3', '1056')\n",
            "('FT911-3', '1057')\n",
            "('FT911-3', '1058')\n",
            "('FT911-3', '1059')\n",
            "('FT911-3', '1060')\n",
            "('FT911-3', '1061')\n",
            "('FT911-3', '1062')\n",
            "('FT911-3', '1063')\n",
            "('FT911-3', '1064')\n",
            "('FT911-3', '1065')\n",
            "('FT911-3', '1066')\n",
            "('FT911-3', '1067')\n",
            "('FT911-3', '1068')\n",
            "('FT911-3', '1069')\n",
            "('FT911-3', '1070')\n",
            "('FT911-3', '1071')\n",
            "('FT911-3', '1072')\n",
            "('FT911-3', '1073')\n",
            "('FT911-3', '1074')\n",
            "('FT911-3', '1075')\n",
            "('FT911-3', '1076')\n",
            "('FT911-3', '1077')\n",
            "('FT911-3', '1078')\n",
            "('FT911-3', '1079')\n",
            "('FT911-3', '1080')\n",
            "('FT911-3', '1081')\n",
            "('FT911-3', '1082')\n",
            "('FT911-3', '1083')\n",
            "('FT911-3', '1084')\n",
            "('FT911-3', '1085')\n",
            "('FT911-3', '1086')\n",
            "('FT911-3', '1087')\n",
            "('FT911-3', '1088')\n",
            "('FT911-3', '1089')\n",
            "('FT911-3', '1090')\n",
            "('FT911-3', '1091')\n",
            "('FT911-3', '1092')\n",
            "('FT911-3', '1093')\n",
            "('FT911-3', '1094')\n",
            "('FT911-3', '1095')\n",
            "('FT911-3', '1096')\n",
            "('FT911-3', '1097')\n",
            "('FT911-3', '1098')\n",
            "('FT911-4', '1099')\n",
            "('FT911-4', '1100')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "egoGsQcTKlx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUMUD64nr7nc",
        "outputId": "626e6575-2c9f-4edf-8832-dd892ef52a4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_1\n",
            "Parsed documents (first 10): ['FT911-1', 'FT911-2', 'FT911-3', 'FT911-4', 'FT911-5', 'FT911-6', 'FT911-7', 'FT911-8', 'FT911-9', 'FT911-10']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_2\n",
            "Parsed documents (first 10): ['FT911-376', 'FT911-377', 'FT911-378', 'FT911-379', 'FT911-380', 'FT911-381', 'FT911-382', 'FT911-383', 'FT911-384', 'FT911-385']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_3\n",
            "Parsed documents (first 10): ['FT911-722', 'FT911-723', 'FT911-724', 'FT911-725', 'FT911-726', 'FT911-727', 'FT911-728', 'FT911-729', 'FT911-730', 'FT911-731']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_4\n",
            "Parsed documents (first 10): ['FT911-1099', 'FT911-1100', 'FT911-1101', 'FT911-1102', 'FT911-1103', 'FT911-1104', 'FT911-1105', 'FT911-1106', 'FT911-1107', 'FT911-1108']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_5\n",
            "Parsed documents (first 10): ['FT911-1479', 'FT911-1480', 'FT911-1481', 'FT911-1482', 'FT911-1483', 'FT911-1484', 'FT911-1485', 'FT911-1486', 'FT911-1487', 'FT911-1488']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_6\n",
            "Parsed documents (first 10): ['FT911-1832', 'FT911-1833', 'FT911-1834', 'FT911-1835', 'FT911-1836', 'FT911-1837', 'FT911-1838', 'FT911-1839', 'FT911-1840', 'FT911-1841']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_7\n",
            "Parsed documents (first 10): ['FT911-2212', 'FT911-2213', 'FT911-2214', 'FT911-2215', 'FT911-2216', 'FT911-2217', 'FT911-2218', 'FT911-2219', 'FT911-2220', 'FT911-2221']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_8\n",
            "Parsed documents (first 10): ['FT911-2621', 'FT911-2622', 'FT911-2623', 'FT911-2624', 'FT911-2625', 'FT911-2626', 'FT911-2627', 'FT911-2628', 'FT911-2629', 'FT911-2630']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_9\n",
            "Parsed documents (first 10): ['FT911-2950', 'FT911-2951', 'FT911-2952', 'FT911-2953', 'FT911-2954', 'FT911-2955', 'FT911-2956', 'FT911-2957', 'FT911-2958', 'FT911-2959']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_10\n",
            "Parsed documents (first 10): ['FT911-3323', 'FT911-3324', 'FT911-3325', 'FT911-3326', 'FT911-3327', 'FT911-3328', 'FT911-3329', 'FT911-3330', 'FT911-3331', 'FT911-3332']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_11\n",
            "Parsed documents (first 10): ['FT911-3694', 'FT911-3695', 'FT911-3696', 'FT911-3697', 'FT911-3698', 'FT911-3699', 'FT911-3700', 'FT911-3701', 'FT911-3702', 'FT911-3703']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_12\n",
            "Parsed documents (first 10): ['FT911-4016', 'FT911-4017', 'FT911-4018', 'FT911-4019', 'FT911-4020', 'FT911-4021', 'FT911-4022', 'FT911-4023', 'FT911-4024', 'FT911-4025']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_13\n",
            "Parsed documents (first 10): ['FT911-4366', 'FT911-4367', 'FT911-4368', 'FT911-4369', 'FT911-4370', 'FT911-4371', 'FT911-4372', 'FT911-4373', 'FT911-4374', 'FT911-4375']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_15\n",
            "Parsed documents (first 10): ['FT911-5129', 'FT911-5130', 'FT911-5131', 'FT911-5132', 'FT911-5133', 'FT911-5134', 'FT911-5135', 'FT911-5136', 'FT911-5137', 'FT911-5138']\n",
            "Checking file: /content/drive/MyDrive/InfoRetrieval/ft911/ft911_14\n",
            "Parsed documents (first 10): ['FT911-4749', 'FT911-4750', 'FT911-4751', 'FT911-4752', 'FT911-4753', 'FT911-4754', 'FT911-4755', 'FT911-4756', 'FT911-4757', 'FT911-4758']\n",
            "WordDictionary saved to /content/drive/MyDrive/InfoRetrieval/word_dictionary.txt\n",
            "FileDictionary saved to /content/drive/MyDrive/InfoRetrieval/file_dictionary.txt\n",
            "FTFileMapping saved to /content/drive/MyDrive/InfoRetrieval/ft_file_mapping.txt\n",
            "DocumentToFileMapping saved to /content/drive/MyDrive/InfoRetrieval/doc_to_file_mapping.txt\n",
            "parser_output.txt saved to /content/drive/MyDrive/InfoRetrieval/parser_output.txt\n",
            "Token and Token ID Mapping (First 20):\n",
            "aa\t1\n",
            "aaa\t2\n",
            "aachen\t3\n",
            "aaf\t4\n",
            "aah\t5\n",
            "aakvaag\t6\n",
            "aalborg\t7\n",
            "aaron\t8\n",
            "ab\t9\n",
            "ababa\t10\n",
            "aback\t11\n",
            "abalkin\t12\n",
            "abandon\t13\n",
            "abash\t14\n",
            "abat\t15\n",
            "abattoir\t16\n",
            "abb\t17\n",
            "abba\t18\n",
            "abbacchio\t19\n",
            "abbado\t20\n",
            "DocumentToFileMapping (Sample):\n",
            "('FT911-3', '901')\n",
            "('FT911-3', '902')\n",
            "('FT911-3', '903')\n",
            "('FT911-3', '904')\n",
            "('FT911-3', '905')\n",
            "('FT911-3', '906')\n",
            "('FT911-3', '907')\n",
            "('FT911-3', '908')\n",
            "('FT911-3', '909')\n",
            "('FT911-3', '910')\n"
          ]
        }
      ],
      "source": [
        "# Load stopwords\n",
        "stopwords_path = \"/content/drive/MyDrive/InfoRetrieval/stopwordlist.txt\"\n",
        "with open(stopwords_path, 'r', encoding='utf-8') as f:\n",
        "    stopwords = set(f.read().split())\n",
        "\n",
        "# Initialize the stemmer\n",
        "word_stemmer = PorterStemmer()\n",
        "\n",
        "docs_path = \"/content/drive/MyDrive/InfoRetrieval/ft911/\"\n",
        "processed_docs = {}\n",
        "FTFileMapping = {}  # Mapping: file_name -> list of DOCNOs\n",
        "FileDictionary = {}  # Mapping: (file_name, local_doc_num) -> global_doc_id\n",
        "DocumentToFileMapping = []  # list of (file_name, numeric_DOCNO)\n",
        "\n",
        "# Sort files numerically based on suffix\n",
        "file_list = sorted(\n",
        "    [f for f in os.listdir(docs_path) if f.startswith(\"ft911_\")],\n",
        "    key=lambda x: int(re.findall(r'\\d+', x)[0])\n",
        ")\n",
        "\n",
        "# Process each file\n",
        "global_doc_id = 1\n",
        "for file_name in file_list:\n",
        "    full_file_path = os.path.join(docs_path, file_name)\n",
        "\n",
        "    if os.path.isfile(full_file_path):\n",
        "        print(f\"Checking file: {full_file_path}\")\n",
        "        with open(full_file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "            file_content = file.read()\n",
        "\n",
        "            # Parse TREC documents\n",
        "            parsed_docs = {}\n",
        "            current_doc_lines = []\n",
        "            current_doc_id = None\n",
        "            for line in file_content.splitlines():\n",
        "                if line.startswith(\"<DOCNO>\"):\n",
        "                    if current_doc_id is not None:\n",
        "                        parsed_docs[current_doc_id] = \" \".join(current_doc_lines)\n",
        "                    current_doc_id = line.replace(\"<DOCNO>\", \"\").replace(\"</DOCNO>\", \"\").strip()\n",
        "                    current_doc_lines = []\n",
        "                elif not line.startswith(\"<\") and current_doc_id:\n",
        "                    current_doc_lines.append(line.strip())\n",
        "            if current_doc_id is not None:\n",
        "                parsed_docs[current_doc_id] = \" \".join(current_doc_lines)\n",
        "\n",
        "            print(f\"Parsed documents (first 10): {list(parsed_docs.keys())[:10]}\")\n",
        "\n",
        "            # Save file -> DOCNOs\n",
        "            FTFileMapping[file_name] = list(parsed_docs.keys())\n",
        "\n",
        "            # Save cleaned (FT911-X, numeric_DOCNO)\n",
        "            for docno in parsed_docs.keys():\n",
        "                numeric_docno = docno.split(\"-\")[-1]\n",
        "                ft_formatted = f\"FT911-{file_name.split('_')[-1]}\"\n",
        "                DocumentToFileMapping.append((ft_formatted, numeric_docno))\n",
        "\n",
        "            # Assign global and local doc IDs\n",
        "            local_id = 1\n",
        "            for doc_id in parsed_docs:\n",
        "                FileDictionary[(file_name, local_id)] = global_doc_id\n",
        "                local_id += 1\n",
        "                global_doc_id += 1\n",
        "\n",
        "            # Process text\n",
        "            for doc_id, doc_text in sorted(parsed_docs.items()):\n",
        "                text = doc_text.lower()\n",
        "                text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
        "                tokens = nltk.word_tokenize(text)\n",
        "                tokens = [word for word in tokens if word.isalpha()]\n",
        "                filtered_tokens = [word for word in tokens if word not in stopwords]\n",
        "                stemmed_tokens = [word_stemmer.stem(word) for word in filtered_tokens]\n",
        "                processed_docs[doc_id] = stemmed_tokens\n",
        "# Create WordDictionary: sort all unique words alphabetically, then assign IDs\n",
        "all_words = set()\n",
        "for words in processed_docs.values():\n",
        "    all_words.update(words)\n",
        "\n",
        "WordDictionary = {}\n",
        "for idx, word in enumerate(sorted(all_words), start=1):\n",
        "    WordDictionary[word] = idx\n",
        "\n",
        "# Sort FileDictionary\n",
        "FileDictionary = dict(sorted(FileDictionary.items()))\n",
        "\n",
        "# Sort DocumentToFileMapping: sort FT911-14 before FT911-15\n",
        "def sort_key(doc):\n",
        "    prefix, number = doc[0].split(\"-\")\n",
        "    return int(number), int(doc[1])\n",
        "\n",
        "DocumentToFileMapping = sorted(DocumentToFileMapping, key=sort_key)\n",
        "\n",
        "# Save WordDictionary\n",
        "word_dict_path = \"/content/drive/MyDrive/InfoRetrieval/word_dictionary.txt\"\n",
        "with open(word_dict_path, \"w\") as word_file:\n",
        "    for word, word_id in WordDictionary.items():\n",
        "        word_file.write(f\"{word}\\t{word_id}\\n\")\n",
        "\n",
        "# Save FileDictionary (filename + local doc number)\n",
        "file_dict_path = \"/content/drive/MyDrive/InfoRetrieval/file_dictionary.txt\"\n",
        "with open(file_dict_path, \"w\") as file_file:\n",
        "    for (file_name, local_doc_num), global_id in FileDictionary.items():\n",
        "        file_file.write(f\"{file_name}\\t{local_doc_num}\\t{global_id}\\n\")\n",
        "\n",
        "# Save FTFileMapping (file -> list of DOCNOs)\n",
        "ft_map_path = \"/content/drive/MyDrive/InfoRetrieval/ft_file_mapping.txt\"\n",
        "with open(ft_map_path, \"w\") as map_file:\n",
        "    for file, doc_ids in FTFileMapping.items():\n",
        "        map_file.write(f\"{file}:\\t{', '.join(doc_ids)}\\n\")\n",
        "\n",
        "# Save final formatted DocumentToFileMapping\n",
        "doc_file_map_path = \"/content/drive/MyDrive/InfoRetrieval/doc_to_file_mapping.txt\"\n",
        "with open(doc_file_map_path, \"w\") as out_file:\n",
        "    for formatted_file, numeric_docno in DocumentToFileMapping:\n",
        "        out_file.write(f\"{formatted_file}\\t{numeric_docno}\\n\")\n",
        "\n",
        "# Create parser_output.txt with token dictionary and document ID mapping\n",
        "parser_output_path = \"/content/drive/MyDrive/InfoRetrieval/parser_output.txt\"\n",
        "with open(parser_output_path, \"w\") as out_file:\n",
        "    # First print word -> ID mapping\n",
        "    for word, word_id in WordDictionary.items():\n",
        "        out_file.write(f\"{word}\\t\\t{word_id}\\n\")\n",
        "\n",
        "    out_file.write(\"\\n\")\n",
        "\n",
        "    # Then print document -> ID mapping\n",
        "    for idx, (formatted_file, numeric_docno) in enumerate(DocumentToFileMapping, start=1):\n",
        "        out_file.write(f\"{formatted_file}\\t{numeric_docno}\\n\")\n",
        "\n",
        "# Logs\n",
        "print(f\"WordDictionary saved to {word_dict_path}\")\n",
        "print(f\"FileDictionary saved to {file_dict_path}\")\n",
        "print(f\"FTFileMapping saved to {ft_map_path}\")\n",
        "print(f\"DocumentToFileMapping saved to {doc_file_map_path}\")\n",
        "print(f\"parser_output.txt saved to {parser_output_path}\")\n",
        "\n",
        "# Print sample of token to ID mapping\n",
        "print(\"Token and Token ID Mapping (First 20):\")\n",
        "for token, token_id in list(WordDictionary.items())[:20]:\n",
        "    print(f\"{token}\\t{token_id}\")\n",
        "\n",
        "# Print DocumentToFileMapping like your format\n",
        "print(\"DocumentToFileMapping (Sample):\")\n",
        "for pair in DocumentToFileMapping[900:910]:\n",
        "    print(pair)"
      ]
    }
  ]
}